<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>GPTembedding和fine-tuning技术解析 | PaperMod</title>
<meta name="keywords" content="唐玮">
<meta name="description" content="唐玮">
<meta name="author" content="王宇">
<link rel="canonical" href="http://localhost:1313/posts/wiki/gptembedding%E5%92%8Cfine-tuning%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/">
<meta name="google-site-verification" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7b92bf4867c997b58ec50f63451b83777173d5bd5593376852abd85746d09d71.css" integrity="sha256-e5K/SGfJl7WOxQ9jRRuDd3Fz1b1VkzdoUqvYV0bQnXE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/posts/wiki/gptembedding%E5%92%8Cfine-tuning%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="GPTembedding和fine-tuning技术解析" />
<meta property="og:description" content="唐玮" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/wiki/gptembedding%E5%92%8Cfine-tuning%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="posts" />




<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="GPTembedding和fine-tuning技术解析"/>
<meta name="twitter:description" content="唐玮"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "GPTembedding和fine-tuning技术解析",
      "item": "http://localhost:1313/posts/wiki/gptembedding%E5%92%8Cfine-tuning%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GPTembedding和fine-tuning技术解析",
  "name": "GPTembedding和fine-tuning技术解析",
  "description": "唐玮",
  "keywords": [
    "唐玮"
  ],
  "articleBody": "大模型（AGI）+embedding、fine-tuning（企业知识） = BGI（企业模型） 会成为很多AI应用系统的技术实现方式之一。目前所有基于GPT、或者类似GPT的语言大模型或者是AIGC类的图像大模型进行扩展的应用，基本都是基于embedding和fine-tuning两门技术实现的。例如replike、duolingo等。embedding、fine-tuning 两门技术实质上不会受到模态的限制。它可以在语言大模型、图像大模型、视频大模型等等不同模态的模型中使用。正如Jay Alammar 一个AI专家所说的：“万物皆可embedding”。\n通常，基于大模型进行的应用开发会比从头开始构建模型要实用和方便得多（更少的数据，更好的效果）。从头构建模型，主要所需要涉及的数据量和计算量是非常庞大的一个数字。而大模型，因为其已经训练有通用的语义理解能力，所以基于大模型去训练企业的模型所需的数据量相比从头构建模型要少得很多。例如下面embedding测试GPT回答万得厨服务助手类问题的示例中，只需把标准问和标准答传给GPT，GPT即能准确识别不同问法的所真正要问的问题，并且能够产生多种不同的回答文本。而原来以知识库的技术方式是需要人工把相似问梳理出来，并且只能产生一种回答。\n经过梳理，基于 ”语言大模型“+embedding、finr-tuning ，可以实现以下功能：\n让GPT回答GPT不熟悉的主题。如21年9月后的事件、非公开的文档、公司私有数据等 推荐相似的项目。如推荐相似的食谱 分析类需求。如用户画像的聚类分析、异常监测、用户分类等 个性化对话。如依据每个用户的用户画像和历史记录，按照用户喜欢的风格回答问题。 修改语言风格，或者回复句式。 … 1. embedding后，GPT能够回答万得厨相关问题，并具备较好的泛化能力 1.1. embedding可以实现的功能 embedding技术，抽象来说是提供给了GPT一个短期的记忆。能让GPT基于短期记忆进行回复。短期记忆中除了与问题相关的知识，还可以补充该用户的画像信息、喜好等数据，以让GPT能够个性化的回复用户。同时结合GPT大模型提供的向量转化算法。embedding能够实现下面这些功能\n让GPT回答GPT不熟悉的主题。如21年9月后的事件、非公开的文档、公司私有数据等 推荐相似的项目。如推荐相似的食谱 分析类需求。如用户画像的聚类分析、异常监测、用户分类等 个性化对话。如依据每个用户的用户画像和历史记录，按照用户喜欢的风格回答问题。 1.2. embedding原理 embedding原理其实很简单。即把公司的知识库以prompt的形式加入到用户的问句中（这些问题中即包含了问题的答案），chatGPT依据这个“知识库”+“用户问题” 组合而成的问题去进行回答。\n因为知识库是庞大的，所以先需要通过 “找相似知识的算法” 去找到相似的知识库，即与提问相关的知识，然后GPT用这些知识来回答问题。\n举个例子：\n用户输入“你知不知道万得厨是什么？” 随即系统计算这个问题与知识库中的哪些知识比较相关。通过计算与各个知识的余弦相似度，按相似度高低进行排序，根据设定条件提取出部分相关的知识 与用户输入进行拼接。下面是拼接的结果（因实际数据很多，这里省略了大部分数据） 使用以下的FAQ来回答后续问题，如果在FAQ中找不到答案，请写下“我找不到一个答案”\n0相似问和回答:\n\"\"\"\n万得厨可以烹饪不是新食记的预制菜嘛 如果是可微波且包装合适的食品，都可以用万得厨进行烹饪哟\n\"\"\"\n1相似问和回答:\n\"\"\"\n一台万得厨多少钱 您可以在京东平台上面查看价格哦\n\"\"\"\n2相似问和回答:\n\"\"\"\n怎么使用语音烹饪 您可以把食品放入万得厨内，然后对我说想要烹饪什么食品\n\"\"\"\n3相似问和回答:\n\"\"\"\n食品烹饪后没有熟怎么办 您可以根据情况使用万得厨继续烹饪哦\n\"\"\"\n4相似问和回答:\n\"\"\"\n万得厨怎么联网 您可以找一下万得厨的操作说明书进行了解哦\n\"\"\"\n5相似问和回答:\n\"\"\"\n购买万得厨会不会有额外的赠品？ 具体信息可以咨询销售人员哦\n\"\"\"\n6相似问和回答:\n\"\"\"\n万得厨最近有什么优惠活动吗 关于万得厨的详细情况请打客服电话4001106888进行了解哦\n\"\"\"\n… … （省略。测试用例中，总共拼接了60条相似知识）\n问题: 你知不知道万得厨是什么？\n将上面拼接后的问题，提供给GPT，接收答案 对于计算拼接的内容环节，可以做更多的事以实现更好的效果\n除了拼接相似度较高的知识库，还可以考虑从流行度、时间接近度、与先前搜索结果的冗余度、点击率等角度进行抽取知识 使用HyDE技术提高问答检索的性能 通过GPT提取问题和知识库的关键字、搜索词集、内容概要，以改善搜索结果 1.3. embedding测试示例 1.3.1. 让GPT回答关于公司内部的知识 这是使用内容团队14周输出的“服务助手”FAQ数据（不包含相似问数据，只包括标准问和回复话术的数据，共164条问答）和gpt-3.5-turbo模型，embedding后输出的部分对话实例结果：\n问\nGPT答\n问\nGPT答\n1\n你知不知道万得厨是什么？\n万得厨是一种智能厨房电器，可以通过语音控制或万得厨App完成食品的烹饪、解冻等功能。\n2\n你知不知道万得厨是什么？\n万得厨是一款智能厨房电器，可以通过语音控制和App远程控制，支持多种烹饪模式，适用于各种食材的烹饪和加热。\n3\n你知不知道万得厨是什么？\n万得厨是一款智能厨具，能够通过语音控制和扫码烹饪，具备多种烹饪模式和智能预判功能，为用户提供便捷的烹饪体验。\n4\n你知不知道万得厨是什么？\n万得厨是一款智能微波炉，可以通过语音指令、扫码等方式进行烹饪，具有智能预判和自动停止等功能。同时还拥有万得厨App提供的食谱、美食广场、商城等多种实用功能。\n5\n你知不知道万得厨是什么？\n万得厨是一款智能厨房电器，可以通过语音、扫码、触屏等方式进行烹饪，具有多种烹饪模式和智能控制功能。\n6\n购买万得厨食品有赠品吗？\n具体信息可以咨询销售人员哦。\n7\n啥是小孩？\n这个问题和万得厨无关，无法回答。\n8\n怎么购买万得厨？\n您可以在万得厨官网、京东、天猫等电商平台上进行购买，也可以到万得厨线下门店购买。具体购买方式可以咨询销售人员哦。\n9\n我想买一台万得厨，该怎么弄？\n您可以在京东等电商平台上搜索“万得厨”进行购买，或者到万得厨官网进行购买。在购买前可以咨询销售人员了解更多信息。\n10\n我想买一台万得厨，该怎么弄？\n您可以在京东等电商平台上搜索“万得厨”进行购买，或者到万得厨官网进行购买。在购买前，建议您先了解一下产品的详细信息和使用说明。\nGPT本身具备比较好的语义理解能力，并且让回答更有创意（通过调整api参数）。如，问了4次“你知不知道万得厨是什么？”，GPT每次都能给出不同的回复。\n2. fine-tuning更改模型 fine-tuning的功能是修改原模型。主要的应用领域在于修改GPT的回复语气、语言风格。比如你是一个情感辅导师，你有自己的说话方式、口头禅，这时你可以通过使用fine-tuning去修改模型，这样模型就可以模仿你的语言风格和口头禅去回复。\n这个技术的应用领域：\n分析类需求。如用户画像的聚类分析、异常监测、用户分类等 修改风格，或者回复句式。 … 理论上，fine-tuning和embedding都能实现模型用户风格、实现分析类需求、个性化，或者是让让GPT回答GPT不熟悉的主题的功能。但是两者的性能、成本和原理的差异让两者主要偏向于不同领域。\n（1）fine-tuning需要花费大量钱进行训练，不过训练完后，对话成本将比embedding小很多。\n训练中，fine-tuning训练需要耗费很多的钱，依据网上网友给出的案例，训练1000条数据，可能会花费4001000左右的人民币，如果训练的好，回复相关问题就不需要插入提示，所以对话的token就会比较少。embedding是在prompt中加入提示，不需要训练，但总体每次对话平均可能会花费30004000左右的token。用个不一定准确的比喻，原来100块钱能对话4k次，使用embedding后，对话降到2k次左右。对于固定的话式、固定的风格很适合fine-tuning，因为这样就可以不用每次都需要把 话式和风格 的prompt加到输入中，可以起到减少token、减少成本的作用。\n（2）一般，fine-tuning回答准确度比embedding回答准确度差。\nfine-tuning的技术原理主要是调整单词的权重，让某个词出现的概率更高。比如， GPT原来喜欢用首先、其次、再次这样的说法，通过数据fine-tuning GPT模型后，首先、其次、再次 这些词的权重降低，GPT再回复的时候就很少出现这些词了。所以从技术角度来说，让GPT回答正确万得厨的问题，fine-tuning的方式效果要比embedding差。因为针对比如“帮我推荐个食品“这样的问题，fine-tuning后，GPT依旧可能不使用万得厨生态的食品去回复用户（概率大，不代表一定会出现），而embedding可以指示该条回答必须用万得厨生态的食品去推荐，所以embedding回复的准确度会高很多。\n（3）fine-tuning回答效率比embedding高。\n在对话时，embedding比fine-tuning多了查询相关知识的一步，所以fine-tuning的延迟更低。\n（4）相关知识更多的时候，fine-tuning效果更好。\nembedding的使用会收到prompt的token数量的限制，即可能只能加2k字以内的提示。当需要的相关知识比较多，超过2k字时，要么通过切换更昂贵的模型，以较少token的限制，或者使用提取概要等缩减文本的技术减少知识的文本量。而fine-tuning不会受到字数的限制，所以当相关知识很多的时候，使用fine-tuning会更好\n所以：\nfine-tuning更适合于改进语气、风格类的，让每个问答都按这样的风格去回答 embedding更适合于提供准确答案 2.1. fine-tuning技术原理 fine-tuning的技术原理概括说主要是调整单词的权重，让某个词出现的概率更高。**通俗说，fine-tuning能够让GPT更偏向于，用我们期望的方式去回答。**偏向不等于一定。\nfine-tuning其实属于模型的二次训练。\n参考文献：\nopenai-cookbook/Question_answering_using_embeddings.ipynb at main · openai/openai-cookbook · GitHub\n嵌入 - OpenAI API\nJay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)\n如何使用OpenAI fine-tuning(微调)训练属于自己的专有模型？ - 知乎 (zhihu.com)\n微调 - OpenAI API\nFilter table dataCreate a pivot tableCreate a chart from data series\nConfigure buttons visibility\n",
  "wordCount" : "198",
  "inLanguage": "zh",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "王宇"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/wiki/gptembedding%E5%92%8Cfine-tuning%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "PaperMod",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">
                        
                    <img src="http://localhost:1313/images/msg_hu15231257772499651944.png" alt="" aria-label="logo"
                        height="20">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">主页</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      GPTembedding和fine-tuning技术解析
    </h1>
    <div class="post-description">
      唐玮
    </div>
    <div class="post-meta">1 分钟&nbsp;·&nbsp;王宇&nbsp;|&nbsp;<a href="https://github.com/WangShaoyu1/myBlogPaperMod/tree/master/content/posts/wiki/GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-embedding%e5%90%8egpt%e8%83%bd%e5%a4%9f%e5%9b%9e%e7%ad%94%e4%b8%87%e5%be%97%e5%8e%a8%e7%9b%b8%e5%85%b3%e9%97%ae%e9%a2%98%e5%b9%b6%e5%85%b7%e5%a4%87%e8%be%83%e5%a5%bd%e7%9a%84%e6%b3%9b%e5%8c%96%e8%83%bd%e5%8a%9b" aria-label="1. embedding后，GPT能够回答万得厨相关问题，并具备较好的泛化能力">1. embedding后，GPT能够回答万得厨相关问题，并具备较好的泛化能力</a><ul>
                        
                <li>
                    <a href="#11-embedding%e5%8f%af%e4%bb%a5%e5%ae%9e%e7%8e%b0%e7%9a%84%e5%8a%9f%e8%83%bd" aria-label="1.1. embedding可以实现的功能">1.1. embedding可以实现的功能</a></li>
                <li>
                    <a href="#12-embedding%e5%8e%9f%e7%90%86" aria-label="1.2. embedding原理">1.2. embedding原理</a></li>
                <li>
                    <a href="#13-embedding%e6%b5%8b%e8%af%95%e7%a4%ba%e4%be%8b" aria-label="1.3. embedding测试示例">1.3. embedding测试示例</a><ul>
                        
                <li>
                    <a href="#131-%e8%ae%a9gpt%e5%9b%9e%e7%ad%94%e5%85%b3%e4%ba%8e%e5%85%ac%e5%8f%b8%e5%86%85%e9%83%a8%e7%9a%84%e7%9f%a5%e8%af%86" aria-label="1.3.1. 让GPT回答关于公司内部的知识">1.3.1. 让GPT回答关于公司内部的知识</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#2-fine-tuning%e6%9b%b4%e6%94%b9%e6%a8%a1%e5%9e%8b" aria-label="2. fine-tuning更改模型">2. fine-tuning更改模型</a><ul>
                        
                <li>
                    <a href="#21-fine-tuning%e6%8a%80%e6%9c%af%e5%8e%9f%e7%90%86" aria-label="2.1. fine-tuning技术原理">2.1. fine-tuning技术原理</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>大模型（AGI）+embedding、fine-tuning（企业知识） = BGI（企业模型） 会成为很多AI应用系统的技术实现方式之一。目前所有基于GPT、或者类似GPT的语言大模型或者是AIGC类的图像大模型进行扩展的应用，基本都是基于embedding和fine-tuning两门技术实现的。例如replike、duolingo等。embedding、fine-tuning 两门技术实质上不会受到模态的限制。它可以在语言大模型、图像大模型、视频大模型等等不同模态的模型中使用。正如Jay Alammar 一个AI专家所说的：“万物皆可embedding”。</p>
<p>通常，基于大模型进行的应用开发会比从头开始构建模型要实用和方便得多（更少的数据，更好的效果）。从头构建模型，主要所需要涉及的数据量和计算量是非常庞大的一个数字。而大模型，因为其已经训练有通用的语义理解能力，所以基于大模型去训练企业的模型所需的数据量相比从头构建模型要少得很多。例如下面embedding测试GPT回答万得厨服务助手类问题的示例中，只需把标准问和标准答传给GPT，GPT即能准确识别不同问法的所真正要问的问题，并且能够产生多种不同的回答文本。而原来以知识库的技术方式是需要人工把相似问梳理出来，并且只能产生一种回答。</p>
<p>经过梳理，基于 ”语言大模型“+embedding、finr-tuning ，可以实现以下功能：</p>
<ul>
<li>让GPT回答GPT不熟悉的主题。如21年9月后的事件、非公开的文档、公司私有数据等</li>
<li>推荐相似的项目。如推荐相似的食谱</li>
<li>分析类需求。如用户画像的聚类分析、异常监测、用户分类等</li>
<li>个性化对话。如依据每个用户的用户画像和历史记录，按照用户喜欢的风格回答问题。</li>
<li>修改语言风格，或者回复句式。</li>
<li>&hellip; </li>
</ul>
<h1 id="1-embedding后gpt能够回答万得厨相关问题并具备较好的泛化能力">1. embedding后，GPT能够回答万得厨相关问题，并具备较好的泛化能力<a hidden class="anchor" aria-hidden="true" href="#1-embedding后gpt能够回答万得厨相关问题并具备较好的泛化能力">#</a></h1>
<h2 id="11-embedding可以实现的功能">1.1. embedding可以实现的功能<a hidden class="anchor" aria-hidden="true" href="#11-embedding可以实现的功能">#</a></h2>
<p>embedding技术，抽象来说是提供给了GPT一个短期的记忆。能让GPT基于短期记忆进行回复。短期记忆中除了与问题相关的知识，还可以补充该用户的画像信息、喜好等数据，以让GPT能够个性化的回复用户。同时结合GPT大模型提供的向量转化算法。embedding能够实现下面这些功能</p>
<ul>
<li><strong>让GPT回答GPT不熟悉的主题。如21年9月后的事件、非公开的文档、公司私有数据等</strong></li>
<li><strong>推荐相似的项目。如推荐相似的食谱</strong></li>
<li><strong>分析类需求。如用户画像的聚类分析、异常监测、用户分类等</strong></li>
<li><strong>个性化对话。如依据每个用户的用户画像和历史记录，按照用户喜欢的风格回答问题。</strong></li>
</ul>
<h2 id="12-embedding原理">1.2. embedding原理<a hidden class="anchor" aria-hidden="true" href="#12-embedding原理">#</a></h2>
<p>embedding原理其实很简单。即把公司的知识库以prompt的形式加入到用户的问句中（这些问题中即包含了问题的答案），chatGPT依据这个“知识库”+“用户问题” 组合而成的问题去进行回答。</p>
<p>因为知识库是庞大的，所以先需要通过 “找相似知识的算法” 去找到相似的知识库，即与提问相关的知识，然后GPT用这些知识来回答问题。</p>
<p><img loading="lazy" src="/download/attachments/101812168/image2023-4-21_11-16-40.png?version=1&amp;modificationDate=1682047000818&amp;api=v2" alt=""  />
</p>
<p>举个例子：</p>
<ul>
<li>用户输入“你知不知道万得厨是什么？”</li>
<li>随即系统计算这个问题与知识库中的哪些知识比较相关。通过计算与各个知识的余弦相似度，按相似度高低进行排序，根据设定条件提取出部分相关的知识</li>
</ul>
<p><img loading="lazy" src="/download/attachments/101812168/image2023-4-21_11-24-12.png?version=1&amp;modificationDate=1682047452957&amp;api=v2" alt=""  />
</p>
<ul>
<li>与用户输入进行拼接。下面是拼接的结果（因实际数据很多，这里省略了大部分数据）</li>
</ul>
<p>使用以下的FAQ来回答后续问题，如果在FAQ中找不到答案，请写下“我找不到一个答案”</p>
<p>0相似问和回答:<br>
&quot;&quot;&quot;<br>
万得厨可以烹饪不是新食记的预制菜嘛 如果是可微波且包装合适的食品，都可以用万得厨进行烹饪哟<br>
&quot;&quot;&quot;</p>
<p>1相似问和回答:<br>
&quot;&quot;&quot;<br>
一台万得厨多少钱 您可以在京东平台上面查看价格哦<br>
&quot;&quot;&quot;</p>
<p>2相似问和回答:<br>
&quot;&quot;&quot;<br>
怎么使用语音烹饪 您可以把食品放入万得厨内，然后对我说想要烹饪什么食品<br>
&quot;&quot;&quot;</p>
<p>3相似问和回答:<br>
&quot;&quot;&quot;<br>
食品烹饪后没有熟怎么办 您可以根据情况使用万得厨继续烹饪哦<br>
&quot;&quot;&quot;</p>
<p>4相似问和回答:<br>
&quot;&quot;&quot;<br>
万得厨怎么联网 您可以找一下万得厨的操作说明书进行了解哦<br>
&quot;&quot;&quot;</p>
<p>5相似问和回答:<br>
&quot;&quot;&quot;<br>
购买万得厨会不会有额外的赠品？ 具体信息可以咨询销售人员哦<br>
&quot;&quot;&quot;</p>
<p>6相似问和回答:<br>
&quot;&quot;&quot;<br>
万得厨最近有什么优惠活动吗 关于万得厨的详细情况请打客服电话4001106888进行了解哦<br>
&quot;&quot;&quot;</p>
<p><strong><em>&hellip; &hellip; （省略。测试用例中，总共拼接了60条相似知识）</em></strong></p>
<p>问题: 你知不知道万得厨是什么？</p>
<ul>
<li>将上面拼接后的问题，提供给GPT，接收答案</li>
</ul>
<p><img loading="lazy" src="/download/attachments/101812168/image2023-4-21_11-35-14.png?version=1&amp;modificationDate=1682048114573&amp;api=v2" alt=""  />
</p>
<p>对于计算拼接的内容环节，可以做更多的事以实现更好的效果</p>
<ul>
<li><strong>除了拼接相似度较高的知识库，还可以考虑从流行度、时间接近度、与先前搜索结果的冗余度、点击率等角度进行抽取知识</strong></li>
<li><strong>使用HyDE技术提高问答检索的性能</strong></li>
<li><strong>通过GPT提取问题和知识库的关键字、搜索词集、内容概要，以改善搜索结果</strong></li>
</ul>
<h2 id="13-embedding测试示例">1.3. embedding测试示例<a hidden class="anchor" aria-hidden="true" href="#13-embedding测试示例">#</a></h2>
<h3 id="131-让gpt回答关于公司内部的知识">1.3.1. 让GPT回答关于公司内部的知识<a hidden class="anchor" aria-hidden="true" href="#131-让gpt回答关于公司内部的知识">#</a></h3>
<p>这是使用内容团队14周输出的“服务助手”FAQ数据（不包含相似问数据，只包括标准问和回复话术的数据，共164条问答）和gpt-3.5-turbo模型，embedding后输出的部分对话实例结果：</p>
<p>问</p>
<p>GPT答</p>
<p>问</p>
<p>GPT答</p>
<p>1</p>
<p>你知不知道万得厨是什么？</p>
<p>万得厨是一种智能厨房电器，可以通过语音控制或万得厨App完成食品的烹饪、解冻等功能。</p>
<p>2</p>
<p>你知不知道万得厨是什么？</p>
<p>万得厨是一款智能厨房电器，可以通过语音控制和App远程控制，支持多种烹饪模式，适用于各种食材的烹饪和加热。</p>
<p>3</p>
<p>你知不知道万得厨是什么？</p>
<p>万得厨是一款智能厨具，能够通过语音控制和扫码烹饪，具备多种烹饪模式和智能预判功能，为用户提供便捷的烹饪体验。</p>
<p>4</p>
<p>你知不知道万得厨是什么？</p>
<p>万得厨是一款智能微波炉，可以通过语音指令、扫码等方式进行烹饪，具有智能预判和自动停止等功能。同时还拥有万得厨App提供的食谱、美食广场、商城等多种实用功能。</p>
<p>5</p>
<p>你知不知道万得厨是什么？</p>
<p>万得厨是一款智能厨房电器，可以通过语音、扫码、触屏等方式进行烹饪，具有多种烹饪模式和智能控制功能。</p>
<p>6</p>
<p>购买万得厨食品有赠品吗？</p>
<p>具体信息可以咨询销售人员哦。</p>
<p>7</p>
<p>啥是小孩？</p>
<p>这个问题和万得厨无关，无法回答。</p>
<p>8</p>
<p>怎么购买万得厨？</p>
<p>您可以在万得厨官网、京东、天猫等电商平台上进行购买，也可以到万得厨线下门店购买。具体购买方式可以咨询销售人员哦。</p>
<p>9</p>
<p>我想买一台万得厨，该怎么弄？</p>
<p>您可以在京东等电商平台上搜索“万得厨”进行购买，或者到万得厨官网进行购买。在购买前可以咨询销售人员了解更多信息。</p>
<p>10</p>
<p>我想买一台万得厨，该怎么弄？</p>
<p>您可以在京东等电商平台上搜索“万得厨”进行购买，或者到万得厨官网进行购买。在购买前，建议您先了解一下产品的详细信息和使用说明。</p>
<p><img loading="lazy" src="/download/attachments/101812168/image2023-4-21_9-6-13.png?version=1&amp;modificationDate=1682039173564&amp;api=v2" alt=""  />
</p>
<p>GPT本身具备比较好的语义理解能力，并且让回答更有创意（通过调整api参数）。如，问了4次“你知不知道万得厨是什么？”，GPT每次都能给出不同的回复。</p>
<h1 id="2-fine-tuning更改模型">2. fine-tuning更改模型<a hidden class="anchor" aria-hidden="true" href="#2-fine-tuning更改模型">#</a></h1>
<p>fine-tuning的功能是修改原模型。主要的应用领域在于修改GPT的回复语气、语言风格。比如你是一个情感辅导师，你有自己的说话方式、口头禅，这时你可以通过使用fine-tuning去修改模型，这样模型就可以模仿你的语言风格和口头禅去回复。</p>
<p>这个技术的应用领域：</p>
<ul>
<li><strong>分析类需求。如用户画像的聚类分析、异常监测、用户分类等</strong></li>
<li><strong>修改风格，或者回复句式。</strong></li>
<li><strong>&hellip;</strong></li>
</ul>
<p>理论上，fine-tuning和embedding都能实现模型用户风格、实现分析类需求、个性化，或者是让让GPT回答GPT不熟悉的主题的功能。但是两者的性能、成本和原理的差异让两者主要偏向于不同领域。</p>
<p><strong>（1）fine-tuning需要花费大量钱进行训练，不过训练完后，对话成本将比embedding小很多。</strong></p>
<p>训练中，fine-tuning训练需要耗费很多的钱，依据网上网友给出的案例，训练1000条数据，可能会花费400<del>1000左右的人民币，如果训练的好，回复相关问题就不需要插入提示，所以对话的token就会比较少。embedding是在prompt中加入提示，不需要训练，但总体每次对话平均可能会花费3000</del>4000左右的token。用个不一定准确的比喻，原来100块钱能对话4k次，使用embedding后，对话降到2k次左右。对于固定的话式、固定的风格很适合fine-tuning，因为这样就可以不用每次都需要把 话式和风格 的prompt加到输入中，可以起到减少token、减少成本的作用。</p>
<p><strong>（2）一般，fine-tuning回答准确度比embedding回答准确度差。</strong></p>
<p>fine-tuning的技术原理主要是调整单词的权重，让某个词出现的概率更高。比如， GPT原来喜欢用首先、其次、再次这样的说法，通过数据fine-tuning GPT模型后，首先、其次、再次 这些词的权重降低，GPT再回复的时候就很少出现这些词了。所以从技术角度来说，让GPT回答正确万得厨的问题，fine-tuning的方式效果要比embedding差。因为针对比如“帮我推荐个食品“这样的问题，fine-tuning后，GPT依旧可能不使用万得厨生态的食品去回复用户（概率大，不代表一定会出现），而embedding可以指示该条回答必须用万得厨生态的食品去推荐，所以embedding回复的准确度会高很多。</p>
<p><strong>（3）fine-tuning回答效率比embedding高。</strong></p>
<p>在对话时，embedding比fine-tuning多了查询相关知识的一步，所以fine-tuning的延迟更低。</p>
<p><strong>（4）相关知识更多的时候，fine-tuning效果更好。</strong></p>
<p>embedding的使用会收到prompt的token数量的限制，即可能只能加2k字以内的提示。当需要的相关知识比较多，超过2k字时，要么通过切换更昂贵的模型，以较少token的限制，或者使用提取概要等缩减文本的技术减少知识的文本量。而fine-tuning不会受到字数的限制，所以当相关知识很多的时候，使用fine-tuning会更好</p>
<p>所以：</p>
<ul>
<li>fine-tuning更适合于改进语气、风格类的，让每个问答都按这样的风格去回答</li>
<li>embedding更适合于提供准确答案</li>
</ul>
<h2 id="21-fine-tuning技术原理">2.1. fine-tuning技术原理<a hidden class="anchor" aria-hidden="true" href="#21-fine-tuning技术原理">#</a></h2>
<p>fine-tuning的技术原理概括说主要是调整单词的权重，让某个词出现的概率更高。**通俗说，fine-tuning能够让GPT更偏向于，用我们期望的方式去回答。**偏向不等于一定。</p>
<p>fine-tuning其实属于模型的二次训练。</p>
<p>参考文献：</p>
<p><a href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb">openai-cookbook/Question_answering_using_embeddings.ipynb at main · openai/openai-cookbook · GitHub</a></p>
<p><a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">嵌入 - OpenAI API</a></p>
<p><a href="https://jalammar.github.io/">Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></p>
<p><a href="https://www.zhihu.com/question/591066880">如何使用OpenAI fine-tuning(微调)训练属于自己的专有模型？ - 知乎 (zhihu.com)</a></p>
<p><a href="https://platform.openai.com/docs/guides/fine-tuning">微调 - OpenAI API</a></p>
<p><a href="/">Filter table data</a><a href="/">Create a pivot table</a><a href="/">Create a chart from data series</a></p>
<p><a href="/users/tfac-settings.action">Configure buttons visibility</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E5%94%90%E7%8E%AE/">唐玮</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/wiki/%E6%83%85%E6%84%9F%E9%A1%BE%E9%97%AE%E8%A1%8C%E4%B8%9A%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/">
    <span class="title">« 上一页</span>
    <br>
    <span>情感顾问行业调研报告</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/wiki/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E4%B8%8E%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E6%94%BE%E4%BB%BB/">
    <span class="title">下一页 »</span>
    <br>
    <span>知识图谱与复杂度的放任</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on x"
            href="https://x.com/intent/tweet/?text=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f&amp;hashtags=%e5%94%90%e7%8e%ae">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f&amp;title=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90&amp;summary=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f&title=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on whatsapp"
            href="https://api.whatsapp.com/send?text=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on telegram"
            href="https://telegram.me/share/url?text=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share GPTembedding和fine-tuning技术解析 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=GPTembedding%e5%92%8cfine-tuning%e6%8a%80%e6%9c%af%e8%a7%a3%e6%9e%90&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2fgptembedding%25E5%2592%258Cfine-tuning%25E6%258A%2580%25E6%259C%25AF%25E8%25A7%25A3%25E6%259E%2590%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://beian.miit.gov.cn/">粤ICP备2023039897号-1</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
