<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>大模型知识问答实现技术调研 | PaperMod</title>
<meta name="keywords" content="黄婷">
<meta name="description" content="黄婷">
<meta name="author" content="王宇">
<link rel="canonical" href="http://localhost:1313/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/">
<meta name="google-site-verification" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7b92bf4867c997b58ec50f63451b83777173d5bd5593376852abd85746d09d71.css" integrity="sha256-e5K/SGfJl7WOxQ9jRRuDd3Fz1b1VkzdoUqvYV0bQnXE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="大模型知识问答实现技术调研" />
<meta property="og:description" content="黄婷" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="posts" />




<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="大模型知识问答实现技术调研"/>
<meta name="twitter:description" content="黄婷"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "大模型知识问答实现技术调研",
      "item": "http://localhost:1313/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大模型知识问答实现技术调研",
  "name": "大模型知识问答实现技术调研",
  "description": "黄婷",
  "keywords": [
    "黄婷"
  ],
  "articleBody": " 11. 概述 22. 向量化知识库的构建 2.12.1. 文档拆分 2.1.12.1.1. 拆分片段的方法 2.1.22.1.2. 模型对片段长度的支持 2.22.2. 向量化模型选择 2.32.3. 向量化并行 2.3.12.3.1. 文档切分并行 2.3.22.3.2. 向量化推理并行 2.3.32.3.3. 批量写入向量数据库 33. 知识召回 3.13.1. 倒排召回 3.1.13.1.1. 基本原理 3.1.23.1.2. 常见问题 \u0026 优化措施 3.23.2. 向量召回 3.2.13.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？ 3.2.23.2.2. 知识增强优化召回 3.2.33.2.3. 向量模型的演进与选型 3.2.43.2.4. Finetune 向量模型是否有收益？ 3.33.3. 倒排召回 \u0026 向量召回的优劣势对比 3.43.4. 模型召回结果的重排序 44. 实践中存在的问题和难点 4.14.1. 常见问题 4.24.2. 难点及经验 4.2.14.2.1. 文档切分 4.2.24.2.2. 文档处理 4.2.34.2.3. 文档召回 4.2.44.2.4. Self-RAG 4.2.54.2.5. 拒绝回复 55. 参考文档 1. 概述 当前大语言模型应用中被广泛关注的技术路线是大模型（LLM）+知识召回（Knowledge Retrieval）的方式，这种方式也被称为RAG（Retrieval Augmented Generation, 检索增强生成），其基本思路是把私域知识文档进行切片，然后向量化，再通过检索召回，最终作为上下文输入到大语言模型进行归纳总结。RAG 方法使得开发者不必为每一个特定的任务重新训练整个大模型，只需要外挂上知识库，即可为模型提供额外的信息输入，提高其回答的准确性。\n在 LLM 已经具备了较强能力的基础上，仍然需要 RAG ，主要有以下几点原因：\n**幻觉问题：**LLM 文本生成的底层原理是基于概率的，因此会不可避免地产生“一本正经的胡说八道”的情况。 **时效性问题：**LLM 的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题，例如“帮我推荐几部热映的电影？”。 **数据安全问题：**通用的 LLM 没有企业内部数据和用户数据，那么企业想要在保证安全的前提下使用 LLM，最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成，而在线的大模型仅仅完成一个归纳总结的功能。 2. 向量化知识库的构建 在这个技术方向的具体实践中， 知识库可以采取基于倒排和基于向量的两种索引方式进行构建，它对于知识问答流程中的知识召回这步起关键作用，和普通的文档索引或日志索引不同，知识的向量化需要借助深度模型的语义化能力，存在文档切分，向量模型部署\u0026推理等额外步骤。知识向量化建库过程中，不仅仅需要考虑原始的文档量级，还需要考虑切分粒度，向量维度等因素，最终被向量数据库索引的知识条数可能达到一个非常大的量级，可能由以下两方面的原因引起：\n各个行业的既有文档量很高，如金融、医药、法律领域等，新增量也很大。 为了召回效果的追求，对文档的切分常常会采用按句或者按段进行多粒度的冗余存贮。 2.1. 文档拆分 知识向量化的前置步骤是进行知识的拆分，**语义完整性的保持是最重要的考量。**基于亚马逊云科技的服务，总结了以下两个方面的经验：\n2.1.1. 拆分片段的方法 关于这部分的工作，Langchain 作为一种流行的大语言模型集成框架，提供了非常多的 Document Loader（文档加载器，用于解析不同格式的文档）和 Text Spiltters（文本分割器，用于文档拆分），其中的一些实现具有借鉴意义。\n目前使用较多的基础方式是采用 Langchain 中的 RecursiveCharacterTextSplitter，属于是 Langchain 的默认拆分器。它采用多级分隔字符列表 [“\\n\\n”， “\\n”， ” “， “”] 来进行拆分，默认先按照段落做拆分，如果拆分结果的 chunk_size （文档块大小，其字符数受限于向量化模型）超出，再继续利用下一级分隔字符继续拆分，直到满足 chunk_size 的要求。\n但这种做法相对来说还是比较粗糙，可能会造成一些关键内容被拆开。对于一些特定的文档格式可以有一些更细致的做法。\nFAQ 文件，必须按照一问一答粒度拆分，后续向量化的输入可以仅仅使用问题，也可以使用问题+答案 Markdown 文件，”#”是用于标识标题的特殊字符，可以采用 MarkdownHeaderTextSplitter 作为分割器，它能更好的保证内容和对应的标题被提取出来。 PDF 文件，会包含更丰富的格式信息。Langchain 里面提供了非常多的 Loader，但 Langchain 中的 PDFMinerPDFasHTMLLoader 的切分效果上会更好，它把 PDF 转换成 HTML，通过 HTML 的块进行切分，这种方式能保留每个块的字号信息，从而可以推导出每块内容的隶属关系，把一个段落的标题和上一级父标题关联上，使得信息更加完整。类似下面这种效果： 2.1.2. 模型对片段长度的支持 由于拆分的片段后续需要通过向量化模型进行推理，所以必须考虑向量化模型的 Max_seq_length 的限制，超出这个限制可能会出现截断，导致语义不完整。从支持的 Max_seq_length 来划分，目前主要有两类 Embedding 模型，如下表所示（这四个是亚马逊有过实践经验的模型）。\n这里的 Max_seq_length 是指 Token 数，和字符数并不等价。依据之前的测试经验，前三个模型一个 token 约为 1.5 个汉字字符左右。而对于大语言模型，如 chatglm，一个 token 一般为 2 个字符左右。如果在切分时不方便计算 token 数，也可以简单按照这个比例来换算，保证不出现截断的情况。\n前三个模型是基于 Bert 的 Embedding 模型，OpenAI 的 text-embedding-ada-002 模型是基于 GPT3 的模型。前者适合句或者短段落的向量化，后者 OpenAI 的 SAAS 化接口，适合长文本的向量化，但不能私有化部署。\n可以根据召回效果进行验证选择。从目前的实践经验上看 text-embedding-ada-002 对于中文的相似性打分排序性可以，但区分度不够（集中 0.7 左右），不太利于直接通过阈值判断是否有相似知识召回。\n另外，对于长度限制的问题也有另外一种改善方法，可以对拆分的片段进行编号，相邻的片段编号也临近，当召回其中一个片段时，可以通过向量数据库的 range search 把附近的片段也召回，可在一定程度上保证召回内容的语意完整性。\n2.2. 向量化模型选择 上面四个模型只是提到了模型对于文本长度的支持差异，效果方面目前并没有非常权威的结论。可以通过 leaderboard 来了解各个模型的性能，榜上的大多数模型的评测还是基于公开数据集的 benchmark，对于真实生产中的场景， benchmark 结论是否成立还需要 case by case 地来看。但原则上有以下几方面的经验可以分享：\n经过垂直领域 Finetune 的模型比原始向量模型有明显的优势。 目前的向量化模型分为两类，对称和非对称。未进行微调的情况下，对于 FAQ 建议走对称召回，也就是 Query 到 Question 的召回。对于文档片段知识，建议使用非对称召回模型，也就是 Query 到 Answer（文档片段）的召回。 没有效果上的明显差异的情况下，尽量选择向量维度短的模型，高维向量会给向量数据库造成检索性能和成本两方面的压力。 2.3. 向量化并行 真实的业务场景中，文档的规模在百到百万这个数量级之间。按照冗余的多级召回方式，对应的知识条目最高可能达到亿的规模。由于整个离线计算的规模很大，所以必须并发进行，否则无法满足知识新增和向量检索效果迭代的要求。\n2.3.1. 文档切分并行 计算的并发粒度是文件级别的，处理的文件格式也是多样的，如 TXT 纯文本，Markdown， PDF 等，其对应的切分逻辑也有差异。而使用 Spark 这种大数据框架来并行处理过重，并不合适。使用多核实例进行多进程并发处理则过于原始，任务的观测追踪上不太方便。具体使用哪种方法还需进一步考虑。\n2.3.2. 向量化推理并行 由于切分的段落和句子相对于文档数量膨胀了很多倍，向量模型的推理吞吐能力决定了整个流程的吞吐能力。一般来说可以采用如下几种策略：\nGPU 实例部署：向量化模型 CPU 实例是可以推理的。但离线场景下，推理并发度高，GPU 相对于 CPU 可以达到 20 倍左右的吞吐量提升。所以离线场景可以采用 GPU 推理，在线场景 CPU 推理的策略。 多节点 Endpoint：对于临时的大并发向量生成，通过部署多节点 Endpoint 进行处理，处理完毕后可以关闭。 2.3.3. 批量写入向量数据库 ES数据库可以通过 bulk 批量写入，比单条写入有很大的优势。\n3. 知识召回 知识召回在基于大语言模型的知识问答中是非常关键的步骤，它决定了大语言模型的输入，对后续回答的可靠性以及回复质量影响非常大。目前基于语义向量召回是用的比较多的方式，但在实际的生产实践中，倒排的召回方式也非常实用，它具备精确匹配、索引效率和可解释的优势。在目前的全文检索系统中，基于倒排的 BM25 相关性打分依然是核心机制。当然随着近些年深度学习的发展，语义召回表现了强大的检索效果，这两路召回在不同场景下都具备自己独立的优势，在知识召回中的互补性很强。\n3.1. 倒排召回 这种方式基于关键词进行召回，比较简单易用，不要求技术人员有算法知识背景。这种召回策略在很多场景下非常有效，特别是一些对领域专词非常敏感的场景。但同时它的缺点在于，与搜索引擎不同，在对话机器人的交互形态下，用户的输入更多表现为完整的句子而不是搜索关键词，这时候倒排召回中的 BM25 打分逻辑对语义信息的捕捉能力弱的问题比较明显，所以一般我们建议把它作为向量召回的一种必要补充。\n3.1.1. 基本原理 倒排索引（Inverted index）作为一种广泛使用的索引方式，也常被称为反向索引、置入档案或反向档案，它是文档检索系统中最常用的数据结构。离线索引构建时，通过分词器对文档进行切词，得到一系列的关键词（Term）集合，然后以 Term 为 key 构建它与相关文档的映射关系。在线搜索流程中，首先通过切词器对用户输入进行切词，得到 Term 列表，然后根据如下 BM25 打分公式进行打分排序返回。\n上述公式中：\n——BM25(D，Q)：文档D对于查询Q的BM25得分。\n——n：查询中的关键词数\n——qi：查询中的第i个关键词\n——idf(qi)：查询中第i个关键词的逆文档频率（IDF = log(语料库的文档总数/(包含该词的文档数+1))）\n——f(qi，D)：查询中第i个关键词在文档D中的频率（TF = 该词在文档中出现的次数/文档的总词数）\n——|D|：文档D的长度（词数）\n——avgdl：所有文档的平均长度\n——b：自由参数\n——k1：自由参数\nk1 与 b 是可以自由调节的参数，其中 k1 默认值为 1.2，会影响词语在文档中出现的次数对于得分的重要性，比如希望搜索词在文档中出现越多则越接近想要的内容，那么可以将这个参数调大一点。b 默认 0.75，它能控制文档长度对于分数的惩罚力度，如果不希望文档长度更大的相似度搜索更好，可以把 b 设置得更小，如果设置为 0，文档的长度将与分数无关。\n3.1.2. 常见问题 \u0026 优化措施 在垂直领域的 FAQ 知识语料的倒排检索实践中，存在两类常见的得分偏差：\na.私有领域文档的 IDF 失真问题\n由于 BM25 打分的 IDF 是基于索引内的所有的文档统计得到的，在一些领域内场景，所有的文档的总量并不大，有些专有词汇在全部文档中的出现频率和一些常见的停用词是差不多的，比如在游戏场景中”专属技能”与”怎么”这两个词的 IDF 是差不多的。后续在打分的时候，对于这两个词不能反映真实的重要性，这就是所谓的 IDF 失真的问题。解决这个问题通常有以下几种方法：\n导入自定义的词频数据（需要搜索引擎的功能支持） 导入一些其他来源的文本语料，这些数据仅仅是为了调整 IDF，使它更加接近真实分布。由于一般这种情况下数据总量不大，加入一些无关的数据并不太会改变集群的查询速度，但需要注意的是在搜索时需要通过字段标记过滤掉这些假数据。 录入停用词，停用词不参与 BM25 的得分计算，一般停用词表为每个词单独一行的纯文本文件 b.无关键词匹配\n这种是倒排召回缺乏语义信息的典型现象，现实中多词一义是非常普遍的。比如在医疗健康场景，一个用户用”症状”来查询与 COVID-19 相关的文档，但一些十分相关的文档中出现的是“病症”、“临床表现”这样的词。除了向量检索能改善这个问题以外，在非常垂直的领域，比如说某个游戏场景中，经常会存在一些专有领域的“黑话”，向量模型的效果可能不见的好，这种情况下可以采用同义词表去枚举这些同义词。构建同义词表时，每行为多个同义词（≥2）用逗号分隔。\n举下面这个例子说明，这里的盾和护盾是场景内的同义词，在不配置同义词表的情况下，用户输入中的关键词是难以匹配的。\nUser Query: “秘密武器为什么不能开盾”\nDoc: “问题: 秘密武器在危险地带提示不能开启护盾，是正常的吗？\n回答: 您好，秘密武器的护盾功能与游戏本身限制相同。游戏内如危险地带等本身无法开启护盾的地点，秘密武器的护盾也无法开启。是游戏的正常设定。”\n3.2. 向量召回 3.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？ 对称召回，一般是指通过两个句子的相似性来召回，比如输入的 query 匹配相似的另一个 question(query-\u003equestion)，也可以是输入的陈述句匹配相似的另一个陈述句。对应这种数据也叫做 NLI 数据，一般是三元组的形式，比较知名的数据集为 SimCLUE，如：\n{“query”: “胡子长得太快怎么办？”, “title”: “胡子长得快怎么办？”, “neg_title”: “怎样使胡子不浓密！”}\n{“query”: “在香港哪里买手表好”, “title”: “香港买手表哪里好”, “neg_title”: “在杭州手机到哪里买”}\n非对称召回一般是检索数据， 通常是一个相对短的问句和一个相对长的答案（一般为一个文本段落）, 比较知名的数据集为 MSMARCO，一些 Question Answering 类的任务都是基于这种数据集，如：\n{“query”: “walgreens store sales average”, “passage_text”: [ “The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager…”,]}\n一般来说，对称召回任务比非对称召回任务要简单，特别在很多垂直领域，向量模型对领域知识的理解有限，对称召回很多时候理解了字面意思就能满足要求。为了方便描述，下文把对称召回称为 QQ（Query-Question）召回，把非对称召回称为 QD（Query-Document）召回。\n在真实的生产场景中，知识的存在形式有两种 —— FAQ 和文本文档， FAQ 由于是一问一答的形式，天然是比较适合进行 QQ 召回，而文本文档比较适应 QD 召回。但在实际的 case 中，单独 QQ 召回或者 QD 召回都存在一定的问题。相对于 QQ 召回，QD 召回需要 embedding 模型对语义有更强的理解。Question 一般相对比较简短，信息量没有 Document 那么多，因此对于只是使用 QQ 召回，可能会有问题，如下面的例子：\nQuestion: AWS Clean Rooms 的数据源必须在 AWS 上么？\nAnswer: 对，目前必须在 AWS 上，而且必须是同一个 region。\n用户输入的问题不一定会和 FAQ 的 Question 的提问角度一致，比如用户可能会问“AWS Clean Rooms 的数据源不在同一个 region，行吗？”，这种问题仅仅考虑 QQ 则会出现问题。\n所以生产实践中，推荐同时考虑 QQ 和 QD 召回，对于 FAQ 的知识形态，对 Question 生成向量作为索引，同时把 Answer 作为 Doc 生成向量作为索引，在召回时同时走 QQ 和 QD 召回。对于文本文档的知识形态，除了对 Doc 生成向量以外，还可以通过知识增强的方式生成多组 Question。\n3.2.2. 知识增强优化召回 知识增强一般是指，根据上下文重新 Refine 原始的文本使得原始的文本信息更完整，自解释性更强。这个过程由于是离线的一次性处理，所以这里可以借助能力更强的商用 LLM 模型，生成的内容可能会更好。另外由于现在的 LLM 能支撑的上下文 context 长度很长，建议在生成的过程中，在 prompt 中去融入一些该场景的丰富背景信息。\n具体的操作方式，可以参考下图：\n通过知识增强，可以把文本文档也转化成 FAQ 的形式，使得对于一般的文档也能够同时进行 QQ 和 QD 召回。但这里需要注意的是，对于特别垂直的领域，能力很强的商业 LLM 也可能无法提取出比较合适的问题，一般观察到的现象是提取出的问题过于直接，或者提取出的问题无实际意义，这部分建议走一遍人工审核。\n对于质量不高的 FAQ，特别是提问过于简略的，知识增强也能够优化其质量。另外经过优化后的知识，在 LLM 生成阶段，对于提升 LLM 对知识的理解也是有一定帮助的。总的来说，知识增强是简单有效的优化手段。\n除了以上通过给文档块增加额外的问答信息增强向量召回，还有很多其他的召回优化措施：\n**层次索引：**创建两个索引，一个由摘要组成，另一个由文档块组成，并且分两步搜索，首先通过摘要过滤出相关文档，然后在相关组内搜索。\n**HyDE：**使用大模型生成一个伪答案，或生成更标准的 prompt 模板，然后去检索文档 chunk。这个方法的口碑很好，用的应该也比较多，但是存在的问题是受限于常见大模型的能力，有可能会产生副作用。\n**利用上下文信息检索：**chunk 窗口检索（为了在获取最相关的单个句子后更好地推理找到上下文，可以把检索到的句子作为基准，将上下文窗口扩展 k 个句子，然后将此扩展上下文一起发送给 LLM）；多层次 Child / Parent 文档检索（首先分 chunks 的时候拆分成子 chunk，然后不同的子 chunk 与同一父 chunk 构建关联索引；检索时，先在子 chunks 堆里寻找 topk 相关的，然后在不同子 chunks 与不同的父 chunk 关联下，把所有相关的父 chunk 都找出来，最后统一丢给 LLM 进行组合问答）。\n**RAG-Fusion：**当接收用户 query 时，让大模型生成 5-10 个相似的 query，然后每个 query 去匹配 5-10 个文本块，接着对所有返回的文本块再做个倒序融合排序，如果有需求就再加个精排，最后取 topk 个文本块拼接至 prompt。这个方法的主要好处，是增加了相关文本块的召回率，同时对用户的 query 自动进行了文本纠错、分解长句等功能。\n**检索时添加一些特殊符号：**增加查询语句的长度（比如在ID、哈希码或产品名称后面加上一些描述性的词语，或者使用一些常见的问题作为查询语句，这样可以增加查询语句的语义信息，提高向量检索的效果）；使用一些特殊的符号或标记（比如在ID、哈希码或产品名称前后加上双引号，或者使用一些特定的字段名，这样可以告诉向量检索系统，这些词语是需要精确匹配的，而不是基于相似度的）；结合关键词检索（比如在向量检索的结果中，再使用关键词检索的方法，对查询语句和文档进行文本匹配和过滤，这样可以排除一些不相关的内容，提高检索的准确性）。\n**混合检索：**多路召回，通过向量化召回一些，通过 BM25 或者 TFIDF 也召回一些 CHUNK，然后组合丢给 LLM。\n3.2.3. 向量模型的演进与选型 首先需要强调的是向量模型与 LLM 模型是完全不同的，两者的训练目标不同，LLM 模型的最终目标是生成。向量模型的最终目标是判别，判断 QQ 之间语义是否相似，判断 QD 之间是否具有相关性。两者的训练目标和训练数据是有不小的区别的。一般都不直接利用 LLM 来生成本文向量。\n向量模型的大概演进路线，可以参考下图：\n从上图可以观察到几个基本趋势，目前的向量模型从单纯的基于 NLI 数据集（对称数据集）发展到基于混合数据（对称+非对称）进行训练，即可以做 QQ 召回任务也能够做 QD 召回任务，通过添加 Instruction 的方式来区分这两类任务（在进行 QD 召回的时候，需要在用户 query 中添加 Instruction 前缀）。在模型的第一 pretrain 阶段，从对比学习发展到 RetroMAE 自动编码的方法。后续的监督训练阶段差异不大，主要是监督数据的区别。\n在亚马逊之前的实践中，采用的模型包括 sentense-bert 系列，text2vec，m3e ，这些模型在公开数据集上的 benchmark 在垂直领域不一定成立，或者说体现不出明显的优势。所以他们采用游戏场景中的 2w+ 知识增强过的 FAQ，拆分出训练数据和测试数据进行评测（详细评测信息：基于大语言模型知识问答应用落地实践 – 知识召回调优（下））。\n模型层面，BGE 向量模型作为最新的 SOTA 模型，在各类数据集上都表现良好。所以他们选择 sentence-bert 的多语言模型 paraphrase-multilingual-mpnet-base-v2 与 bge-large-zh-v1.5 进行对比。\n召回形式层面，分别对 QQ 和 QD 两种召回形式进行评测，得到如下的结论：\na. 模型层面\n在没有微调的情况下，bge-large-zh 与 sbert 相比，无论从召回率上还是相似度的值域区分性来看，bge 都有明显优势。 b. 召回形式\n相似度值域范围在 QQ 和 QD 之间是有区别的，两者分布的 kde 曲线（直方图上的曲线）交点位置有差异，召回时可分别考虑 QQ 和 QD 的截断阈值。另外 QD 召回的负例相似度值域上限要低于 QQ 召回的，所以高分的 QD 召回内容是置信度更高的知识。 QQ 召回场景中，原始 Q 的召回效果是明显低于增强 Q 的，其top2召回准确率低，且负例的相似度值域分布在 0-1 的整个空间。 其背后的原因主要是由于数据集中原始的 Q 十分简短，且很多都是相似的概念名词。在这种情况下，过于简短的原始 Q（1-2 词）不该生成向量被索引，可考虑去掉这类知识仅保留增强后的 Q。 QD 场景中，增强 Q 和增强 D 之间的top2召回率和相似分区分度都是最理想的，但是由于它是 LLM 生成的，本身存在幻觉的情况，另外如果增强 D 和原始 D 重复性过高，可能导致召回了过多重复的内容挤占了其他内容的空间。建议结合人审或者其他方式保证这部分知识的质量。 3.2.4. Finetune 向量模型是否有收益？ 在一些专业领域，通用的向量模型可能无法很好的理解一些专有词汇，导致检索性能不足, 且正例和负例相似度的值域有比较多的重叠，无法通过阈值判断该召回内容是否属于相关信息。过多的无关信息可能会在下游误导大语言模型，如果能在上游做截断或者通过 Prompt 暗示 LLM 可能没有相关信息，则会大大减少 LLM 幻觉的问题，实现更好的拒答。\n所以向量模型 Finetune 的主要目标一方面是为了提高 topk 召回的准确率，另外一方面想要拉开正例和负例的 similarity 值域分布，从而得到更好的分隔阈值，使得当召回的 similarity 值低于阈值时，直接放弃这些无效的召回，提示下游进行拒答。\n亚马逊在实践中对 bge-large-zh-v1.5 模型进行 Finetune 以及测试评估，得到了如下的结论（详细评测信息：基于大语言模型知识问答应用落地实践 – 知识召回调优（下））：\n微调对于训练集有明显提升效果，无论 QQ 还是 QD 召回，top2 召回率或者正负样例的值域分布上都会更好。在不考虑泛化性的情况下，假设能持续收集真实的用户反馈不断纳入训练集，最终也能覆盖绝大多数用户的提问场景。 微调在测试集上表现出一定的泛化性，top2 召回率在 QD 召回上提升 9pp 至 87.2%，QQ 召回上提升 1.2pp 至 85.8%，相似度值域从分布上看区分度更好。 注意：KDE 交点并不能直接作为召回是否有效的阈值，因为微调数据是基于知识的一问一答对训练出来的，和真实的用户输入在分布上还是有很大的区别，在实践中，对应的阈值需要多次尝试确定，一般要比 KDE 交点要高。本质上需要不断收集越来越多的用户真实的正负反馈用于调优向量召回。\n3.3. 倒排召回 \u0026 向量召回的优劣势对比 对比项\n倒排召回\n向量召回\n对比项\n倒排召回\n向量召回\n优势\n发展成熟，易达到非常好的 baseline 性能\n检索速度更快\n可解释能力强\n精确匹配能力强\n支持自定义停用词表，同义词表\n考虑语义相似性，更加智能\n语义相近即可召回，无需关键词命中\n无需考虑复杂的传统倒排的调优手段\n具备支持跨模态召回的潜力\n劣势\n没有语义信息，对”一词多义”现象解决的不好\n关键字不匹配，用户在搜索时并不知道对应文档中准确的关键词，需要通过同义词扩充，query 改写来解决\n语义偏移问题，虽然关键词字面上匹配，但是命中顺序和用户输入不一样，语义上可能完全不相关\n需要模型训练，对垂直领域落地支持有限\n预训练模型对于公开数据集的 benchmark 参考性不足。对垂直领域泛化性不足，可能出现以下情况：不理解专有词汇；容易出现语义相似但主题不相似的情况\n对精准匹配支持不足，难以用专业词汇精准召回\n对”多词一义”情况的支持不如倒排召回中的同义词表简单直接\n可解释能力弱\n需要更多的计算资源\n3.4. 模型召回结果的重排序 在传统的搜索场景中，如何结合这两路召回的结果是一个问题，因为两者不在一个评分体系下。**在基于大语言模型的知识问答机器人场景， 可以简单采用倒排召回和向量召回各 topk 去重取并集的方式。**主要是因为大语言模型对召回内容的相关性排序不敏感，且能够容忍一些不相关的知识召回内容。\nElasticSearch 在使用 KNN+BM25 检索的混合检索分值 Score 计算公式是：knn_score * boost + bm25_score * boost\n对于混合检索，我们在算法层面有了直接的了解后，最终在产品层面会影响一些设计。\n混合检索的权重设置：在上面的 score 分值计算公式中，可以看到 boost 会影响最终的内容，因为并不是所有的客户和业务场景都适合 knn 检索（向量检索），可能在其他关键的场景中，关键词检索会更适合。我们在产品设计中则可以根据不同的客户诉求以及业务诉求，设置不同的 boost 来调节召回结果，从而改善我们的产品效果。 在 BM25 算法的场景中，分词是非常重要的一个特性，对于不同的行业客户，词库的收集建立对于产品应用的提升肯定是会有质的提升，也是每个公司做 RAG 产品的核心竞争力。 另外一个思路是再训练一个排序模型的来进一步打分，这种方式可以更好的结合倒排和向量召回的结果。比如两路召回各生成了10个候选，但最终用于构建 Prompt 的只需要 4 个，那么通过 Rerank（重排序） 就能很好的达到目的并科学的结合两路结果。而且也可以不断的把线上用户的真实反馈信息作为训练数据不断迭代效果。\n由于在 Rerank 阶段，所有的候选都是模型召回的 topk，所以在训练的时候正负例的选择范围也应该在召回的 topk 中。微调阶段使用的随机负采样的训练数据是不合适的，建议以上游打分 topk 作为训练数据。可以先挖掘难样本然后把这部分作为排序模型的训练数据。一般情况会带来一定的正向提升，特别是结合真实的用户反馈数据。\n4. 实践中存在的问题和难点 4.1. 常见问题 内容缺失——这是生产案例中最大的问题之一。用户假设特定问题的答案存在于知识库中，但有时并非如此，系统也没有兜底回应。相反，它提供了一个看似合理的错误答案，但实际是“毫无意义”。\n漏掉排名靠前的文档——检索器是小型搜索系统，要获得正确的结果并不简单，简单的向量检索很少能达到目的。有时，检索器返回的前 K 个文档中不存在正确答案，从而导致失败。\n上下文受限—— RAG 系统可能会检索到太多文档，若强制根据上下文分割并输入文档。问题的答案可能不在上下文中，这可能会导致模型产生幻觉。\n未提取到有用信息——当 LLM 无法从上下文中提取答案时，可能会感到困惑，不同大模型对背景信息的理解能力参差不齐。\n格式错误——这种需要特定格式的输出，需要进行大量的系统提示和指令微调，以生成特定格式的信息。\n不合适的回答——响应中返回答案，但不够具体或过于具体，无法满足用户的需求。当 RAG 系统设计者对给定问题有期望的结果时，就会发生这种情况。在这种情况下，应该提供具体的提示内容和答案，而不仅仅是答案。当用户不确定如何提出问题并且过于笼统时，也会出现不正确的回答。\n4.2. 难点及经验 4.2.1. 文档切分 如何将文档拆分为文本片段。主要有两种，一种就是基于策略规则，另外一种是基于算法模型。\n**如何保证文档切片不会造成相关内容的丢失？**一般而言，文本分割如果按照字符长度进行分割，这是最简单的方式，但会带来很多问题。例如，如果文本是一段代码，一个函数被分割到两段之后就成了没有意义的字符。因此，我们也通常会使用特定的分隔符进行切分，如句号，换行符，问号等。可以使用专门的模型去切分文本，尽量保证一个 chunk 的语义是完整的，且长度不会超过限制。 **文档切片的大小如何控制？**太小则容易造成信息丢失，太大则不利于向量检索命中。此外还要考虑 LLM context 长度的限制，常见大模型支持的上下文窗口为 4096 个 token，这意味着输入 token 和生成的输出 token 的总和不能超过 4096，否则会出错。为了保证不超过这个限制，可以预留约 2000 个 token 作为输入信息，留下约 2000 个 token 作为输入提示和返回消息。这样，如果提取出了五个相关信息块，那么每个片的大小不应超过 400 个 token。 基于算法模型，主要是使用类似 BERT 结构的语义段落分割模型，能够较好的对段落进行切割，并获取尽量完整的上下文语义。该方法能解决一部分问题，但效果有限，若想要达到较优的效果，需要微调，上手难度高。而且切分出的段落有可能大于向量模型所支持的长度，这样就还需要进行切分或提取关键信息。 常见的分段方式是在固定max_length的基础上，对出现。/；/？/....../\\n等等地方进行切割。但这种方式显然比较武断，面对特殊情况需要进一步优化。比如1.xxx, 2.xxx, ..., 10.xxx超长内容的情况，直接按这种方法切割就会导致潜在的内容遗漏。对于这种候选语料”内聚性“很强的情况，容易想到，我们要么在切割语料时动手脚（不把连续数字符号所引领的多段文本切开）；要么在切割时照常切割、但在召回时动手脚（若命中了带连续数字符号的这种长文本的开头，那么就一并把后面连续数字符号引领的多段文本一起召回）。前者方案有较明显瑕疵：\n相对于更短文段而言，长文段的语义更丰富，每个单独的语义点更容易被淹没，所以在有明确语义 query 的召回下这种长文段可能会吃亏； 长文段一旦被召回，只要不是针对整段文本的提问，那么也是引入了更多的噪声（不过鉴于 LLM 的能力，这可能也无伤大雅，就是费点显存or接口费用）。 文档切片最好是按照语义切割，以下是一种实现方案：\n1）将包含主从关系的段落合并，保证每一段在说同一件事情。\n可以利用 NLP 的篇章分析（discourse parsing）工具提取出段落之间的主要关系，或利用BERT等模型来实现语义分割。BERT等模型在预训练的时候采用了 NSP（next sentence prediction）的训练任务，因此BERT可以判断两个句子（段落）是否具有语义衔接关系。这里可以设置相似度阈值t，从前往后依次判断相邻两个段落的相似度分数是否大于t，如果大于则合并，否则断开。 2）针对合并后的段落提取关键信息。\n利用 NLP 中的成分句法分析（constituency parsing）工具和命名实体识别（NER）工具提取，前者可以提取核心部分（名词短语、动词短语……），后者可以提取重要实体（货币名、人名、企业名……）。 可以用语义角色标注（Semantic Role Labeling）来分析句子的谓词论元结构，提取“谁对谁做了什么”的信息作为关键信息。 NLP 的研究中本来就有关键词提取工作（Keyphrase Extraction）。一个工具是 HanLP ，中文效果好，但是付费，免费版调用次数有限。还有一个开源工具是 KeyBERT，英文效果好，但是中文效果差。 垂直领域建议的方法，以上方法在垂直领域都有准确度低的缺陷，垂直领域可以仿照 ChatLaw 的做法，即：训练一个生成关键词的模型（ChatLaw 就是训练了一个 KeyLLM）。 3）对关键信息做 embedding\n4）问题查询时，先根据 query 向量检索到关键信息，再由关键信息找到对应的段落文本\n分片粒度过细，知识点会比较零碎影响了相互间的关系；分片粒度过粗，在匹配时可能会携带冗余信息，另外对 Embedding、处理、索引的效率也有影响。粒度从细到粗可以使用标点符号、段落、章节等进行区分。\n如何让 LLM 简要、准确回答细粒度知识？如何让 LLM 回答出全面的粗粒度（跨段落）知识？QA 的难度主要在于回答这个问题所依赖的信息在长文档中的分布情况，具体大致分为以下三种情况：\nL1：相关信息出现在不超过一个固定上下文长度（512）限制的 block 内 L2：相关信息出现在多个 block 里，出现的总数不超过 5-10 个，最好是能在大模型支持的有效长度内。 L3：需要考虑多个片段，并综合结合上下文信息甚至全部内容才能得到结果。 受限于数据集的大小和规模以及问题的难度，目前相关研究主要偏向于 L1 和 L2 。能比较好的整合和回答 L1 类问题，L2 类问题也有比较不错的结果，但对于 L3 类问题，如果所涉及到的片段长度过长，还是无法做到有效的回答。了解RAG能做什么和不能做什么，可以让我们为RAG寻找最适合的领域，避免强行进入错误的地方。\n4.2.2. 文档处理 经常遇到一些复杂文档的情况，这些文档中可能有表格，有图片，有单双栏等情况。尤其是对于一些扫描版本的文档时候，则需要将文档转换成可以编辑的文档，这就变成了版面还原的问题。具体的，可以利用 ppstructrue 进行文档版面分析，在具体实现路线上，图像首先经过版面分析模型，将图像划分为文本、表格、图像等不同区域，随后对这些区域分别进行识别。这里可以尝试使用 python 库 textract，支持从多种类型文件中提取文字信息，普通文本文件自不必说，其它各种常用格式文件也都支持，比如：Microsoft 全家桶 docx，xlsx；图像 gif， jpg 等；音频文件 mp3，ogg 等。\n针对各种类型的文档，有不同的措施，用于完整的提取文档内容。\nDoc 类文档还是比较好处理的，直接解析其实就能得到文本到底是什么元素，比如标题、表格、段落等等。这部分直接将文本段及其对应的属性存储下来，用于后续切分的依据。 PDF 类文档的难点在于，如何完整恢复图片、表格、标题、段落等内容，形成一个文字版的文档。可能需要使用了多个开源模型进行协同分析，例如版面分析使用百度的 PP-StructureV2，能够对Text、Title、Figure、Figure caption、Table、Table caption、Header、Footer、Reference、Equation 这10类区域进行检测，统一了OCR和文本属性分类两个任务。 PPT 的难点在于，如何对 PPT 中大量的流程图，架构图进行提取。因为这些图多以形状元素在 PPT 中呈现，如果光提取文字，大量潜藏的信息就完全丢失了。可以先将 PPT 转换成PDF形式，然后用上述处理PDF的方式来进行解析。 以上均没有解决图片信息如何还原的问题。大量的文档使用了图文混排的形式，例如上述的 PPT 文件，转换成 PDF 后，仅仅是能够识别出这一块是一幅图片，对于图片，直接转换成向量，不利于后续的检索。一个较为昂贵的方案是，使用一个多模态模型，并通过 prompt 来对文档中的图片进行关键信息提取，形成一段摘要描述，作为文档图片的索引。 对于每个文档，实际上元素的组织形式是树状形式。例如一个文档包含多个标题，每个标题又包括多个小标题，每个小标题包括一段文本等等。我们只需要根据元素之间的关系，通过遍历这颗文档树，就能取到各个较为完整的语义段落及其对应的标题。有些完整语义段落可能较长，可以对每一个语义段落，通过大模型生成摘要。这样文档就形成了一个结构化的表达形式： |id| text| summary| source| type| image_source| |–|–|–|–|–|–|，例如： |1| 文本原始段落| 文本摘要| 来源文件| 文本元素类别（主要用于区分图片和文本）| 图片存储位置（在回答中返回这个位置，前端进行渲染）|。\n对于简单的知识问答系统而言，向量数据库中存放的仅仅是由 document 拆分的 chunks，在检索时也只能使用一些 top-k 的策略，精细化程度不够。**一种技巧是在 document 和 chunks 中增加一些描述信息，为后续的检索和生成提供更多效果提升和管控的可能性。这就是元数据（metadata），用来描述文档或者 chunks 的附加信息，如标题，摘要，问题集，关键词，所有者等。在后续根据 query 进行匹配时，就可以先根据元数据过滤（比如时间、内容源等），再找到原始 chunk。**从使用场景上看，metadata 可以用于提升效果，内容引用溯源，document 级甚至 chunk 级的权限控制，排序，过滤，后处理等业务策略等。\n4.2.3. 文档召回 文档召回过程中如何保证召回内容跟问题是相关的？ 或者说，如何尽可能减少无关信息？ 召回数据相关性的影响方面很多，既包括文档的切分，也包括文档 query 输入的清晰度，因此现在也出现了 query 改写、多召回策略以及排序修正等多个方案。\na.优化召回的输入\n常见的是query 改写，为什么需要改写？一方面改写能提高召回系统的召回率；另一方面能让整个系统对用户的输入更鲁棒。常见的有：同义词改写、拼音改写、前缀补全、丢词和留词、近义词拓展召回、过长提问的总结、多视角提问改写等。\n在专业的垂直领域，待检索的文档往往都是非常专业的表述，而用户的问题往往是非常不专业的白话表达。所以直接拿用户的 query 去检索，召回的效果就会比较差。Keyword LLM 就是解决这个问题的一种方案。例如在 ChatDoctor 中会先让大模型基于用户的 query 生成一系列的关键词，然后再用关键词去知识库中做检索。ChatDoctor 是直接用 In-Context Learning 的方式进行关键词的生成。我们也可以对大模型在这个任务上进行微调，训练一个专门根据用户问题生成关键词的大模型。这就是 ChatLaw 中的方案，方案内容大致如下：\n1）基于传统 NLP 的成分句法分析，提取名词短语；再通过短语间的依存关系，生成关键词列表\n2）从完整语句的 Embedding，切换为关键词 Embedding：\n知识库构建时，基于单知识点入库，入库时提取关键词列表进行 Embedding，用于检索。 查询时，对用户的问题提取关键词列表进行 Embedding 后，从本地知识库检索多条记录。 3）将单问句中的多知识点拆解后检索，将召回的多条记录交给 LLM 整合。\n该方法的优势在于：\n相比传统 Embedding，大幅提升召回精准度。 支持单次交互，对多知识点进行聚合处理。而不必让用户手动分别查询单个知识点，然后让 LLM 对会话历史中的单个知识点进行汇总。 使用传统 NLP，在专项问题处理上，相比 LLM 提供更好的精度和性能。 减少了对 LLM 的交互频次；提升了交给 LLM 的有效信息密度；大大提升问答系统的交互速度。 b.优化召回输出\n如果一篇文档与查询非常相关，但与已经呈现给用户的文档非常相似，那么这篇文档的边际收益可能就不大。MMR 是一种广泛应用于信息检索和自然语言处理领域的算法。MMR 的主要目标是在文档排序和摘要生成等任务中平衡相关性和新颖性。换句话说，MMR 旨在为用户提供既相关又包含新信息的结果。 输入文档重排：LLM 对位置是相对比较敏感的，得分好的放在首或尾，LLM 会重点关注。 可以召回小片段，溯源大片段，类似于 langchain 的 ParentDocumentRetriever 方法，主要是解决知识不全的问题。 也可以召回大片段，模型使用小片段，主要是想解决模型上下文长度限制的问题。 4.2.4. Self-RAG 自反思的检索增强生成方法（SELF-RAG），通过按需检索和自我反思来改进 LLM 的生成质量。SELF-RAG 会训练一个任意的 LM，使其能够反思自己的生成过程，并生成任务输出和中间的特殊tokens（reflection tokens）。Self-RAG 主要步骤概括如下：\n判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回 平行处理每个片段：生成 prompt +一个片段的结果。PS： query + chunk ==\u003e 带有反思标记（relevant/supported/partital/inrelevant）的 chunk 使用反思字段，检查输出是否相关，选择最符合需要的片段； 再重复检索 生成结果会引用相关片段，以及标记输出结果是否符合该片段，便于查证事实。 Self-RAG 的一个重要创新是 Reflection tokens (反思字符)：通过生成反思字符这一特殊标记来检查输出。这些字符会分为 Retrieve 和 Critique 两种类型，会标示：检查是否有检索的必要，完成检索后检查输出的相关性、完整性、检索片段是否支持输出的观点。模型会基于原有词库和反思字段来生成下一个 token。\n4.2.5. 拒绝回复 拒绝回复其实很多场景会应用到，例如在面对用户提问的内容并不是目前我们预期支持的领域（客服场景问天气），或者用户所问超出观点类（如何看待俄乌问题）、黄反类问题、大模型安全（query 中带有诱导性错误）时，再就是知识库为空的情况，就需要进行拒绝，并给用户一些回复。常见的策略一般有这些：\n一句写死的回复：“哎呀，这方面的问题我还不太懂，需要学习下”。 用大模型生成，例如借助 prompt 引导生成一些安抚性的回复，“对不起，你问的问题，我好像还不太懂。你可以试试问问别的”。 使用推荐问或者追问的策略。比如：增加追问机制，只要在 Prompt 中加入“如果无法从背景知识回答用户的问题，则根据背景知识内容，对用户进行追问，问题限制在3个以内”。这个机制并没有什么技术含量，主要依靠大模型的能力。不过大大改善了用户体验，用户在多轮引导中逐步明确了自己的问题，从而能够得到合适的答案。（你是否在找以下几个问题XX；你描述的我好像不太懂，能再补充补充吗） 5. 参考文档 基于大语言模型知识问答应用落地实践 – 知识库构建（上） 基于大语言模型知识问答应用落地实践 – 知识召回调优（上） 基于大语言模型知识问答应用落地实践 – 知识召回调优（下） LLM外挂知识库 引入元数据(metadata)提升RAG架构下LLM应用的效果和管控精度 基于ElasticSearch的混合检索实战\u0026原理分析 大模型检索增强生成（RAG）系统进化指南 大模型+知识库/数据库问答实践过程的经验汇总（三） 大模型RAG 场景、数据、应用难点与解决（四） Filter table dataCreate a pivot tableCreate a chart from data series\nConfigure buttons visibility\n",
  "wordCount" : "1156",
  "inLanguage": "zh",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "王宇"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "PaperMod",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">
                        
                    <img src="http://localhost:1313/images/msg_hu15231257772499651944.png" alt="" aria-label="logo"
                        height="20">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">主页</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      大模型知识问答实现技术调研
    </h1>
    <div class="post-description">
      黄婷
    </div>
    <div class="post-meta">6 分钟&nbsp;·&nbsp;王宇&nbsp;|&nbsp;<a href="https://github.com/WangShaoyu1/myBlogPaperMod/tree/master/content/posts/wiki/%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-%e6%a6%82%e8%bf%b0" aria-label="1. 概述">1. 概述</a></li>
                <li>
                    <a href="#2-%e5%90%91%e9%87%8f%e5%8c%96%e7%9f%a5%e8%af%86%e5%ba%93%e7%9a%84%e6%9e%84%e5%bb%ba" aria-label="2. 向量化知识库的构建">2. 向量化知识库的构建</a><ul>
                        
                <li>
                    <a href="#21-%e6%96%87%e6%a1%a3%e6%8b%86%e5%88%86" aria-label="2.1. 文档拆分">2.1. 文档拆分</a><ul>
                        
                <li>
                    <a href="#211-%e6%8b%86%e5%88%86%e7%89%87%e6%ae%b5%e7%9a%84%e6%96%b9%e6%b3%95" aria-label="2.1.1. 拆分片段的方法">2.1.1. 拆分片段的方法</a></li>
                <li>
                    <a href="#212-%e6%a8%a1%e5%9e%8b%e5%af%b9%e7%89%87%e6%ae%b5%e9%95%bf%e5%ba%a6%e7%9a%84%e6%94%af%e6%8c%81" aria-label="2.1.2. 模型对片段长度的支持">2.1.2. 模型对片段长度的支持</a></li></ul>
                </li>
                <li>
                    <a href="#22-%e5%90%91%e9%87%8f%e5%8c%96%e6%a8%a1%e5%9e%8b%e9%80%89%e6%8b%a9" aria-label="2.2. 向量化模型选择">2.2. 向量化模型选择</a></li>
                <li>
                    <a href="#23-%e5%90%91%e9%87%8f%e5%8c%96%e5%b9%b6%e8%a1%8c" aria-label="2.3. 向量化并行">2.3. 向量化并行</a><ul>
                        
                <li>
                    <a href="#231-%e6%96%87%e6%a1%a3%e5%88%87%e5%88%86%e5%b9%b6%e8%a1%8c" aria-label="2.3.1. 文档切分并行">2.3.1. 文档切分并行</a></li>
                <li>
                    <a href="#232-%e5%90%91%e9%87%8f%e5%8c%96%e6%8e%a8%e7%90%86%e5%b9%b6%e8%a1%8c" aria-label="2.3.2. 向量化推理并行">2.3.2. 向量化推理并行</a></li>
                <li>
                    <a href="#233-%e6%89%b9%e9%87%8f%e5%86%99%e5%85%a5%e5%90%91%e9%87%8f%e6%95%b0%e6%8d%ae%e5%ba%93" aria-label="2.3.3. 批量写入向量数据库">2.3.3. 批量写入向量数据库</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#3-%e7%9f%a5%e8%af%86%e5%8f%ac%e5%9b%9e" aria-label="3. 知识召回">3. 知识召回</a><ul>
                        
                <li>
                    <a href="#31-%e5%80%92%e6%8e%92%e5%8f%ac%e5%9b%9e" aria-label="3.1. 倒排召回">3.1. 倒排召回</a><ul>
                        
                <li>
                    <a href="#311-%e5%9f%ba%e6%9c%ac%e5%8e%9f%e7%90%86" aria-label="3.1.1. 基本原理">3.1.1. 基本原理</a></li>
                <li>
                    <a href="#312-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98--%e4%bc%98%e5%8c%96%e6%8e%aa%e6%96%bd" aria-label="3.1.2. 常见问题 &amp; 优化措施">3.1.2. 常见问题 &amp; 优化措施</a></li></ul>
                </li>
                <li>
                    <a href="#32-%e5%90%91%e9%87%8f%e5%8f%ac%e5%9b%9e" aria-label="3.2. 向量召回">3.2. 向量召回</a><ul>
                        
                <li>
                    <a href="#321-%e4%bb%80%e4%b9%88%e6%98%af%e5%af%b9%e7%a7%b0%e5%8f%ac%e5%9b%9e%e5%92%8c%e9%9d%9e%e5%af%b9%e7%a7%b0%e5%8f%ac%e5%9b%9e%e5%9c%a8%e5%90%91%e9%87%8f%e5%8f%ac%e5%9b%9e%e4%b8%ad%e8%af%a5%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9" aria-label="3.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？">3.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？</a></li>
                <li>
                    <a href="#322-%e7%9f%a5%e8%af%86%e5%a2%9e%e5%bc%ba%e4%bc%98%e5%8c%96%e5%8f%ac%e5%9b%9e" aria-label="3.2.2. 知识增强优化召回">3.2.2. 知识增强优化召回</a></li>
                <li>
                    <a href="#323-%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%bc%94%e8%bf%9b%e4%b8%8e%e9%80%89%e5%9e%8b" aria-label="3.2.3. 向量模型的演进与选型">3.2.3. 向量模型的演进与选型</a></li>
                <li>
                    <a href="#324-finetune%e5%90%91%e9%87%8f%e6%a8%a1%e5%9e%8b%e6%98%af%e5%90%a6%e6%9c%89%e6%94%b6%e7%9b%8a" aria-label="3.2.4. Finetune 向量模型是否有收益？">3.2.4. Finetune 向量模型是否有收益？</a></li></ul>
                </li>
                <li>
                    <a href="#33-%e5%80%92%e6%8e%92%e5%8f%ac%e5%9b%9e%e5%90%91%e9%87%8f%e5%8f%ac%e5%9b%9e%e7%9a%84%e4%bc%98%e5%8a%a3%e5%8a%bf%e5%af%b9%e6%af%94" aria-label="3.3. 倒排召回 &amp; 向量召回的优劣势对比">3.3. 倒排召回 &amp; 向量召回的优劣势对比</a></li>
                <li>
                    <a href="#34-%e6%a8%a1%e5%9e%8b%e5%8f%ac%e5%9b%9e%e7%bb%93%e6%9e%9c%e7%9a%84%e9%87%8d%e6%8e%92%e5%ba%8f" aria-label="3.4. 模型召回结果的重排序">3.4. 模型召回结果的重排序</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e5%ae%9e%e8%b7%b5%e4%b8%ad%e5%ad%98%e5%9c%a8%e7%9a%84%e9%97%ae%e9%a2%98%e5%92%8c%e9%9a%be%e7%82%b9" aria-label="4. 实践中存在的问题和难点">4. 实践中存在的问题和难点</a><ul>
                        
                <li>
                    <a href="#41-%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98" aria-label="4.1. 常见问题">4.1. 常见问题</a></li>
                <li>
                    <a href="#42-%e9%9a%be%e7%82%b9%e5%8f%8a%e7%bb%8f%e9%aa%8c" aria-label="4.2. 难点及经验">4.2. 难点及经验</a><ul>
                        
                <li>
                    <a href="#421-%e6%96%87%e6%a1%a3%e5%88%87%e5%88%86" aria-label="4.2.1. 文档切分">4.2.1. 文档切分</a></li>
                <li>
                    <a href="#422-%e6%96%87%e6%a1%a3%e5%a4%84%e7%90%86" aria-label="4.2.2. 文档处理">4.2.2. 文档处理</a></li>
                <li>
                    <a href="#423-%e6%96%87%e6%a1%a3%e5%8f%ac%e5%9b%9e" aria-label="4.2.3. 文档召回">4.2.3. 文档召回</a></li>
                <li>
                    <a href="#424-self-rag" aria-label="4.2.4. Self-RAG">4.2.4. Self-RAG</a></li>
                <li>
                    <a href="#425-%e6%8b%92%e7%bb%9d%e5%9b%9e%e5%a4%8d" aria-label="4.2.5. 拒绝回复">4.2.5. 拒绝回复</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#5-%e5%8f%82%e8%80%83%e6%96%87%e6%a1%a3" aria-label="5. 参考文档">5. 参考文档</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><ul>
<li>1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-概述">1. 概述</a></li>
<li>2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量化知识库的构建">2. 向量化知识库的构建</a>
<ul>
<li>2.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-文档拆分">2.1. 文档拆分</a>
<ul>
<li>2.1.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-拆分片段的方法">2.1.1. 拆分片段的方法</a></li>
<li>2.1.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-模型对片段长度的支持">2.1.2. 模型对片段长度的支持</a></li>
</ul>
</li>
<li>2.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量化模型选择">2.2. 向量化模型选择</a></li>
<li>2.3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量化并行">2.3. 向量化并行</a>
<ul>
<li>2.3.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-文档切分并行">2.3.1. 文档切分并行</a></li>
<li>2.3.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量化推理并行">2.3.2. 向量化推理并行</a></li>
<li>2.3.3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-批量写入向量数据库">2.3.3. 批量写入向量数据库</a></li>
</ul>
</li>
</ul>
</li>
<li>3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-知识召回">3. 知识召回</a>
<ul>
<li>3.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-倒排召回">3.1. 倒排召回</a>
<ul>
<li>3.1.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-基本原理">3.1.1. 基本原理</a></li>
<li>3.1.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-常见问题&优化措施">3.1.2. 常见问题 &amp; 优化措施</a></li>
</ul>
</li>
<li>3.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量召回">3.2. 向量召回</a>
<ul>
<li>3.2.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-什么是对称召回和非对称召回？在向量召回中该如何选择？">3.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？</a></li>
<li>3.2.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-知识增强优化召回">3.2.2. 知识增强优化召回</a></li>
<li>3.2.3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-向量模型的演进与选型">3.2.3. 向量模型的演进与选型</a></li>
<li>3.2.4<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-Finetune向量模型是否有收益？">3.2.4. Finetune 向量模型是否有收益？</a></li>
</ul>
</li>
<li>3.3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-倒排召回&向量召回的优劣势对比">3.3. 倒排召回 &amp; 向量召回的优劣势对比</a></li>
<li>3.4<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-模型召回结果的重排序">3.4. 模型召回结果的重排序</a></li>
</ul>
</li>
<li>4<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-实践中存在的问题和难点">4. 实践中存在的问题和难点</a>
<ul>
<li>4.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-常见问题">4.1. 常见问题</a></li>
<li>4.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-难点及经验">4.2. 难点及经验</a>
<ul>
<li>4.2.1<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-文档切分">4.2.1. 文档切分</a></li>
<li>4.2.2<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-文档处理">4.2.2. 文档处理</a></li>
<li>4.2.3<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-文档召回">4.2.3. 文档召回</a></li>
<li>4.2.4<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-Self-RAG">4.2.4. Self-RAG</a></li>
<li>4.2.5<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-拒绝回复">4.2.5. 拒绝回复</a></li>
</ul>
</li>
</ul>
</li>
<li>5<a href="/posts/wiki/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94%E5%AE%9E%E7%8E%B0%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-大模型知识问答实现技术调研-参考文档">5. 参考文档</a></li>
</ul>
<h1 id="1-概述">1. 概述<a hidden class="anchor" aria-hidden="true" href="#1-概述">#</a></h1>
<p>当前大语言模型应用中被广泛关注的技术路线是大模型（LLM）+知识召回（Knowledge Retrieval）的方式，这种方式也被称为RAG（Retrieval Augmented Generation, 检索增强生成），其基本思路是把私域知识文档进行切片，然后向量化，再通过检索召回，最终作为上下文输入到大语言模型进行归纳总结。RAG 方法使得开发者不必为每一个特定的任务重新训练整个大模型，只需要外挂上知识库，即可为模型提供额外的信息输入，提高其回答的准确性。</p>
<p>在 LLM 已经具备了较强能力的基础上，仍然需要 RAG ，主要有以下几点原因：</p>
<ul>
<li>**幻觉问题：**LLM 文本生成的底层原理是基于概率的，因此会不可避免地产生“一本正经的胡说八道”的情况。</li>
<li>**时效性问题：**LLM 的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题，例如“帮我推荐几部热映的电影？”。</li>
<li>**数据安全问题：**通用的 LLM 没有企业内部数据和用户数据，那么企业想要在保证安全的前提下使用 LLM，最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成，而在线的大模型仅仅完成一个归纳总结的功能。</li>
</ul>
<h1 id="2-向量化知识库的构建">2. 向量化知识库的构建<a hidden class="anchor" aria-hidden="true" href="#2-向量化知识库的构建">#</a></h1>
<p>在这个技术方向的具体实践中， 知识库可以采取基于倒排和基于向量的两种索引方式进行构建，它对于知识问答流程中的知识召回这步起关键作用，和普通的文档索引或日志索引不同，知识的向量化需要借助深度模型的语义化能力，存在文档切分，向量模型部署&amp;推理等额外步骤。知识向量化建库过程中，不仅仅需要考虑原始的文档量级，还需要考虑切分粒度，向量维度等因素，最终被向量数据库索引的知识条数可能达到一个非常大的量级，可能由以下两方面的原因引起：</p>
<ul>
<li>各个行业的既有文档量很高，如金融、医药、法律领域等，新增量也很大。</li>
<li><strong>为了召回效果的追求，对文档的切分常常会采用按句或者按段进行多粒度的冗余存贮。</strong></li>
</ul>
<h2 id="21-文档拆分">2.1. 文档拆分<a hidden class="anchor" aria-hidden="true" href="#21-文档拆分">#</a></h2>
<p>知识向量化的前置步骤是进行知识的拆分，**语义完整性的保持是最重要的考量。**基于亚马逊云科技的服务，总结了以下两个方面的经验：</p>
<h3 id="211-拆分片段的方法">2.1.1. <strong>拆分片段的方法</strong><a hidden class="anchor" aria-hidden="true" href="#211-拆分片段的方法">#</a></h3>
<p>关于这部分的工作，Langchain 作为一种流行的大语言模型集成框架，提供了非常多的 Document Loader（文档加载器，用于解析不同格式的文档）和 Text Spiltters（文本分割器，用于文档拆分），其中的一些实现具有借鉴意义。</p>
<p>目前使用较多的基础方式是采用 Langchain 中的 RecursiveCharacterTextSplitter，属于是 Langchain 的默认拆分器。它采用多级分隔字符列表 [“\n\n”， “\n”， ” “， “”] 来进行拆分，默认先按照段落做拆分，如果拆分结果的 chunk_size （文档块大小，其字符数受限于向量化模型）超出，再继续利用下一级分隔字符继续拆分，直到满足 chunk_size 的要求。</p>
<p>但这种做法相对来说还是比较粗糙，可能会造成一些关键内容被拆开。对于一些特定的文档格式可以有一些更细致的做法。</p>
<ul>
<li>FAQ 文件，必须按照一问一答粒度拆分，后续向量化的输入可以仅仅使用问题，也可以使用问题+答案</li>
<li>Markdown 文件，”#”是用于标识标题的特殊字符，可以采用 MarkdownHeaderTextSplitter 作为分割器，它能更好的保证内容和对应的标题被提取出来。</li>
<li>PDF 文件，会包含更丰富的格式信息。Langchain 里面提供了非常多的 Loader，但 Langchain 中的 PDFMinerPDFasHTMLLoader 的切分效果上会更好，它把 PDF 转换成 HTML，通过 HTML 的<div>块进行切分，这种方式能保留每个块的字号信息，从而可以推导出每块内容的隶属关系，把一个段落的标题和上一级父标题关联上，使得信息更加完整。类似下面这种效果：</li>
</ul>
<p><strong><img loading="lazy" src="https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-12.jpg" alt=""  />
</strong></p>
<h3 id="212-模型对片段长度的支持">2.1.2. <strong>模型对片段长度的支持</strong><a hidden class="anchor" aria-hidden="true" href="#212-模型对片段长度的支持">#</a></h3>
<p>由于拆分的片段后续需要通过向量化模型进行推理，所以必须考虑向量化模型的 Max_seq_length 的限制，超出这个限制可能会出现截断，导致语义不完整。从支持的 Max_seq_length 来划分，目前主要有两类 Embedding 模型，如下表所示（这四个是亚马逊有过实践经验的模型）。</p>
<p><strong><img loading="lazy" src="/download/attachments/114685107/image2024-1-19_11-30-0.png?version=1&amp;modificationDate=1705635000701&amp;api=v2" alt=""  />
</strong></p>
<p>这里的 Max_seq_length 是指 Token 数，和字符数并不等价。依据之前的测试经验，前三个模型一个 token 约为 1.5 个汉字字符左右。而对于大语言模型，如 chatglm，一个 token 一般为 2 个字符左右。如果在切分时不方便计算 token 数，也可以简单按照这个比例来换算，保证不出现截断的情况。</p>
<p>前三个模型是基于 Bert 的 Embedding 模型，OpenAI 的 text-embedding-ada-002 模型是基于 GPT3 的模型。前者适合句或者短段落的向量化，后者 OpenAI 的 SAAS 化接口，适合长文本的向量化，但不能私有化部署。</p>
<p>可以根据召回效果进行验证选择。从目前的实践经验上看 text-embedding-ada-002 对于中文的相似性打分排序性可以，但区分度不够（集中 0.7 左右），不太利于直接通过阈值判断是否有相似知识召回。</p>
<p><strong>另外，对于长度限制的问题也有另外一种改善方法，可以对拆分的片段进行编号，相邻的片段编号也临近，当召回其中一个片段时，可以通过向量数据库的 range search 把附近的片段也召回，可在一定程度上保证召回内容的语意完整性。</strong></p>
<h2 id="22-向量化模型选择">2.2. 向量化模型选择<a hidden class="anchor" aria-hidden="true" href="#22-向量化模型选择">#</a></h2>
<p>上面四个模型只是提到了模型对于文本长度的支持差异，效果方面目前并没有非常权威的结论。可以通过 <a href="https://huggingface.co/spaces/mteb/leaderboard">leaderboard</a> 来了解各个模型的性能，榜上的大多数模型的评测还是基于公开数据集的 benchmark，对于真实生产中的场景， benchmark 结论是否成立还需要 case by case 地来看。但原则上有以下几方面的经验可以分享：</p>
<ul>
<li><strong>经过垂直领域 Finetune 的模型比原始向量模型有明显的优势。</strong></li>
<li>目前的向量化模型分为两类，对称和非对称。<strong>未进行微调的情况下，对于 FAQ 建议走对称召回，也就是 Query 到 Question 的召回。对于文档片段知识，建议使用非对称召回模型，也就是 Query 到 Answer（文档片段）的召回。</strong></li>
<li><strong>没有效果上的明显差异的情况下，尽量选择向量维度短的模型</strong>，高维向量会给向量数据库造成检索性能和成本两方面的压力。</li>
</ul>
<h2 id="23-向量化并行">2.3. 向量化并行<a hidden class="anchor" aria-hidden="true" href="#23-向量化并行">#</a></h2>
<p>真实的业务场景中，文档的规模在百到百万这个数量级之间。按照冗余的多级召回方式，对应的知识条目最高可能达到亿的规模。由于整个离线计算的规模很大，所以必须并发进行，否则无法满足知识新增和向量检索效果迭代的要求。</p>
<h3 id="231-文档切分并行">2.3.1. <strong>文档切分并行</strong><a hidden class="anchor" aria-hidden="true" href="#231-文档切分并行">#</a></h3>
<p>计算的并发粒度是文件级别的，处理的文件格式也是多样的，如 TXT 纯文本，Markdown， PDF 等，其对应的切分逻辑也有差异。而使用 Spark 这种大数据框架来并行处理过重，并不合适。使用多核实例进行多进程并发处理则过于原始，任务的观测追踪上不太方便。具体使用哪种方法还需进一步考虑。</p>
<h3 id="232-向量化推理并行">2.3.2. <strong>向量化推理并行</strong><a hidden class="anchor" aria-hidden="true" href="#232-向量化推理并行">#</a></h3>
<p>由于切分的段落和句子相对于文档数量膨胀了很多倍，向量模型的推理吞吐能力决定了整个流程的吞吐能力。一般来说可以采用如下几种策略：</p>
<ul>
<li>GPU 实例部署：向量化模型 CPU 实例是可以推理的。但离线场景下，推理并发度高，GPU 相对于 CPU 可以达到 20 倍左右的吞吐量提升。所以离线场景可以采用 GPU 推理，在线场景 CPU 推理的策略。</li>
<li>多节点 Endpoint：对于临时的大并发向量生成，通过部署多节点 Endpoint 进行处理，处理完毕后可以关闭。</li>
</ul>
<h3 id="233-批量写入向量数据库">2.3.3. <strong>批量写入向量数据库</strong><a hidden class="anchor" aria-hidden="true" href="#233-批量写入向量数据库">#</a></h3>
<p>ES数据库可以通过 bulk 批量写入，比单条写入有很大的优势。</p>
<h1 id="3-知识召回">3. 知识召回<a hidden class="anchor" aria-hidden="true" href="#3-知识召回">#</a></h1>
<p>知识召回在基于大语言模型的知识问答中是非常关键的步骤，它决定了大语言模型的输入，对后续回答的可靠性以及回复质量影响非常大。目前基于语义向量召回是用的比较多的方式，但在实际的生产实践中，倒排的召回方式也非常实用，它具备精确匹配、索引效率和可解释的优势。在目前的全文检索系统中，基于倒排的 BM25 相关性打分依然是核心机制。当然随着近些年深度学习的发展，语义召回表现了强大的检索效果，这两路召回在不同场景下都具备自己独立的优势，在知识召回中的互补性很强。</p>
<h2 id="31-倒排召回">3.1. 倒排召回<a hidden class="anchor" aria-hidden="true" href="#31-倒排召回">#</a></h2>
<p>这种方式<strong>基于关键词进行召回</strong>，比较简单易用，不要求技术人员有算法知识背景。这种召回策略在很多场景下非常有效，特别是一些对领域专词非常敏感的场景。但同时它的缺点在于，与搜索引擎不同，在对话机器人的交互形态下，用户的输入更多表现为完整的句子而不是搜索关键词，这时候倒排召回中的 BM25 打分逻辑对语义信息的捕捉能力弱的问题比较明显，所以一般我们建议把它作为向量召回的一种必要补充。</p>
<h3 id="311-基本原理">3.1.1. <strong>基本原理</strong><a hidden class="anchor" aria-hidden="true" href="#311-基本原理">#</a></h3>
<p>倒排索引（Inverted index）作为一种广泛使用的索引方式，也常被称为反向索引、置入档案或反向档案，它是文档检索系统中最常用的数据结构。离线索引构建时，通过分词器对文档进行切词，得到一系列的关键词（Term）集合，然后以 Term 为 key 构建它与相关文档的映射关系。在线搜索流程中，首先通过切词器对用户输入进行切词，得到 Term 列表，然后根据如下 BM25 打分公式进行打分排序返回。</p>
<p><img loading="lazy" src="/download/attachments/114685107/image2024-1-19_14-49-57.png?version=1&amp;modificationDate=1705646997757&amp;api=v2" alt=""  />
</p>
<p>上述公式中：</p>
<p>——BM25(D，Q)：文档D对于查询Q的BM25得分。<br>
——n：查询中的关键词数<br>
——qi：查询中的第i个关键词<br>
——idf(qi)：查询中第i个关键词的逆文档频率（IDF = log(语料库的文档总数/(包含该词的文档数+1))）<br>
——f(qi，D)：查询中第i个关键词在文档D中的频率（TF = 该词在文档中出现的次数/文档的总词数）<br>
——|D|：文档D的长度（词数）<br>
——avgdl：所有文档的平均长度<br>
——b：自由参数<br>
——k1：自由参数</p>
<p>k1 与 b 是可以自由调节的参数，其中 k1 默认值为 1.2，会影响词语在文档中出现的次数对于得分的重要性，比如希望搜索词在文档中出现越多则越接近想要的内容，那么可以将这个参数调大一点。b 默认 0.75，它能控制文档长度对于分数的惩罚力度，如果不希望文档长度更大的相似度搜索更好，可以把 b 设置得更小，如果设置为 0，文档的长度将与分数无关。</p>
<h3 id="312-常见问题--优化措施">3.1.2. <strong>常见问题 &amp; 优化措施</strong><a hidden class="anchor" aria-hidden="true" href="#312-常见问题--优化措施">#</a></h3>
<p>在垂直领域的 FAQ 知识语料的倒排检索实践中，存在两类常见的得分偏差：</p>
<p><strong>a.私有领域文档的 IDF 失真问题</strong></p>
<p>由于 BM25 打分的 IDF 是基于索引内的所有的文档统计得到的，在一些领域内场景，所有的文档的总量并不大，有些专有词汇在全部文档中的出现频率和一些常见的停用词是差不多的，比如在游戏场景中”专属技能”与”怎么”这两个词的 IDF 是差不多的。后续在打分的时候，对于这两个词不能反映真实的重要性，这就是所谓的 IDF 失真的问题。解决这个问题通常有以下几种方法：</p>
<ul>
<li>导入自定义的词频数据（需要搜索引擎的功能支持）</li>
<li>导入一些其他来源的文本语料，这些数据仅仅是为了调整 IDF，使它更加接近真实分布。由于一般这种情况下数据总量不大，加入一些无关的数据并不太会改变集群的查询速度，但需要注意的是在搜索时需要通过字段标记过滤掉这些假数据。</li>
<li>录入停用词，停用词不参与 BM25 的得分计算，一般停用词表为每个词单独一行的纯文本文件</li>
</ul>
<p><strong>b.无关键词匹配</strong></p>
<p>这种是倒排召回缺乏语义信息的典型现象，现实中多词一义是非常普遍的。比如在医疗健康场景，一个用户用”症状”来查询与 COVID-19 相关的文档，但一些十分相关的文档中出现的是“病症”、“临床表现”这样的词。除了向量检索能改善这个问题以外，在非常垂直的领域，比如说某个游戏场景中，经常会存在一些专有领域的“黑话”，向量模型的效果可能不见的好，这种情况下可以采用同义词表去枚举这些同义词。构建同义词表时，每行为多个同义词（≥2）用逗号分隔。<br>
举下面这个例子说明，这里的盾和护盾是场景内的同义词，在不配置同义词表的情况下，用户输入中的关键词是难以匹配的。</p>
<p>User Query:  “秘密武器为什么不能开盾”<br>
Doc: “问题: 秘密武器在危险地带提示不能开启护盾，是正常的吗？<br>
回答: 您好，秘密武器的护盾功能与游戏本身限制相同。游戏内如危险地带等本身无法开启护盾的地点，秘密武器的护盾也无法开启。是游戏的正常设定。”</p>
<h2 id="32-向量召回">3.2. 向量召回<a hidden class="anchor" aria-hidden="true" href="#32-向量召回">#</a></h2>
<h3 id="321-什么是对称召回和非对称召回在向量召回中该如何选择">3.2.1. 什么是对称召回和非对称召回？在向量召回中该如何选择？<a hidden class="anchor" aria-hidden="true" href="#321-什么是对称召回和非对称召回在向量召回中该如何选择">#</a></h3>
<p>对称召回，一般是指通过两个句子的相似性来召回，比如输入的 query 匹配相似的另一个 question(query-&gt;question)，也可以是输入的陈述句匹配相似的另一个陈述句。对应这种数据也叫做 NLI 数据，一般是三元组的形式，比较知名的数据集为 <a href="https://github.com/CLUEbenchmark/SimCLUE">SimCLUE</a>，如：</p>
<p>{&ldquo;query&rdquo;: &ldquo;胡子长得太快怎么办？&rdquo;, &ldquo;title&rdquo;: &ldquo;胡子长得快怎么办？&rdquo;, &ldquo;neg_title&rdquo;: &ldquo;怎样使胡子不浓密！&rdquo;}<br>
{&ldquo;query&rdquo;: &ldquo;在香港哪里买手表好&rdquo;, &ldquo;title&rdquo;: &ldquo;香港买手表哪里好&rdquo;, &ldquo;neg_title&rdquo;: &ldquo;在杭州手机到哪里买&rdquo;}</p>
<p>非对称召回一般是检索数据， 通常是一个相对短的问句和一个相对长的答案（一般为一个文本段落）, 比较知名的数据集为 <a href="https://microsoft.github.io/msmarco/">MSMARCO</a>，一些 Question Answering 类的任务都是基于这种数据集，如：</p>
<p>{&ldquo;query&rdquo;: &ldquo;walgreens store sales average&rdquo;, &ldquo;passage_text&rdquo;: [ &ldquo;The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager&hellip;&rdquo;,]}</p>
<p>一般来说，对称召回任务比非对称召回任务要简单，特别在很多垂直领域，向量模型对领域知识的理解有限，对称召回很多时候理解了字面意思就能满足要求。为了方便描述，下文<strong>把对称召回称为 QQ（Query-Question）召回，把非对称召回称为 QD（Query-Document）召回</strong>。</p>
<p>在真实的生产场景中，知识的存在形式有两种 —— FAQ 和文本文档， FAQ 由于是一问一答的形式，天然是比较适合进行 QQ 召回，而文本文档比较适应 QD 召回。但在实际的 case 中，单独 QQ 召回或者 QD 召回都存在一定的问题。相对于 QQ 召回，QD 召回需要 embedding 模型对语义有更强的理解。Question 一般相对比较简短，信息量没有 Document 那么多，因此对于只是使用 QQ 召回，可能会有问题，如下面的例子：</p>
<p>Question: AWS Clean Rooms 的数据源必须在 AWS 上么？<br>
Answer: 对，目前必须在 AWS 上，而且必须是同一个 region。</p>
<p>用户输入的问题不一定会和 FAQ 的 Question 的提问角度一致，比如用户可能会问“AWS Clean Rooms 的数据源不在同一个 region，行吗？”，这种问题仅仅考虑 QQ 则会出现问题。</p>
<p><strong>所以生产实践中，推荐同时考虑 QQ 和 QD 召回，对于 FAQ 的知识形态，对 Question 生成向量作为索引，同时把 Answer 作为 Doc 生成向量作为索引，在召回时同时走 QQ 和 QD 召回。对于文本文档的知识形态，除了对 Doc 生成向量以外，还可以通过知识增强的方式生成多组 Question。</strong></p>
<h3 id="322-知识增强优化召回">3.2.2. 知识增强优化召回<a hidden class="anchor" aria-hidden="true" href="#322-知识增强优化召回">#</a></h3>
<p>知识增强一般是指，根据上下文重新 Refine 原始的文本使得原始的文本信息更完整，自解释性更强。这个过程由于是离线的一次性处理，所以这里可以借助能力更强的商用 LLM 模型，生成的内容可能会更好。另外由于现在的 LLM 能支撑的上下文 context 长度很长，建议在生成的过程中，在 prompt 中去融入一些该场景的丰富背景信息。</p>
<p>具体的操作方式，可以参考下图：</p>
<p><img loading="lazy" src="https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-41.jpg" alt=""  />
</p>
<p>通过知识增强，可以把文本文档也转化成 FAQ 的形式，使得对于一般的文档也能够同时进行 QQ 和 QD 召回。<strong>但这里需要注意的是，对于特别垂直的领域，能力很强的商业 LLM 也可能无法提取出比较合适的问题，一般观察到的现象是提取出的问题过于直接，或者提取出的问题无实际意义，这部分建议走一遍人工审核。</strong></p>
<p>对于质量不高的 FAQ，特别是提问过于简略的，知识增强也能够优化其质量。另外经过优化后的知识，在 LLM 生成阶段，对于提升 LLM 对知识的理解也是有一定帮助的。总的来说，知识增强是简单有效的优化手段。</p>
<p>除了以上通过给文档块增加额外的问答信息增强向量召回，还有很多其他的召回优化措施：</p>
<ul>
<li>
<p>**层次索引：**创建两个索引，一个由摘要组成，另一个由文档块组成，并且分两步搜索，首先通过摘要过滤出相关文档，然后在相关组内搜索。</p>
</li>
<li>
<p>**HyDE：**使用大模型生成一个伪答案，或生成更标准的 prompt 模板，然后去检索文档 chunk。这个方法的口碑很好，用的应该也比较多，但是存在的问题是受限于常见大模型的能力，有可能会产生副作用。</p>
</li>
<li>
<p>**利用上下文信息检索：**chunk 窗口检索（为了在获取最相关的单个句子后更好地推理找到上下文，可以把检索到的句子作为基准，将上下文窗口扩展 <em>k</em> 个句子，然后将此扩展上下文一起发送给 LLM）；多层次 Child / Parent 文档检索（首先分 chunks 的时候拆分成子 chunk，然后不同的子 chunk 与同一父 chunk 构建关联索引；检索时，先在子 chunks 堆里寻找 topk 相关的，然后在不同子 chunks 与不同的父 chunk 关联下，把所有相关的父 chunk 都找出来，最后统一丢给 LLM 进行组合问答）。</p>
</li>
<li>
<p>**RAG-Fusion：**当接收用户 query 时，让大模型生成 5-10 个相似的 query，然后每个 query 去匹配 5-10 个文本块，接着对所有返回的文本块再做个倒序融合排序，如果有需求就再加个精排，最后取 topk 个文本块拼接至 prompt。这个方法的主要好处，是增加了相关文本块的召回率，同时对用户的 query 自动进行了文本纠错、分解长句等功能。</p>
</li>
<li>
<p>**检索时添加一些特殊符号：**增加查询语句的长度（比如在ID、哈希码或产品名称后面加上一些描述性的词语，或者使用一些常见的问题作为查询语句，这样可以增加查询语句的语义信息，提高向量检索的效果）；使用一些特殊的符号或标记（比如在ID、哈希码或产品名称前后加上双引号，或者使用一些特定的字段名，这样可以告诉向量检索系统，这些词语是需要精确匹配的，而不是基于相似度的）；结合关键词检索（比如在向量检索的结果中，再使用关键词检索的方法，对查询语句和文档进行文本匹配和过滤，这样可以排除一些不相关的内容，提高检索的准确性）。</p>
</li>
<li>
<p>**混合检索：**多路召回，通过向量化召回一些，通过 BM25 或者 TFIDF 也召回一些 CHUNK，然后组合丢给 LLM。</p>
</li>
</ul>
<h3 id="323-向量模型的演进与选型">3.2.3. 向量模型的演进与选型<a hidden class="anchor" aria-hidden="true" href="#323-向量模型的演进与选型">#</a></h3>
<p>首先需要强调的是向量模型与 LLM 模型是完全不同的，两者的训练目标不同，LLM 模型的最终目标是生成。向量模型的最终目标是判别，判断 QQ 之间语义是否相似，判断 QD 之间是否具有相关性。两者的训练目标和训练数据是有不小的区别的。一般都不直接利用 LLM 来生成本文向量。</p>
<p>向量模型的大概演进路线，可以参考下图：</p>
<p><img loading="lazy" src="https://s3.cn-north-1.amazonaws.com.cn/awschinablog/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-42.jpg" alt=""  />
</p>
<p>从上图可以观察到几个基本趋势，目前的向量模型从单纯的基于 NLI 数据集（对称数据集）发展到基于混合数据（对称+非对称）进行训练，即可以做 QQ 召回任务也能够做 QD 召回任务，通过添加 Instruction 的方式来区分这两类任务（在进行 QD 召回的时候，需要在用户 query 中添加 Instruction 前缀）。在模型的第一 pretrain 阶段，从对比学习发展到 RetroMAE 自动编码的方法。后续的监督训练阶段差异不大，主要是监督数据的区别。</p>
<p>在亚马逊之前的实践中，采用的模型包括 sentense-bert 系列，text2vec，m3e ，这些模型在公开数据集上的 benchmark 在垂直领域不一定成立，或者说体现不出明显的优势。所以他们采用游戏场景中的 2w+ 知识增强过的 FAQ，拆分出训练数据和测试数据进行评测（详细评测信息：<a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-4/">基于大语言模型知识问答应用落地实践 – 知识召回调优（下）</a>）。</p>
<p>模型层面，BGE 向量模型作为最新的 SOTA 模型，在各类数据集上都表现良好。所以他们选择 sentence-bert 的多语言模型 paraphrase-multilingual-mpnet-base-v2 与 bge-large-zh-v1.5 进行对比。</p>
<p>召回形式层面，分别对 QQ 和 QD 两种召回形式进行评测，得到如下的结论：</p>
<p><strong>a. 模型层面</strong></p>
<ul>
<li>在没有微调的情况下，bge-large-zh 与 sbert 相比，无论从召回率上还是相似度的值域区分性来看，bge 都有明显优势。</li>
</ul>
<p><strong>b. 召回形式</strong></p>
<ul>
<li>相似度值域范围在 QQ 和 QD 之间是有区别的，两者分布的 kde 曲线（直方图上的曲线）交点位置有差异，召回时可分别考虑 QQ 和 QD 的截断阈值。另外 QD 召回的负例相似度值域上限要低于 QQ 召回的，所以高分的 QD 召回内容是置信度更高的知识。</li>
<li> QQ 召回场景中，原始 Q 的召回效果是明显低于增强 Q 的，其top2召回准确率低，且负例的相似度值域分布在 0-1 的整个空间。 其背后的原因主要是由于数据集中原始的 Q 十分简短，且很多都是相似的概念名词。在这种情况下，过于简短的原始 Q（1-2 词）不该生成向量被索引，可考虑去掉这类知识仅保留增强后的 Q。</li>
<li> QD 场景中，增强 Q 和增强 D 之间的top2召回率和相似分区分度都是最理想的，但是由于它是 LLM 生成的，本身存在幻觉的情况，另外如果增强 D 和原始 D 重复性过高，可能导致召回了过多重复的内容挤占了其他内容的空间。建议结合人审或者其他方式保证这部分知识的质量。</li>
</ul>
<h3 id="324-finetune向量模型是否有收益">3.2.4. <strong>Finetune</strong> <strong>向量模型是否有收益？</strong><a hidden class="anchor" aria-hidden="true" href="#324-finetune向量模型是否有收益">#</a></h3>
<p>在一些专业领域，通用的向量模型可能无法很好的理解一些专有词汇，导致检索性能不足,  且正例和负例相似度的值域有比较多的重叠，无法通过阈值判断该召回内容是否属于相关信息。过多的无关信息可能会在下游误导大语言模型，如果能在上游做截断或者通过 Prompt 暗示 LLM 可能没有相关信息，则会大大减少 LLM 幻觉的问题，实现更好的拒答。</p>
<p>所以向量模型 Finetune 的主要目标一方面是为了提高 topk 召回的准确率，另外一方面想要拉开正例和负例的 similarity 值域分布，从而得到更好的分隔阈值，使得当召回的 similarity 值低于阈值时，直接放弃这些无效的召回，提示下游进行拒答。</p>
<p>亚马逊在实践中对 bge-large-zh-v1.5 模型进行 Finetune 以及测试评估，得到了如下的结论（详细评测信息：<a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-4/">基于大语言模型知识问答应用落地实践 – 知识召回调优（下）</a>）：</p>
<ul>
<li><strong>微调对于训练集有明显提升效果</strong>，无论 QQ 还是 QD 召回，top2 召回率或者正负样例的值域分布上都会更好。在不考虑泛化性的情况下，假设能持续收集真实的用户反馈不断纳入训练集，最终也能覆盖绝大多数用户的提问场景。</li>
<li><strong>微调在测试集上表现出一定的泛化性</strong>，top2 召回率在 QD 召回上提升 9pp 至 87.2%，QQ 召回上提升 1.2pp 至 85.8%，相似度值域从分布上看区分度更好。</li>
</ul>
<p>注意：<strong>KDE 交点并不能直接作为召回是否有效的阈值，因为微调数据是基于知识的一问一答对训练出来的，和真实的用户输入在分布上还是有很大的区别，在实践中，对应的阈值需要多次尝试确定，一般要比 KDE 交点要高。本质上需要不断收集越来越多的用户真实的正负反馈用于调优向量召回。</strong></p>
<h2 id="33-倒排召回向量召回的优劣势对比">3.3. 倒排召回 &amp; 向量召回的优劣势对比<a hidden class="anchor" aria-hidden="true" href="#33-倒排召回向量召回的优劣势对比">#</a></h2>
<p>对比项</p>
<p><strong>倒排召回</strong></p>
<p><strong>向量召回</strong></p>
<p>对比项</p>
<p><strong>倒排召回</strong></p>
<p><strong>向量召回</strong></p>
<p><strong>优势</strong></p>
<ul>
<li>
<p>发展成熟，易达到非常好的 baseline 性能</p>
</li>
<li>
<p>检索速度更快</p>
</li>
<li>
<p>可解释能力强</p>
</li>
<li>
<p>精确匹配能力强</p>
</li>
<li>
<p>支持自定义停用词表，同义词表</p>
</li>
<li>
<p>考虑语义相似性，更加智能</p>
</li>
<li>
<p>语义相近即可召回，无需关键词命中</p>
</li>
<li>
<p> 无需考虑复杂的传统倒排的调优手段</p>
</li>
<li>
<p>具备支持跨模态召回的潜力</p>
</li>
</ul>
<p><strong>劣势</strong></p>
<ul>
<li>
<p>没有语义信息，对”一词多义”现象解决的不好</p>
</li>
<li>
<p>关键字不匹配，用户在搜索时并不知道对应文档中准确的关键词，需要通过同义词扩充，query 改写来解决</p>
</li>
<li>
<p>语义偏移问题，虽然关键词字面上匹配，但是命中顺序和用户输入不一样，语义上可能完全不相关</p>
</li>
<li>
<p>需要模型训练，对垂直领域落地支持有限</p>
</li>
<li>
<p>预训练模型对于公开数据集的 benchmark 参考性不足。对垂直领域泛化性不足，可能出现以下情况：不理解专有词汇；容易出现语义相似但主题不相似的情况</p>
</li>
<li>
<p>对精准匹配支持不足，难以用专业词汇精准召回</p>
</li>
<li>
<p>对”多词一义”情况的支持不如倒排召回中的同义词表简单直接</p>
</li>
<li>
<p>可解释能力弱</p>
</li>
<li>
<p>需要更多的计算资源</p>
</li>
</ul>
<h2 id="34-模型召回结果的重排序">3.4. 模型召回结果的重排序<a hidden class="anchor" aria-hidden="true" href="#34-模型召回结果的重排序">#</a></h2>
<p>在传统的搜索场景中，如何结合这两路召回的结果是一个问题，因为两者不在一个评分体系下。**在基于大语言模型的知识问答机器人场景， 可以简单采用倒排召回和向量召回各 topk 去重取并集的方式。**主要是因为大语言模型对召回内容的相关性排序不敏感，且能够容忍一些不相关的知识召回内容。</p>
<p><strong>ElasticSearch 在使用 KNN+BM25 检索的混合检索分值 Score 计算公式是：knn_score</strong> <strong>*</strong> <strong>boost</strong> <strong>+ bm25_score <strong>*</strong> <strong>boost</strong></strong></p>
<p>对于混合检索，我们在算法层面有了直接的了解后，最终在产品层面会影响一些设计。</p>
<ul>
<li>混合检索的权重设置：在上面的 score 分值计算公式中，可以看到 boost 会影响最终的内容，因为并不是所有的客户和业务场景都适合 knn 检索（向量检索），可能在其他关键的场景中，关键词检索会更适合。我们在产品设计中则可以根据不同的客户诉求以及业务诉求，设置不同的 boost 来调节召回结果，从而改善我们的产品效果。</li>
<li>在 BM25 算法的场景中，分词是非常重要的一个特性，对于不同的行业客户，词库的收集建立对于产品应用的提升肯定是会有质的提升，也是每个公司做 RAG 产品的核心竞争力。</li>
</ul>
<p>另外一个思路是再训练一个排序模型的来进一步打分，这种方式可以更好的结合倒排和向量召回的结果。比如两路召回各生成了10个候选，但最终用于构建 Prompt 的只需要 4 个，那么通过 Rerank（重排序） 就能很好的达到目的并科学的结合两路结果。而且也可以不断的把线上用户的真实反馈信息作为训练数据不断迭代效果。</p>
<p>由于在 Rerank 阶段，所有的候选都是模型召回的 topk，所以在训练的时候正负例的选择范围也应该在召回的 topk 中。微调阶段使用的随机负采样的训练数据是不合适的，建议以上游打分 topk 作为训练数据。可以先挖掘难样本然后把这部分作为排序模型的训练数据。一般情况会带来一定的正向提升，特别是结合真实的用户反馈数据。</p>
<h1 id="4-实践中存在的问题和难点">4. 实践中存在的问题和难点<a hidden class="anchor" aria-hidden="true" href="#4-实践中存在的问题和难点">#</a></h1>
<h2 id="41-常见问题">4.1. 常见问题<a hidden class="anchor" aria-hidden="true" href="#41-常见问题">#</a></h2>
<ul>
<li>
<p><strong>内容缺失</strong>——这是生产案例中最大的问题之一。用户假设特定问题的答案存在于知识库中，但有时并非如此，系统也没有兜底回应。相反，它提供了一个看似合理的错误答案，但实际是“毫无意义”。</p>
</li>
<li>
<p><strong>漏掉排名靠前的文档</strong>——检索器是小型搜索系统，要获得正确的结果并不简单，简单的向量检索很少能达到目的。有时，检索器返回的前 K 个文档中不存在正确答案，从而导致失败。</p>
</li>
<li>
<p><strong>上下文受限</strong>—— RAG 系统可能会检索到太多文档，若强制根据上下文分割并输入文档。问题的答案可能不在上下文中，这可能会导致模型产生幻觉。</p>
</li>
<li>
<p><strong>未提取到有用信息</strong>——当 LLM 无法从上下文中提取答案时，可能会感到困惑，不同大模型对背景信息的理解能力参差不齐。</p>
</li>
<li>
<p><strong>格式错误</strong>——这种需要特定格式的输出，需要进行大量的系统提示和指令微调，以生成特定格式的信息。</p>
</li>
<li>
<p><strong>不合适的回答</strong>——响应中返回答案，但不够具体或过于具体，无法满足用户的需求。当 RAG 系统设计者对给定问题有期望的结果时，就会发生这种情况。在这种情况下，应该提供具体的提示内容和答案，而不仅仅是答案。当用户不确定如何提出问题并且过于笼统时，也会出现不正确的回答。</p>
</li>
</ul>
<h2 id="42-难点及经验">4.2. 难点及经验<a hidden class="anchor" aria-hidden="true" href="#42-难点及经验">#</a></h2>
<h3 id="421-文档切分">4.2.1. 文档切分<a hidden class="anchor" aria-hidden="true" href="#421-文档切分">#</a></h3>
<p>如何将文档拆分为文本片段。主要有两种，一种就是基于策略规则，另外一种是基于算法模型。</p>
<ul>
<li>**如何保证文档切片不会造成相关内容的丢失？**一般而言，文本分割如果按照字符长度进行分割，这是最简单的方式，但会带来很多问题。例如，如果文本是一段代码，一个函数被分割到两段之后就成了没有意义的字符。因此，我们也通常会使用特定的分隔符进行切分，如句号，换行符，问号等。可以使用专门的模型去切分文本，尽量保证一个 chunk 的语义是完整的，且长度不会超过限制。</li>
<li>**文档切片的大小如何控制？**太小则容易造成信息丢失，太大则不利于向量检索命中。此外还要考虑 LLM context 长度的限制，常见大模型支持的上下文窗口为 4096 个 token，这意味着输入 token 和生成的输出 token 的总和不能超过 4096，否则会出错。为了保证不超过这个限制，可以预留约 2000 个 token 作为输入信息，留下约 2000 个 token 作为输入提示和返回消息。这样，如果提取出了五个相关信息块，那么每个片的大小不应超过 400 个 token。</li>
<li>基于算法模型，主要是使用类似 BERT 结构的语义段落分割模型，能够较好的对段落进行切割，并获取尽量完整的上下文语义。该方法能解决一部分问题，但效果有限，若想要达到较优的效果，需要微调，上手难度高。而且切分出的段落有可能大于向量模型所支持的长度，这样就还需要进行切分或提取关键信息。</li>
</ul>
<p>常见的分段方式是在固定max_length的基础上，对出现<code>。/；/？/....../\n</code>等等地方进行切割。但这种方式显然比较武断，面对特殊情况需要进一步优化。比如<code>1.xxx, 2.xxx, ..., 10.xxx</code>超长内容的情况，直接按这种方法切割就会导致潜在的内容遗漏。对于这种候选语料”内聚性“很强的情况，容易想到，我们<strong>要么在切割语料时动手脚</strong>（不把连续数字符号所引领的多段文本切开）；<strong>要么在切割时照常切割、但在召回时动手脚</strong>（若命中了带连续数字符号的这种长文本的开头，那么就一并把后面连续数字符号引领的多段文本一起召回）。前者方案有较明显瑕疵：</p>
<ul>
<li>相对于更短文段而言，长文段的语义更丰富，每个单独的语义点更容易被淹没，所以在有明确语义 query 的召回下这种长文段可能会吃亏；</li>
<li>长文段一旦被召回，只要不是针对整段文本的提问，那么也是引入了更多的噪声（不过鉴于 LLM 的能力，这可能也无伤大雅，就是费点显存or接口费用）。</li>
</ul>
<p><strong>文档切片最好是按照语义切割，以下是一种实现方案：</strong></p>
<p><strong><img loading="lazy" src="/download/attachments/114685107/image2024-1-20_10-31-12.png?version=1&amp;modificationDate=1705717874195&amp;api=v2" alt=""  />
</strong></p>
<p><strong>1）将包含主从关系的段落合并，保证每一段在说同一件事情。</strong></p>
<ul>
<li>可以利用 NLP 的篇章分析（discourse parsing）工具提取出段落之间的主要关系，或利用BERT等模型来实现语义分割。BERT等模型在预训练的时候采用了 NSP（next sentence prediction）的训练任务，因此BERT可以判断两个句子（段落）是否具有语义衔接关系。这里可以设置相似度阈值t，从前往后依次判断相邻两个段落的相似度分数是否大于t，如果大于则合并，否则断开。</li>
</ul>
<p><strong>2）针对合并后的段落提取关键信息。</strong></p>
<ul>
<li>利用 NLP 中的成分句法分析（constituency parsing）工具和命名实体识别（NER）工具提取，前者可以提取核心部分（名词短语、动词短语……），后者可以提取重要实体（货币名、人名、企业名……）。</li>
<li>可以用语义角色标注（Semantic Role Labeling）来分析句子的谓词论元结构，提取“谁对谁做了什么”的信息作为关键信息。</li>
<li> NLP 的研究中本来就有关键词提取工作（Keyphrase Extraction）。一个工具是 HanLP ，中文效果好，但是付费，免费版调用次数有限。还有一个开源工具是 KeyBERT，英文效果好，但是中文效果差。</li>
<li><strong>垂直领域建议的方法，以上方法在垂直领域都有准确度低的缺陷，垂直领域可以仿照 ChatLaw 的做法，即：训练一个生成关键词的模型</strong>（ChatLaw 就是训练了一个 KeyLLM）。</li>
</ul>
<p><strong>3）对关键信息做 embedding</strong></p>
<p><strong>4）问题查询时，先根据 query 向量检索到关键信息，再由关键信息找到对应的段落文本</strong></p>
<p>分片粒度过细，知识点会比较零碎影响了相互间的关系；分片粒度过粗，在匹配时可能会携带冗余信息，另外对 Embedding、处理、索引的效率也有影响。粒度从细到粗可以使用标点符号、段落、章节等进行区分。</p>
<p>如何让 LLM 简要、准确回答细粒度知识？如何让 LLM 回答出全面的粗粒度（跨段落）知识？QA 的难度主要在于回答这个问题所依赖的信息在长文档中的分布情况，具体大致分为以下三种情况：</p>
<ul>
<li>L1：相关信息出现在不超过一个固定上下文长度（512）限制的 block 内</li>
<li>L2：相关信息出现在多个 block 里，出现的总数不超过 5-10 个，最好是能在大模型支持的有效长度内。</li>
<li>L3：需要考虑多个片段，并综合结合上下文信息甚至全部内容才能得到结果。 </li>
</ul>
<p>受限于数据集的大小和规模以及问题的难度，目前相关研究主要偏向于 L1 和 L2 。能比较好的整合和回答 L1 类问题，L2 类问题也有比较不错的结果，但对于 L3 类问题，如果所涉及到的片段长度过长，还是无法做到有效的回答。<strong>了解RAG能做什么和不能做什么，可以让我们为RAG寻找最适合的领域，避免强行进入错误的地方</strong>。</p>
<h3 id="422-文档处理">4.2.2. 文档处理<a hidden class="anchor" aria-hidden="true" href="#422-文档处理">#</a></h3>
<p>经常遇到一些复杂文档的情况，这些文档中可能有表格，有图片，有单双栏等情况。尤其是对于一些扫描版本的文档时候，则需要将文档转换成可以编辑的文档，这就变成了版面还原的问题。具体的，可以利用 ppstructrue 进行文档版面分析，在具体实现路线上，图像首先经过版面分析模型，将图像划分为文本、表格、图像等不同区域，随后对这些区域分别进行识别。这里可以尝试使用 python 库 textract，支持从多种类型文件中提取文字信息，普通文本文件自不必说，其它各种常用格式文件也都支持，比如：Microsoft 全家桶 docx，xlsx；图像 gif， jpg 等；音频文件 mp3，ogg 等。</p>
<p>针对各种类型的文档，有不同的措施，用于完整的提取文档内容。</p>
<ul>
<li>Doc 类文档还是比较好处理的，直接解析其实就能得到文本到底是什么元素，比如标题、表格、段落等等。这部分直接将文本段及其对应的属性存储下来，用于后续切分的依据。</li>
<li>PDF 类文档的难点在于，如何完整恢复图片、表格、标题、段落等内容，形成一个文字版的文档。可能需要使用了多个开源模型进行协同分析，例如版面分析使用百度的 PP-StructureV2，能够对Text、Title、Figure、Figure caption、Table、Table caption、Header、Footer、Reference、Equation 这10类区域进行检测，统一了OCR和文本属性分类两个任务。</li>
<li>PPT 的难点在于，如何对 PPT 中大量的流程图，架构图进行提取。因为这些图多以形状元素在 PPT 中呈现，如果光提取文字，大量潜藏的信息就完全丢失了。可以先将 PPT 转换成PDF形式，然后用上述处理PDF的方式来进行解析。</li>
<li>以上均没有解决图片信息如何还原的问题。大量的文档使用了图文混排的形式，例如上述的 PPT 文件，转换成 PDF 后，仅仅是能够识别出这一块是一幅图片，对于图片，直接转换成向量，不利于后续的检索。一个较为昂贵的方案是，使用一个多模态模型，并通过 prompt 来对文档中的图片进行关键信息提取，形成一段摘要描述，作为文档图片的索引。</li>
</ul>
<p>对于每个文档，实际上元素的组织形式是树状形式。例如一个文档包含多个标题，每个标题又包括多个小标题，每个小标题包括一段文本等等。我们只需要根据元素之间的关系，通过遍历这颗文档树，就能取到各个较为完整的语义段落及其对应的标题。有些完整语义段落可能较长，可以对每一个语义段落，通过大模型生成摘要。这样文档就形成了一个结构化的表达形式： |id| text| summary| source| type| image_source| |–|–|–|–|–|–|，例如： |1| 文本原始段落| 文本摘要| 来源文件| 文本元素类别（主要用于区分图片和文本）| 图片存储位置（在回答中返回这个位置，前端进行渲染）|。</p>
<p>对于简单的知识问答系统而言，向量数据库中存放的仅仅是由 document 拆分的 chunks，在检索时也只能使用一些 top-k 的策略，精细化程度不够。**一种技巧是在 document 和 chunks 中增加一些描述信息，为后续的检索和生成提供更多效果提升和管控的可能性。这就是元数据（metadata），用来描述文档或者 chunks 的附加信息，如标题，摘要，问题集，关键词，所有者等。在后续根据 query 进行匹配时，就可以先根据元数据过滤（比如时间、内容源等），再找到原始 chunk。**从使用场景上看，metadata 可以用于提升效果，内容引用溯源，document 级甚至 chunk 级的权限控制，排序，过滤，后处理等业务策略等。</p>
<h3 id="423-文档召回">4.2.3. 文档召回<a hidden class="anchor" aria-hidden="true" href="#423-文档召回">#</a></h3>
<p>文档召回过程中如何保证召回内容跟问题是相关的？ 或者说，如何尽可能减少无关信息？ 召回数据相关性的影响方面很多，既包括文档的切分，也包括文档 query 输入的清晰度，因此现在也出现了 query 改写、多召回策略以及排序修正等多个方案。</p>
<p><strong>a.优化召回的输入</strong></p>
<p>常见的是query 改写，为什么需要改写？一方面改写能提高召回系统的召回率；另一方面能让整个系统对用户的输入更鲁棒。常见的有：同义词改写、拼音改写、前缀补全、丢词和留词、近义词拓展召回、过长提问的总结、多视角提问改写等。</p>
<p>在专业的垂直领域，待检索的文档往往都是非常专业的表述，而用户的问题往往是非常不专业的白话表达。所以直接拿用户的 query 去检索，召回的效果就会比较差。Keyword LLM 就是解决这个问题的一种方案。例如在 ChatDoctor 中会先让大模型基于用户的 query 生成一系列的关键词，然后再用关键词去知识库中做检索。ChatDoctor 是直接用 In-Context Learning 的方式进行关键词的生成。我们也可以对大模型在这个任务上进行微调，训练一个专门根据用户问题生成关键词的大模型。这就是 ChatLaw 中的方案，方案内容大致如下：</p>
<p>1）基于传统 NLP 的成分句法分析，提取名词短语；再通过短语间的依存关系，生成关键词列表</p>
<p>2）从完整语句的 Embedding，切换为关键词 Embedding：</p>
<ul>
<li>知识库构建时，基于单知识点入库，入库时提取关键词列表进行 Embedding，用于检索。</li>
<li>查询时，对用户的问题提取关键词列表进行 Embedding 后，从本地知识库检索多条记录。</li>
</ul>
<p>3）将单问句中的多知识点拆解后检索，将召回的多条记录交给 LLM 整合。</p>
<p>该方法的优势在于：</p>
<ul>
<li>相比传统 Embedding，大幅提升召回精准度。</li>
<li>支持单次交互，对多知识点进行聚合处理。而不必让用户手动分别查询单个知识点，然后让 LLM 对会话历史中的单个知识点进行汇总。</li>
<li>使用传统 NLP，在专项问题处理上，相比 LLM 提供更好的精度和性能。</li>
<li>减少了对 LLM 的交互频次；提升了交给 LLM 的有效信息密度；大大提升问答系统的交互速度。</li>
</ul>
<p><strong>b.优化召回输出</strong></p>
<ul>
<li>如果一篇文档与查询非常相关，但与已经呈现给用户的文档非常相似，那么这篇文档的边际收益可能就不大。MMR 是一种广泛应用于信息检索和自然语言处理领域的算法。MMR 的主要目标是在文档排序和摘要生成等任务中平衡相关性和新颖性。换句话说，MMR 旨在为用户提供既相关又包含新信息的结果。</li>
<li>输入文档重排：LLM 对位置是相对比较敏感的，得分好的放在首或尾，LLM 会重点关注。</li>
<li>可以召回小片段，溯源大片段，类似于 langchain 的 ParentDocumentRetriever 方法，主要是解决知识不全的问题。</li>
<li>也可以召回大片段，模型使用小片段，主要是想解决模型上下文长度限制的问题。</li>
</ul>
<h3 id="424-self-rag">4.2.4. Self-RAG<a hidden class="anchor" aria-hidden="true" href="#424-self-rag">#</a></h3>
<p>自反思的检索增强生成方法（SELF-RAG），通过按需检索和自我反思来改进 LLM 的生成质量。SELF-RAG 会训练一个任意的 LM，使其能够反思自己的生成过程，并生成任务输出和中间的特殊tokens（reflection tokens）。Self-RAG 主要步骤概括如下：</p>
<ul>
<li>判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回</li>
<li>平行处理每个片段：生成 prompt +一个片段的结果。PS： query + chunk ==&gt; 带有反思标记（relevant/supported/partital/inrelevant）的 chunk</li>
<li>使用反思字段，检查输出是否相关，选择最符合需要的片段；</li>
<li>再重复检索</li>
<li>生成结果会引用相关片段，以及标记输出结果是否符合该片段，便于查证事实。</li>
</ul>
<p>Self-RAG 的一个重要创新是 Reflection tokens (反思字符)：通过生成反思字符这一特殊标记来检查输出。这些字符会分为 Retrieve 和 Critique 两种类型，会标示：检查是否有检索的必要，完成检索后检查输出的相关性、完整性、检索片段是否支持输出的观点。模型会基于原有词库和反思字段来生成下一个 token。</p>
<h3 id="425-拒绝回复">4.2.5. 拒绝回复<a hidden class="anchor" aria-hidden="true" href="#425-拒绝回复">#</a></h3>
<p>拒绝回复其实很多场景会应用到，例如在面对用户提问的内容并不是目前我们预期支持的领域（客服场景问天气），或者用户所问超出观点类（如何看待俄乌问题）、黄反类问题、大模型安全（query 中带有诱导性错误）时，再就是知识库为空的情况，就需要进行拒绝，并给用户一些回复。常见的策略一般有这些：</p>
<ul>
<li>一句写死的回复：“哎呀，这方面的问题我还不太懂，需要学习下”。</li>
<li>用大模型生成，例如借助 prompt 引导生成一些安抚性的回复，“对不起，你问的问题，我好像还不太懂。你可以试试问问别的”。</li>
<li>使用推荐问或者追问的策略。比如：增加追问机制，只要在 Prompt 中加入“如果无法从背景知识回答用户的问题，则根据背景知识内容，对用户进行追问，问题限制在3个以内”。这个机制并没有什么技术含量，主要依靠大模型的能力。不过大大改善了用户体验，用户在多轮引导中逐步明确了自己的问题，从而能够得到合适的答案。（你是否在找以下几个问题XX；你描述的我好像不太懂，能再补充补充吗）</li>
</ul>
<h1 id="5-参考文档">5. 参考文档<a hidden class="anchor" aria-hidden="true" href="#5-参考文档">#</a></h1>
<ul>
<li><a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-1/">基于大语言模型知识问答应用落地实践 – 知识库构建（上）</a></li>
<li><a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-3/">基于大语言模型知识问答应用落地实践 – 知识召回调优（上）</a></li>
<li><a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-4/">基于大语言模型知识问答应用落地实践 – 知识召回调优（下）</a></li>
<li><a href="https://qiankunli.github.io/2023/09/25/llm_retrieval.html">LLM外挂知识库</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzA5MTIxNTY4MQ==&mid=2461141608&idx=1&sn=a10ecbf70ee01eb44bb76d9275cfb723&chksm=87396846b04ee150cc0009ca0755eb38d52d96a0d83d8a4ed23e1cf89d034e5280025f032a1d&scene=21#wechat_redirect">引入元数据(metadata)提升RAG架构下LLM应用的效果和管控精度</a></li>
<li><a href="https://mp.weixin.qq.com/s/EBaGXFOnNHmF_rj_NLf_Ww">基于ElasticSearch的混合检索实战&amp;原理分析</a></li>
<li><a href="https://mp.weixin.qq.com/s/4Ttd3hi13B4VdoPGauwMzA">大模型检索增强生成（RAG）系统进化指南</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/642125832">大模型+知识库/数据库问答实践过程的经验汇总（三）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/673465732">大模型RAG 场景、数据、应用难点与解决（四）</a></li>
</ul>
<p><a href="/">Filter table data</a><a href="/">Create a pivot table</a><a href="/">Create a chart from data series</a></p>
<p><a href="/users/tfac-settings.action">Configure buttons visibility</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E9%BB%84%E5%A9%B7/">黄婷</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/wiki/vscode%E7%9B%B8%E5%85%B3/">
    <span class="title">« 上一页</span>
    <br>
    <span>VSCode相关</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/wiki/docker%E4%B8%AD%E8%BF%90%E8%A1%8C%E8%99%9A%E6%8B%9F%E4%BA%BAcpu%E8%BF%87%E9%AB%98%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">
    <span class="title">下一页 »</span>
    <br>
    <span>docker中运行虚拟人CPU过高问题排查</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on x"
            href="https://x.com/intent/tweet/?text=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&amp;hashtags=%e9%bb%84%e5%a9%b7">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&amp;title=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;summary=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&title=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on whatsapp"
            href="https://api.whatsapp.com/send?text=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on telegram"
            href="https://telegram.me/share/url?text=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 大模型知识问答实现技术调研 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=%e5%a4%a7%e6%a8%a1%e5%9e%8b%e7%9f%a5%e8%af%86%e9%97%ae%e7%ad%94%e5%ae%9e%e7%8e%b0%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f%25E5%25A4%25A7%25E6%25A8%25A1%25E5%259E%258B%25E7%259F%25A5%25E8%25AF%2586%25E9%2597%25AE%25E7%25AD%2594%25E5%25AE%259E%25E7%258E%25B0%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://beian.miit.gov.cn/">粤ICP备2023039897号-1</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
