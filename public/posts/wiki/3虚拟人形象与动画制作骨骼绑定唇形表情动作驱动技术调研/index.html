<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 | PaperMod</title>
<meta name="keywords" content="（五）虚拟人资源">
<meta name="description" content="（五）虚拟人资源">
<meta name="author" content="王宇">
<link rel="canonical" href="http://localhost:1313/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/">
<meta name="google-site-verification" content="XYZabc">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7b92bf4867c997b58ec50f63451b83777173d5bd5593376852abd85746d09d71.css" integrity="sha256-e5K/SGfJl7WOxQ9jRRuDd3Fz1b1VkzdoUqvYV0bQnXE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="http://localhost:1313/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研" />
<meta property="og:description" content="（五）虚拟人资源" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/" />
<meta property="og:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta property="article:section" content="posts" />




<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png" />
<meta name="twitter:title" content="3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研"/>
<meta name="twitter:description" content="（五）虚拟人资源"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研",
      "item": "http://localhost:1313/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研",
  "name": "3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研",
  "description": "（五）虚拟人资源",
  "keywords": [
    "（五）虚拟人资源"
  ],
  "articleBody": " 11. 调研背景（目的） 22. 相关概念 2.12.1. 何为数字人？ 2.22.2. 2D和3D数字人 2.32.3. 超写实虚拟人 33. 当前技术现状 44. 虚拟人3D内容生产的技术流程 4.14.1. 3D建模（静态）： 4.24.2. 建模方式 4.2.14.2.1. 主要建模方式技术概况及相关公司 4.2.24.2.2. 建模方式一：手工建模 4.2.34.2.3. 建模方式二：仪器采集建模 4.2.44.2.4. 建模方式三：自动化建模 4.34.3. 绑定 4.3.14.3.1. 骨骼绑定 4.3.24.3.2. 混合变形 4.3.34.3.3. 重定位 4.44.4. 驱动（动态） 4.4.14.4.1. 驱动类型 4.4.24.4.2. 主要驱动技术概况及相关公司 4.54.5. 渲染 4.64.6. 换装 55. 思考与总结 1. 调研背景（目的） 目前小万PTA写实型形象正处于前期制作阶段，本次调研目的是为了通过了解目前虚拟人形象与动画的制作、骨骼绑定、动作驱动等技术发展状况及虚拟人3D形象资源的制作流程，来辅助我们能够从比较专业的角度进行虚拟人形象资源的选款以及对接第三方公司的制作进程工作。\n2. 相关概念 2.1. 何为数字人？ 数字人是多技术综合产物，数字人近年的发展来源于CG(Computer Graphics，利用计算机进行视觉设计和生产)、语音识别、图像识别、动捕等相关技术的共同成熟。 数字人具有以下三方面特征，分别是由建模、物理仿真、渲染、动捕、面捕和AI等技术支持，各项技术不断迭代，推动数字人制作效能和智能水平提升，其中通过AI技术实现高度拟人化的“思想和行为”，进而给用户带来亲切感、参与感、互动感和沉浸感是未来发展的重要方向。 （1）人的外观，具有人的相貌、性别和性格等人物特征。\n（2）人的行为，具有语言、面部表情和肢体动作的能力。\n（3）人的思想，具有识别外界环境，并与人交流互动的能力。\n2.2. 2D和3D数字人 当前市场上的虚拟数字人，根据偶人物图形维度，可分为2D和3D两大类，从外形来看上可分为卡通Q版、二次元、偏写实、超写实四种类型。\n2D数字人和3D数字人生成方式不同，2D数字人本质是生成图像，因此主要方式是利用深度学习技术根据视频生成与真人相同的图像，技术相对成熟。 3D数字人需要建立3D模型，主要是利用软件传统的手工建模、静态扫描建模（相机阵列扫描建模）、动态光场或者AI建模（3D重建技术）生成3D模型。目前手工建模和静态扫描建模技术较为成熟且应用广泛，利用AI建模技术或动态光场重建可极大节约建模时间，提升建模效率，是未来重要布局方向。其中动态光场重建优势在于重建人物的几何模型同时还可一次性获取动态的人物模型数据，并高品质重现不同视角下观看人体的光影效果 。 2.3. 超写实虚拟人 超写实虚拟人是指通过技术合成、尽可能贴合真人的“虚拟形象”。而在“未来元宇宙”领域里，超写实虚拟人更可能成为人群与场景连接的最新工具。比如Ling、柳夜熙等就是典型的3D超写实虚拟数字人。超写实是指人物外观仿真度高，栩栩如生，这种虚拟人需要面部面数在1万面以上，高精度经得起360度无死角的怼拍。面部材质不仅十分接近真实皮肤的质感，还可以根据相机的距离进行自动优化，皮肤、五官、头发、肢体几近真人。\n这种3D超写实虚拟数字人是怎么创造出来的？简单来说，一个3D虚拟数字人的形象制作需要经过形象生成、动画生成两个环节。形象生成决定了虚拟人的长相，就是我们的外壳，包括头、躯干、四肢等，动画生成能够让虚拟人灵活地动起来，就是身体的动作，面部的表情，说话时的嘴形等。\n其中，静态外形的诞生主要依赖于各种建模技术；动态的产生则要依赖各类驱动技术。对于虚拟人而言，想让静态和动态联动起来，二者之间必须的一个桥梁就是骨骼绑定，通过对身体各个骨点的绑定，来达到控制各个身体部分动起来的目的，如下图所示：\n3. 当前技术现状 技术集综合迭代驱动数字人形似人，制作效能将继续提升 建模技术的发展推动超写实数字人制作门槛、成本和周期进一步下降。 物理仿真算法迭代推动服装动态展示趋向真实 布料仿真一直是CG动画中的研究热点与难点，对提高动画质量以及用户体验具有重要意义。虚拟世界中虚拟角色强烈的视觉真实感主要来源于逼真的虚拟人物的服装动画，布料的仿真程度很大程度上可以增强用户的体验感。 将服装通过骨骼绑定驱动的方式是市场上最为常见的应用方式。这种应用方式下，数字服装作为刚性物体被驱动，只能进行拉伸，好处是对于服装解算没有算力消耗，因此，更常见于实时互动应用中。 基于位置的动力学(Position Based Dynamics,PBD)技术，面数较大容易出现网格穿模，动力学表现生硬，且较难区分不同面料质感的物理差异，修型的工作量极大，单件服装的效果调优无法被负责且较难体现布料细节褶皱。 物理仿真算法的服装把动力学解算能够无限接近真实世界的服装表现，能够很好的表达服装的物理质感，服装与人体的关系，同时也不依赖于美术的高复杂且繁重的美术工作，但是 过于依赖端的计算能力，移动端只能够通过云端服务器的计算然后进行数据网络传输，进而对网络传输速度也有了极高的要求。 基于AI算法的服装动态模拟在学术上有所表现，但是成熟度较低，目前无法较好的处理多层级服装的效果解算，服装的细节表现也较弱，但是仍有较大的潜力与可能性。 渲染 算法 面部 4. 虚拟人3D内容生产的技术流程 4.1. 3D建模（静态）： 建模可以简单理解为在虚拟世界“捏泥人”的过程。模型师一般需要原画或照片做参考，制作尽量还原参考图的3D模型。建模主要分为建模和贴图两大部分，可以简单地将建模的过程理解为“塑骨”，贴图的过程理解为“美容”。\n建模 （Modeling）\n分角色建模和场景建模，建模师根据原画建立基本模型把控造型，这是整个游戏美术设计的重要环节。\n贴图（Texturing）\n将制作好的材质平面图覆盖在模型表面。场景道具是什么颜色？是金属的还是木材的，是崭新的还是破旧的？角色是年轻的还是苍老的？皮肤是光滑细腻还是满是雀斑皱纹的？······这些模型的外观表现都是由贴图决定。\n值得注意的是：建模师通常不是直接在立体的模型上进行“装扮”，而是将三维的模型展开为二维的平面后再进行贴图、增添细节等，这个就叫“UV拆分”或“展UV”。\n4.2. 建模方式 4.2.1. 主要建模方式技术概况及相关公司 序号\n数字人类型\n生成/建模方式\n精度\n建模/扫描时间\n特点\n数字人外形方向\n应用领域\n技术成熟度\n国内相关公司\n国外相关公司\n1\n2D数字人\n深度学习\n完全复刻\n小时-天\n可以定制形象或者融合形象，均和真人无异\n真人\n传媒、金融、政务\n相对成熟\n2\n3D数字人\n手工建模\n0.1毫米\n月\n人工制作，周期较长，目前仍广泛使用\n各种类型\n电影、传媒、行业应用\n成熟\n相芯科技、唯物（杭州）科技有限公司\n虚幻引擎、Reallusion\n3\n相机阵列扫描建模\n毫米\n高速，后续需要人工进一步完善模型\n重建数字人三维模型\n重建数字人纹理贴\n超写实\n电影、传媒\n相对成熟\n4\n光场扫描\n微米\n高速\n重建数字人三维模型、纹理贴图、法线贴图、材质贴图、动态网格\n超写实\n电影、传媒\n落地完善中\n元境科技、影眸科技\n5\nAI建模\n相对较高\n1分钟生成\n成本低，可定制，1分钟生成，实现千人千面，简单易用，零基础使用\n写实\n互联网\n发展中，需要多种技术路径验证\n相芯科技、唯物（杭州）科技有限公司、北京深镀科技、影眸科技\n虚幻引擎、Reallusion\n4.2.2. 建模方式一：手工建模 指通过3D建模软件来人工塑造出3D的模型，该方式人工制作周期较长，但效果可控，是目前应用最广泛的建模手段。\n常用的3D建模软件有很多，主要有以下几类：\n传统3D建模：3Dmax、Maya、blender等 雕刻软件：zbrush、blender等 程序化建模：houdini等 其中，传统3D软件主要负责制作低模，雕刻软件可以辅助制作高模。\n低/中/高模：通常一个物体是由几个点线面组成，一个多边形可以认定为一面。习惯上把一个三维模型有多少个多边形称之为多少面，即模型的面数是多少。低模的特点是面数少，视觉效果一般，但所占计算资源少，运行速度快；高模则正好相反，面数多，视觉效果好，但占用资源多，容易卡顿。\n3D建模分为3D手绘建模和次世代建模：\n传统3D手绘建模：简单说就是3D设计师根据原画，通过3D制作的形式还原原画3D造型，大概流程是先作低模，然后直接手工画贴图，结构上的材质等信息全靠人手作画，这种方式只能做出比较卡通的模型，做不出特别精致的效果，制作步骤：3Dmax建模——UV拆分——手绘贴图。 次世代建模：大概流程是先做中模，然后用zbrush等软件做雕刻使其变成高模，然后按照高模的布线原则重建一个低模，最后再把高模各个面的贴图拆分，烘焙到低模上，这样一来，模型结构是低模的，上面的贴图是逼真的高模渲染出来的，因此看上去既真实，又不卡内存，次世代模式可以做出非常精致的模型，制作步骤：中模起模——高模精雕——低模拓扑——UV拆分——烘焙贴图——绘制材质——引擎渲染。 下面这块砖头很好的解释了高模和低模的区别(雕刻软件的「雕刻」二字含义就是精细的雕出坑坑洼洼的细节，使其看上去更真实)。\n次世代建模流程中一般会使用“烘培”的方法，简单来说就是底层结构是低模，但是在低模的面上贴上高模的贴图，类似于「披着羊皮的狼」，达到一种看上去视觉效果很好，运行速度又快的效果。\n4.2.3. 建模方式二：仪器采集建模 相比于手工建模，仪器采集建模是通过仪器扫描的方式来进行建模。该方式成本较高，目前一般用于影视特效制作等领域居多。仪器采集建模技术分为静态扫描建模和动态光场重建：\n静态扫描模型技术是目前的主流，可具体细分为结构光扫描重建与相机阵列扫描重建等 动态光场重建技术是目前重点发展的方向，不仅可以重建人物的几何模型，还可一次性获取动态的人物模型数据，并高品质重现不同视角下观看人体的光影效果，具有高视觉保真度 4.2.4. 建模方式三：自动化建模 自动化建模主要包含以下一些方式：\n图像采集建模：通过采集照片来还原人脸 3D 结构 AI建模：利用AI算法直接生成人脸、身体等的建模方式 4.3. 绑定 绑定技术是动态与静态联动的桥梁，建模完成后要想让冰冷的模型动起来，还需要进行一系列绑定和驱动。简单来说就是给做好的虚拟小人在关键位置打上点，方便后续通过驱动关键点来驱动小人做出各种表情与姿态。关键点的位置遍布全身，例如躯干上，手肘、手腕、膝盖、脚踝等关节就是关键点；面部的眼皮、嘴角、眉头等关键位置也要打上关键点，让虚拟小人“眉飞色舞”。\n4.3.1. 骨骼绑定 骨骼绑定顾名思义，就是在模型里架设一套骨骼，由每个骨骼的位移、旋转带动其附近表面的网格顶点同步运动，而关节处的网格顶点会同时受多个附近骨骼的影响。所以骨骼绑定的核心工作是确定各个顶点受各个骨骼影响的权重关系，这个过程也叫做蒙皮。\n躯体部分的绑定的流程如下：\n创建骨骼 我们看到的3D动画是这样的\n但是对于计算机来讲它是这样的。\n模型是由大量顶点（Vertex）组成的，如果每一帧都要手动移动如此大量的顶点到指定位置，显然是个不可能完成的任务。于是人们借鉴了动物骨骼和皮肤的关系，为模型也设计了虚拟的骨骼。骨骼（Skeleton）有时也称作骨架（Armature），就如同人体的骨骼一样由一根根骨头（Bone）组成。\n下图是一个简单的人体骨骼\n添加骨骼的联动（例如脚踝抬起时膝盖也会自然弯曲） 有了骨骼，控制起来方便多了。但是我们还想在为角色摆姿势时更加方便。于是人们借鉴了机械设计的原理，设计出了一些骨骼约束，并且通过巧妙组合这些约束以及添加一些控制器，把一些复杂的可能需要移动很多骨骼才能实现的一个姿势，只移动一两个控制器就可以实现。\n为骨骼蒙皮 蒙皮就是把骨骼和模型结合起来的过程。\n这样每一个骨头控制着附近区域的顶点。当骨头移动时，就会牵引着它控制的骨头一起移动。\n调整权重（让虚拟人在运动时肌肉的形变更加自然） 刚才说过，一根骨头可以控制很多顶点。同时一个顶点也可以被多根骨头控制。这时就需要我们分配这些骨头对该顶点的控制权，这个控制权就是所谓的权重。同一个骨架和同一个模型，权重的配置不同，最终生成的动画效果也会有很大差异。\n4.3.2. 混合变形 骨骼带来的顶点变换很粗矿，无法完成对嘴形状的定制，因为这样一个看似简单的外表，实际上在模型中会涉及数以万计的顶点进行不同规律的变换。于是，我们就为这一组顶点变换专门设置变形器，业界一般称为“Morph Target”或“Blend Shape”。这种变换的原理是给顶点准备一个基准位置，再提供一个”极端变化“后的最大位置，之后乘以一定的”权重比例“，就能让这个顶点在基准位置和极端位置中的任一位置。但因为脸部需要局部变形的地方太多，且一个变形可能涉及数以万计的顶点，所以对实时计算压力也是不小的。\n两种技术比较来说，骨骼绑定简单高效但不够灵活，而混合变形更加自由灵活但制作和计算成本都较高。所以在实际开发中，哪些用骨骼，哪些用混合变形，这是一个权衡“效果”与“效率”的工作，需要具体去看，反复拿捏。\n4.3.3. 重定位 骨骼层次相同，但骨骼比例不同，如何重定位？\n动画重定位 是一种允许在共用相同骨架资源但比例差异很大的角色之间复用动画的功能。在我们接触业务时，我们知道不同的骨骼比例模型，动画和服装的绑定也不同，那么通过重定位，可以防止生成动画的骨架在使用来自不同外形的角色的动画时丢失比例或产生不必要的变形。 通过动画重定位，还可以在使用 不同骨架 资源的角色之间共享动画，前提是他们使用相似的骨骼层级，并使用名为 绑定（Rig） 的共享资源在骨架之间传递动画数据。\n1、假如有多个角色，你希望在基本角色和矮壮角色和高瘦角色之间共享动画。\n2、重定位前的结果\n在应用重定位前，您就可以在任何共用相同骨架资源的骨骼网格之间使用动画。但是，如果角色身材比例如上图所示有差异，就会得到一些很难看的结果。请注意矮个角色是如何被不必要地拉长的，高个角色又是如何被压短的，这都是系统为了使其符合基本角色的骨骼比例而进行的更改。\n3、重定位后的结果\n对角色应用重定位以后，系统就不再考虑它们的比例差异，动画会在每个角色身上正常播放。图中可查看原骨架（以米黄色显示）和当前骨架（白色）的差异。在基本角色身上，米黄色的非重定位骨骼与骨架是完美重合的。\n4.4. 驱动（动态） 4.4.1. 驱动类型 虚拟人驱动可以分为交互型、非交互型两种，他们的制作成本和灵活度区别也很大。\n驱动类型\n驱动方式\n主要特点\n成本\n技术流程\n技术成熟度\n适用场景\n非交互型\n预制动作\n不能实现实时互动\n成熟\n动画片、宣传视频等\n交互型\n真人驱动方法\n真人驱动，在动作灵活度、互动效果等方面有明显优势\n真人+动捕设备\n使用门槛较高\n形象设计—建模绑定—动捕设备或摄像头将基于真人的动作/表情等驱动数字虚拟人—实时渲染—完成内容录制或现场互动\n成熟\n虚拟偶像、大型直播、游戏等\n智能驱动方法\n语言表达、面部表情、具体动作将主要通过深度学习的运算结果实时或离线驱动\nAI技术，造价成本低\n形象设计—建模绑定—训练各类驱动的深度模型，学习模特语言、唇形、表情参数间的潜在映射关系—内容制作，基于输入的语言（或TTS），预测唇动、表情等参数，推理图片与时间戳结合—渲染并生成内容\n技术有限\n虚拟客服、虚拟助手等\n非交互型主要通过设置预制动作来让人物动起来，类似于动画片的原理，不能实现实时互动。\n交互型虚拟人是我们的重点。交互型虚拟人需要靠驱动技术来驱动**动作、表情、嘴形，**这样，虚拟人才能做到根据外界刺激进行反馈的效果。交互型数字人的驱动可以分为真人驱动方法和智能驱动方法。\n真人驱动方法：需要”真人+动捕设备”来进行驱动，通过捕捉技术采集真人演员的动作和面部表情数据之后将这些数据迁移合成到虚拟数字人身上，光学捕捉和惯性捕捉是常见的动捕方式，使用门槛较高。而近年来基于计算机视觉的捕捉技术发展迅猛，超写实虚拟数字人的面部表情捕捉还可以通过一个景深摄像头采集真人的面部3D点阵云图，然后实时地将面部动作和表情迁移到虚拟人的身上，相比需要穿戴动捕设备、租赁动捕棚的方式，驱动虚拟人只需要一台手机，使操作流程更加方便。 智能驱动方法：智能驱动是指通过AI技术，例如CV、ASR、TTS等来对虚拟人进行驱动，该方式造价成本低，可以无限拓展，在未来有很大的想象空间。不过现阶段AI技术有限，一般需要结合合适的场景，通过较多垂直领域的训练才能达到商业可用的效果。 4.4.2. 主要驱动技术概况及相关公司 **智能合成：**2D、3D数字人均已实现嘴型动作的智能合成，其他面部/身体部位的动作智能合成未能完全实现。 **动捕：**通过将捕捉采集的动作迁移至数字虚拟人是目前动作生成主要方式，核心技术是动作捕捉。可分为光学式、惯性式及计算机视觉动捕等。现阶段光学式和惯性式动捕占据主导。计算机视觉动捕虽然相对开发难度大，目前精度较低，但就成本/对环境要求低，可移动范围大，使用场景想象力较大，目前已有消费级应用（部分VR设备采用），成为聚焦热点。 项目\n分类\n技术\n优缺点\n国外相关技术和公司\n国内相关技术和公司\n智能合成\n唇形动作\n建立输入文本到输出语言与视觉信息的相关映射，主要是对于已采集到的文本到语言（TTS）和嘴型动画的数据模型训练，得到输入任意文本都可以驱动嘴型的模型，通过模型智能合成\n已实现智能合成\nReallusion公司的Craytalk技术\n搜狗、相芯科技等\n唇形之外的动作\n眨眼、点头、挑眉等动画目前都是通过采用一直随机策略或某个脚本策略将预制好的3D动作进行循环播放来实现，触发策略是通过人手动配置得到\n尚未实现智能合成，未来希望通过智能分析文本，学习人类的表达，实现制动配置\nZiva Dynamics\n影眸科技\n动作/面部捕捉\n光学动捕\n通过对目标上特定光点的监视和跟踪来完成运动捕捉的任务，即在真人身上粘贴能够反射红外光的马克点，通过摄像头对反光马克点的追踪，从而对真人动作进行捕捉\n造价昂贵，捕捉精度高\n英国的Vicon，美国的OptiTrack(NP)和魔神\nNoKov、uSens、青瞳视觉等\n惯性动捕\n基于惯性测量单元IMU来完成对人体动作的捕捉，即把集成了加速度计、陀螺仪、磁力计的IMU绑定在人体特定骨骼节点上，通过算法对测量数据进行计算，从而完成动作捕捉。\n价格相对低廉，精度较低，会睡着连续使用时间的增加产生累积误差，发生位置漂移，扛遮挡力强\n荷兰的Xsens\n诺亦腾（Noitom)、幻境、国承万通等。\n计算机视觉动捕\n由多个高速相机从不同角度对目标进行监视和跟踪\n简单、易用、硬件成本较低\nLeap Motion、微软Kinect\n针对光学捕捉、惯性捕捉、视觉捕捉做更进一步的综合对比：\n4.5. 渲染 定义：渲染是对3D数字人或虚拟人加入几何、视点、纹理、照明和阴影等信息从而达成从模型到图像的转变，渲染决定了最终数字人的质量，而渲染引擎和GPU算力的发展推动了数字人渲染更加信息和实时化，渲染分为实时渲染和离线渲染。\n目前离线渲染比较成熟，应用较为广泛。而实时渲染尚有提升空间，伴随GPU算力的不断提升和渲染引擎的优化，将推动实时渲染的速度和真实度，未来实时渲染技术的发展也为数字人的实时交互提供了极大助力。 主流的3D渲染引擎UnrealEngine和Unity3D版本不断迭代，推动数字人皮肤纹理、3D效果、质感和细节等方面渲染效果更佳，同时生产效率更高。 渲染方式\n离线渲染\n实时渲染\n定义\n在计算出画面时并不显示画面，计算机根据预先定义好的光线、轨迹渲染图片，渲染完成后再将图片连续播放，实现动画效果。\n图形数据实时计算与输出，每一帧都是针对当前实际的环境光源、相机位置和材质参数计算出的图像。\n渲染时间\n长、花费几十分钟甚至更长时间渲染一帧画面\n短、每秒至少渲染30帧\n计算资源\n多，受时效限制有限，可临时调配更多计算资源\n少，受限于时效要求，计算资源一般不能及时调整\n渲染质量\n高\n中\n数字人交互\n无\n有交互/无交互\n优缺点\n强调美学和视觉效果，主要优点是渲染时可以不考虑时间对渲染效果的影响；缺点是渲染画面播放时用户不能实时控制物体和场景。\n强调“交互性和实时性”，优点是就可以实时操控；缺点是要受系统的负荷能力的限制，必要时要牺牲画面效果（模型的精细、光影的应用贴图的精细程度）来满足实时系统的要求。\n应用领域\n广告营销/影视等预先设计好的模式的演示\n直播/行业数字客服/游戏等无预定脚本场景\n4.6. 换装 现实生活中，衣服穿在身上它就和皮肤紧贴着或者有一定空隙，这种想法放到虚拟世界中其实非常难实现。因为皮肤和衣服其实就是Mesh（网格），当衣服穿在身上时实际上是两组网格“碰撞”在一起，于是会引出如下两个问题：\n如何在身体做动作时，衣服也跟着身体“做动作”？身体是拥有骨骼的，骨骼外包裹着一层“皮肤”，用同样思路，衣服实际上也是包裹在同样骨骼上的一层“皮肤”。将身体和衣服采用同一套骨骼模板，并在渲染时实现了两份骨骼数据的实时“同步”。 如何解决身体的网格穿透到衣服外？用相同骨骼的方案解决衣服“穿”在身上的效果非常巧妙，但也容易出现问题，比如某件衣服局部非常内凹，那么就很容易出现身体的皮肤突出在衣服之外的问题，俗称“穿模”。因为仔细调整道具成本实在太高了，所以我们也做了个取巧的方案：通过把人体进行“切割”，并对每件衣服遮挡的人体部位进行标记，当渲染某件衣服时，直接隐藏被遮挡部位皮肤Mesh即可。 有了这两项技术后，我们就可以批量的生产衣服了，而换装仅仅只是加载不同的模型而已，无须准个特殊处理也能达到目的。\n5. 思考与总结 1、通过了解第三方公司提供能力以及万得厨的硬件能力，大概清晰了小万的PTA形象资源应该采用的是手工建模中的次世代建模方式，让低模以更低的面数展现更多的细节，面数低的模型能够降低虚拟人对资源的占用，保证终端设备能够流畅运行，之后虚拟人形象资源的选款也应该尽量选择细节简单，褶皱少的款式，尽量将模型的面数控制在一个较低的范围内。 2、目前小万使用的应该是手工建模，还无法做到我们所期望的千人千面。自动化建模技术还不算特别成熟，建模结果到直接商用还有一段距离，不过，该类技术会大大降低建模的人力成本和时间成本。目前已经出现了一些支持虚拟人创建的工具化平台，如国内唯物（杭州）科技有限公司的NextHuman、国外虚幻引擎的 MetaHuman Creator 等。尤其是2022年6月虚幻引擎发布的MetaHuman Creator ，其效果令人惊艳。这些平台的建模精度虽不足以建立超高质量的模型，但能够大幅降低虚拟人建模的成本，让普通人也能快速拥有属于自己的虚拟形象。随着技术的发展，自动化建模的效果还会变得越来越好。在未来，这也是小万PTA形象发展的方向，和元宇宙入口、虚拟分身、千人千面等概念联系起来。\n3、目前的技术从成熟趋向于简单，建模技术由手工建模逐渐演变成通过AI技术能直接生成3D模型，面部捕捉的技术也趋向更简单的硬件、更细腻的表情，影眸科技的“一句话人脸资产生成”，使用一段描述生成/编辑3D数字形象模型与高精度PBR材质，使用单张照片生成/编辑3D数字形象模型与高精度PBR材质就极大程度的减少了用户使用门槛。\nFilter table dataCreate a pivot tableCreate a chart from data series\nConfigure buttons visibility\n",
  "wordCount" : "359",
  "inLanguage": "zh",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "王宇"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "PaperMod",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/%3Clink%20/%20abs%20url%3E"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Home (Alt + H)">
                        
                    <img src="http://localhost:1313/images/msg_hu15231257772499651944.png" alt="" aria-label="logo"
                        height="20">Home</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">主页</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研
    </h1>
    <div class="post-description">
      （五）虚拟人资源
    </div>
    <div class="post-meta">2 分钟&nbsp;·&nbsp;王宇&nbsp;|&nbsp;<a href="https://github.com/WangShaoyu1/myBlogPaperMod/tree/master/content/posts/wiki/3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94.md" rel="noopener noreferrer" target="_blank">Suggest Changes</a>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-%e8%b0%83%e7%a0%94%e8%83%8c%e6%99%af%e7%9b%ae%e7%9a%84" aria-label="1. 调研背景（目的）">1. 调研背景（目的）</a></li>
                <li>
                    <a href="#2-%e7%9b%b8%e5%85%b3%e6%a6%82%e5%bf%b5" aria-label="2. 相关概念">2. 相关概念</a><ul>
                        
                <li>
                    <a href="#21-%e4%bd%95%e4%b8%ba%e6%95%b0%e5%ad%97%e4%ba%ba" aria-label="2.1. 何为数字人？">2.1. 何为数字人？</a></li>
                <li>
                    <a href="#22-2d%e5%92%8c3d%e6%95%b0%e5%ad%97%e4%ba%ba" aria-label="2.2. 2D和3D数字人">2.2. 2D和3D数字人</a></li>
                <li>
                    <a href="#23-%e8%b6%85%e5%86%99%e5%ae%9e%e8%99%9a%e6%8b%9f%e4%ba%ba" aria-label="2.3. 超写实虚拟人">2.3. 超写实虚拟人</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e5%bd%93%e5%89%8d%e6%8a%80%e6%9c%af%e7%8e%b0%e7%8a%b6" aria-label="3. 当前技术现状">3. 当前技术现状</a></li>
                <li>
                    <a href="#4-%e8%99%9a%e6%8b%9f%e4%ba%ba3d%e5%86%85%e5%ae%b9%e7%94%9f%e4%ba%a7%e7%9a%84%e6%8a%80%e6%9c%af%e6%b5%81%e7%a8%8b" aria-label="4. 虚拟人3D内容生产的技术流程">4. 虚拟人3D内容生产的技术流程</a><ul>
                        
                <li>
                    <a href="#41-3d%e5%bb%ba%e6%a8%a1%e9%9d%99%e6%80%81" aria-label="4.1. 3D建模（静态）：">4.1. 3D建模（静态）：</a></li>
                <li>
                    <a href="#42-%e5%bb%ba%e6%a8%a1%e6%96%b9%e5%bc%8f" aria-label="4.2. 建模方式">4.2. 建模方式</a><ul>
                        
                <li>
                    <a href="#421-%e4%b8%bb%e8%a6%81%e5%bb%ba%e6%a8%a1%e6%96%b9%e5%bc%8f%e6%8a%80%e6%9c%af%e6%a6%82%e5%86%b5%e5%8f%8a%e7%9b%b8%e5%85%b3%e5%85%ac%e5%8f%b8" aria-label="4.2.1. 主要建模方式技术概况及相关公司">4.2.1. 主要建模方式技术概况及相关公司</a></li>
                <li>
                    <a href="#422-%e5%bb%ba%e6%a8%a1%e6%96%b9%e5%bc%8f%e4%b8%80%e6%89%8b%e5%b7%a5%e5%bb%ba%e6%a8%a1" aria-label="4.2.2. 建模方式一：手工建模">4.2.2. 建模方式一：手工建模</a></li>
                <li>
                    <a href="#423-%e5%bb%ba%e6%a8%a1%e6%96%b9%e5%bc%8f%e4%ba%8c%e4%bb%aa%e5%99%a8%e9%87%87%e9%9b%86%e5%bb%ba%e6%a8%a1" aria-label="4.2.3. 建模方式二：仪器采集建模">4.2.3. 建模方式二：仪器采集建模</a></li>
                <li>
                    <a href="#424-%e5%bb%ba%e6%a8%a1%e6%96%b9%e5%bc%8f%e4%b8%89%e8%87%aa%e5%8a%a8%e5%8c%96%e5%bb%ba%e6%a8%a1" aria-label="4.2.4. 建模方式三：自动化建模">4.2.4. 建模方式三：自动化建模</a></li></ul>
                </li>
                <li>
                    <a href="#43-%e7%bb%91%e5%ae%9a" aria-label="4.3. 绑定">4.3. 绑定</a><ul>
                        
                <li>
                    <a href="#431-%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a" aria-label="4.3.1. 骨骼绑定">4.3.1. 骨骼绑定</a></li>
                <li>
                    <a href="#432-%e6%b7%b7%e5%90%88%e5%8f%98%e5%bd%a2" aria-label="4.3.2. 混合变形">4.3.2. 混合变形</a></li>
                <li>
                    <a href="#433-%e9%87%8d%e5%ae%9a%e4%bd%8d" aria-label="4.3.3. 重定位">4.3.3. 重定位</a></li></ul>
                </li>
                <li>
                    <a href="#44-%e9%a9%b1%e5%8a%a8%e5%8a%a8%e6%80%81" aria-label="4.4. 驱动（动态）">4.4. 驱动（动态）</a><ul>
                        
                <li>
                    <a href="#441-%e9%a9%b1%e5%8a%a8%e7%b1%bb%e5%9e%8b" aria-label="4.4.1. 驱动类型">4.4.1. 驱动类型</a></li>
                <li>
                    <a href="#442-%e4%b8%bb%e8%a6%81%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e6%a6%82%e5%86%b5%e5%8f%8a%e7%9b%b8%e5%85%b3%e5%85%ac%e5%8f%b8" aria-label="4.4.2. 主要驱动技术概况及相关公司">4.4.2. 主要驱动技术概况及相关公司</a></li></ul>
                </li>
                <li>
                    <a href="#45-%e6%b8%b2%e6%9f%93" aria-label="4.5. 渲染">4.5. 渲染</a></li>
                <li>
                    <a href="#46-%e6%8d%a2%e8%a3%85" aria-label="4.6. 换装">4.6. 换装</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e6%80%9d%e8%80%83%e4%b8%8e%e6%80%bb%e7%bb%93" aria-label="5. 思考与总结">5. 思考与总结</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><ul>
<li>1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-调研背景（目的）">1. 调研背景（目的）</a></li>
<li>2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-相关概念">2. 相关概念</a>
<ul>
<li>2.1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-何为数字人？">2.1. 何为数字人？</a></li>
<li>2.2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-2D和3D数字人">2.2. 2D和3D数字人</a></li>
<li>2.3<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-超写实虚拟人">2.3. 超写实虚拟人</a></li>
</ul>
</li>
<li>3<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-当前技术现状">3. 当前技术现状</a></li>
<li>4<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-虚拟人3D内容生产的技术流程">4. 虚拟人3D内容生产的技术流程</a>
<ul>
<li>4.1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-3D建模（静态）：">4.1. 3D建模（静态）：</a></li>
<li>4.2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-建模方式">4.2. 建模方式</a>
<ul>
<li>4.2.1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-主要建模方式技术概况及相关公司">4.2.1. 主要建模方式技术概况及相关公司</a></li>
<li>4.2.2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-建模方式一：手工建模">4.2.2. 建模方式一：手工建模</a></li>
<li>4.2.3<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-建模方式二：仪器采集建模">4.2.3. 建模方式二：仪器采集建模</a></li>
<li>4.2.4<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-建模方式三：自动化建模">4.2.4. 建模方式三：自动化建模</a></li>
</ul>
</li>
<li>4.3<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-绑定">4.3. 绑定</a>
<ul>
<li>4.3.1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-骨骼绑定">4.3.1. 骨骼绑定</a></li>
<li>4.3.2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-混合变形">4.3.2. 混合变形</a></li>
<li>4.3.3<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-重定位">4.3.3. 重定位</a></li>
</ul>
</li>
<li>4.4<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-驱动（动态）">4.4. 驱动（动态）</a>
<ul>
<li>4.4.1<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-驱动类型">4.4.1. 驱动类型</a></li>
<li>4.4.2<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-主要驱动技术概况及相关公司">4.4.2. 主要驱动技术概况及相关公司</a></li>
</ul>
</li>
<li>4.5<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-渲染">4.5. 渲染</a></li>
<li>4.6<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-换装">4.6. 换装</a></li>
</ul>
</li>
<li>5<a href="/posts/wiki/3%E8%99%9A%E6%8B%9F%E4%BA%BA%E5%BD%A2%E8%B1%A1%E4%B8%8E%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C%E9%AA%A8%E9%AA%BC%E7%BB%91%E5%AE%9A%E5%94%87%E5%BD%A2%E8%A1%A8%E6%83%85%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/#id-3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研-思考与总结">5. 思考与总结</a></li>
</ul>
<h1 id="1-调研背景目的">1. 调研背景（目的）<a hidden class="anchor" aria-hidden="true" href="#1-调研背景目的">#</a></h1>
<p>目前小万PTA写实型形象正处于前期制作阶段，本次调研目的是为了通过了解目前虚拟人形象与动画的制作、骨骼绑定、动作驱动等技术发展状况及虚拟人3D形象资源的制作流程，来辅助我们能够从比较专业的角度进行虚拟人形象资源的选款以及对接第三方公司的制作进程工作。</p>
<h1 id="2-相关概念">2. 相关概念<a hidden class="anchor" aria-hidden="true" href="#2-相关概念">#</a></h1>
<h2 id="21-何为数字人">2.1. 何为数字人？<a hidden class="anchor" aria-hidden="true" href="#21-何为数字人">#</a></h2>
<ul>
<li>数字人是多技术综合产物，数字人近年的发展来源于CG(Computer Graphics，利用计算机进行视觉设计和生产)、语音识别、图像识别、动捕等相关技术的共同成熟。</li>
<li>数字人具有以下三方面特征，分别是由建模、物理仿真、渲染、动捕、面捕和AI等技术支持，各项技术不断迭代，推动数字人制作效能和智能水平提升，其中通过AI技术实现高度拟人化的“思想和行为”，进而给用户带来亲切感、参与感、互动感和沉浸感是未来发展的重要方向。</li>
</ul>
<p>（1）人的外观，具有人的相貌、性别和性格等人物特征。</p>
<p>（2）人的行为，具有语言、面部表情和肢体动作的能力。</p>
<p>（3）人的思想，具有识别外界环境，并与人交流互动的能力。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-4_17-46-51.png?version=1&amp;modificationDate=1680601612162&amp;api=v2" alt=""  />
</p>
<h2 id="22-2d和3d数字人">2.2. 2D和3D数字人<a hidden class="anchor" aria-hidden="true" href="#22-2d和3d数字人">#</a></h2>
<p>当前市场上的虚拟数字人，根据偶人物图形维度，可分为2D和3D两大类，从外形来看上可分为卡通Q版、二次元、偏写实、超写实四种类型。</p>
<ul>
<li>2D数字人和3D数字人生成方式不同，2D数字人本质是生成图像，因此主要方式是利用深度学习技术根据视频生成与真人相同的图像，技术相对成熟。</li>
<li>3D数字人需要建立3D模型，主要是利用软件传统的手工建模、静态扫描建模（相机阵列扫描建模）、动态光场或者AI建模（3D重建技术）生成3D模型。目前手工建模和静态扫描建模技术较为成熟且应用广泛，利用AI建模技术或动态光场重建可极大节约建模时间，提升建模效率，是未来重要布局方向。其中动态光场重建优势在于重建人物的几何模型同时还可一次性获取动态的人物模型数据，并高品质重现不同视角下观看人体的光影效果 。</li>
</ul>
<h2 id="23-超写实虚拟人">2.3. 超写实虚拟人<a hidden class="anchor" aria-hidden="true" href="#23-超写实虚拟人">#</a></h2>
<p>    超写实虚拟人是指通过技术合成、尽可能贴合真人的“虚拟形象”。而在“未来元宇宙”领域里，超写实虚拟人更可能成为人群与场景连接的最新工具。比如Ling、柳夜熙等就是典型的3D超写实虚拟数字人。超写实是指人物外观仿真度高，栩栩如生，这种虚拟人需要面部面数在1万面以上，高精度经得起360度无死角的怼拍。面部材质不仅十分接近真实皮肤的质感，还可以根据相机的距离进行自动优化，皮肤、五官、头发、肢体几近真人。</p>
<p>    这种3D超写实虚拟数字人是怎么创造出来的？简单来说，一个3D虚拟数字人的形象制作需要经过形象生成、动画生成两个环节。形象生成决定了虚拟人的长相，就是我们的外壳，包括头、躯干、四肢等，动画生成能够让虚拟人灵活地动起来，就是身体的动作，面部的表情，说话时的嘴形等。</p>
<p>    其中，静态外形的诞生主要依赖于各种建模技术；动态的产生则要依赖各类驱动技术。对于虚拟人而言，想让静态和动态联动起来，二者之间必须的一个桥梁就是骨骼绑定，通过对身体各个骨点的绑定，来达到控制各个身体部分动起来的目的，如下图所示：</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-4_16-54-14.png?version=1&amp;modificationDate=1680598454224&amp;api=v2" alt=""  />
</p>
<h1 id="3-当前技术现状">3. 当前技术现状<a hidden class="anchor" aria-hidden="true" href="#3-当前技术现状">#</a></h1>
<ul>
<li>技术集综合迭代驱动数字人形似人，制作效能将继续提升</li>
</ul>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-4_17-58-55.png?version=1&amp;modificationDate=1680602335714&amp;api=v2" alt=""  />
</p>
<ul>
<li>建模技术的发展推动超写实数字人制作门槛、成本和周期进一步下降。</li>
<li>物理仿真算法迭代推动服装动态展示趋向真实</li>
</ul>
<ol>
<li>布料仿真一直是CG动画中的研究热点与难点，对提高动画质量以及用户体验具有重要意义。虚拟世界中虚拟角色强烈的视觉真实感主要来源于逼真的虚拟人物的服装动画，布料的仿真程度很大程度上可以增强用户的体验感。</li>
<li>将服装通过骨骼绑定驱动的方式是市场上最为常见的应用方式。这种应用方式下，数字服装作为刚性物体被驱动，只能进行拉伸，好处是对于服装解算没有算力消耗，因此，更常见于实时互动应用中。</li>
<li>基于位置的动力学(Position Based Dynamics,PBD)技术，面数较大容易出现网格穿模，动力学表现生硬，且较难区分不同面料质感的物理差异，修型的工作量极大，单件服装的效果调优无法被负责且较难体现布料细节褶皱。</li>
<li>物理仿真算法的服装把动力学解算能够无限接近真实世界的服装表现，能够很好的表达服装的物理质感，服装与人体的关系，同时也不依赖于美术的高复杂且繁重的美术工作，但是 过于依赖端的计算能力，移动端只能够通过云端服务器的计算然后进行数据网络传输，进而对网络传输速度也有了极高的要求。</li>
<li>基于AI算法的服装动态模拟在学术上有所表现，但是成熟度较低，目前无法较好的处理多层级服装的效果解算，服装的细节表现也较弱，但是仍有较大的潜力与可能性。</li>
</ol>
<ul>
<li>渲染</li>
<li>算法</li>
<li>面部</li>
</ul>
<h1 id="4-虚拟人3d内容生产的技术流程">4. 虚拟人3D内容生产的技术流程<a hidden class="anchor" aria-hidden="true" href="#4-虚拟人3d内容生产的技术流程">#</a></h1>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-10_16-40-5.png?version=1&amp;modificationDate=1681116005194&amp;api=v2" alt=""  />
</p>
<h2 id="41-3d建模静态">4.1. 3D建模（静态）：<a hidden class="anchor" aria-hidden="true" href="#41-3d建模静态">#</a></h2>
<p>建模可以简单理解为在虚拟世界“捏泥人”的过程。模型师一般需要原画或照片做参考，制作尽量还原参考图的3D模型。建模主要分为建模和贴图两大部分，可以简单地将建模的过程理解为“塑骨”，贴图的过程理解为“美容”。</p>
<p><em>建模</em> <em>（Modeling）</em></p>
<p>分角色建模和场景建模，建模师根据原画建立基本模型把控造型，这是整个游戏美术设计的重要环节。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_16-2-41.png?version=1&amp;modificationDate=1680768162366&amp;api=v2" alt=""  />
</p>
<p><em>贴图（Texturing）</em></p>
<p>将制作好的材质平面图覆盖在模型表面。<em>场景道具是什么颜色？是金属的还是木材的，是崭新的还是破旧的？角色是年轻的还是苍老的？皮肤是光滑细腻还是满是雀斑皱纹的？······这些模型的外观表现都是由贴图决定。</em></p>
<p><em><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_16-4-17.png?version=1&amp;modificationDate=1680768257169&amp;api=v2" alt=""  />
</em></p>
<p><em>值得注意的是：建模师通常不是直接在立体的模型上进行“装扮”，而是将三维的模型展开为二维的平面后再进行贴图、增添细节等，这个就叫“UV拆分”或“展UV”。</em></p>
<p><em><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_16-6-43.png?version=1&amp;modificationDate=1680768403499&amp;api=v2" alt=""  />
</em></p>
<h2 id="42-建模方式">4.2. 建模方式<a hidden class="anchor" aria-hidden="true" href="#42-建模方式">#</a></h2>
<h3 id="421-主要建模方式技术概况及相关公司">4.2.1. 主要建模方式技术概况及相关公司<a hidden class="anchor" aria-hidden="true" href="#421-主要建模方式技术概况及相关公司">#</a></h3>
<p>序号</p>
<p>数字人类型</p>
<p>生成/建模方式</p>
<p>精度</p>
<p>建模/扫描时间</p>
<p>特点</p>
<p>数字人外形方向</p>
<p>应用领域</p>
<p>技术成熟度</p>
<p>国内相关公司</p>
<p>国外相关公司</p>
<p>1</p>
<p>2D数字人</p>
<p>深度学习</p>
<p>完全复刻</p>
<p>小时-天</p>
<p>可以定制形象或者融合形象，均和真人无异</p>
<p>真人</p>
<p>传媒、金融、政务</p>
<p>相对成熟</p>
<p>2</p>
<p>3D数字人</p>
<p>手工建模</p>
<p>0.1毫米</p>
<p>月</p>
<p>人工制作，周期较长，目前仍广泛使用</p>
<p>各种类型</p>
<p>电影、传媒、行业应用</p>
<p>成熟</p>
<p>相芯科技、唯物（杭州）科技有限公司</p>
<p>虚幻引擎、Reallusion</p>
<p>3</p>
<p>相机阵列扫描建模</p>
<p>毫米</p>
<p>高速，后续需要人工进一步完善模型</p>
<p>重建数字人三维模型</p>
<p>重建数字人纹理贴</p>
<p>超写实</p>
<p>电影、传媒</p>
<p>相对成熟</p>
<p>4</p>
<p>光场扫描</p>
<p>微米</p>
<p>高速</p>
<p>重建数字人三维模型、纹理贴图、法线贴图、材质贴图、动态网格</p>
<p>超写实</p>
<p>电影、传媒</p>
<p>落地完善中</p>
<p>元境科技、影眸科技</p>
<p>5</p>
<p>AI建模</p>
<p>相对较高</p>
<p>1分钟生成</p>
<p>成本低，可定制，1分钟生成，实现千人千面，简单易用，零基础使用</p>
<p>写实</p>
<p>互联网</p>
<p>发展中，需要多种技术路径验证</p>
<p>相芯科技、唯物（杭州）科技有限公司、北京深镀科技、影眸科技</p>
<p>虚幻引擎、Reallusion</p>
<h3 id="422-建模方式一手工建模">4.2.2. 建模方式一：手工建模<a hidden class="anchor" aria-hidden="true" href="#422-建模方式一手工建模">#</a></h3>
<p>指通过3D建模软件来人工塑造出3D的模型，该方式人工制作周期较长，但效果可控，是目前应用最广泛的建模手段。</p>
<p>常用的3D建模软件有很多，主要有以下几类：</p>
<ul>
<li>传统3D建模：3Dmax、Maya、blender等</li>
<li>雕刻软件：zbrush、blender等</li>
<li>程序化建模：houdini等</li>
</ul>
<p>其中，传统3D软件主要负责制作低模，雕刻软件可以辅助制作高模。</p>
<p>低/中/高模：通常一个物体是由几个点线面组成，一个多边形可以认定为一面。习惯上把一个三维模型有多少个多边形称之为多少面，即模型的面数是多少。低模的特点是面数少，视觉效果一般，但所占计算资源少，运行速度快；高模则正好相反，面数多，视觉效果好，但占用资源多，容易卡顿。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_16-9-10.png?version=1&amp;modificationDate=1680768550189&amp;api=v2" alt=""  />
</p>
<p>3D建模分为3D手绘建模和次世代建模：</p>
<ul>
<li>传统3D手绘建模：简单说就是3D设计师根据原画，通过3D制作的形式还原原画3D造型，大概流程是先作低模，然后直接手工画贴图，结构上的材质等信息全靠人手作画，这种方式只能做出比较卡通的模型，做不出特别精致的效果，制作步骤：3Dmax建模——UV拆分——手绘贴图。</li>
</ul>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_15-31-7.png?version=1&amp;modificationDate=1680766267273&amp;api=v2" alt=""  />
</p>
<ul>
<li>次世代建模：大概流程是先做中模，然后用zbrush等软件做雕刻使其变成高模，然后按照高模的布线原则重建一个低模，最后再把高模各个面的贴图拆分，烘焙到低模上，这样一来，模型结构是低模的，上面的贴图是逼真的高模渲染出来的，因此看上去既真实，又不卡内存，次世代模式可以做出非常精致的模型，制作步骤：中模起模——高模精雕——低模拓扑——UV拆分——烘焙贴图——绘制材质——引擎渲染。</li>
</ul>
<p>下面这块砖头很好的解释了高模和低模的区别(雕刻软件的「雕刻」二字含义就是精细的雕出坑坑洼洼的细节，使其看上去更真实)。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-6_15-3-45.png?version=1&amp;modificationDate=1680764625888&amp;api=v2" alt=""  />
</p>
<p>次世代建模流程中一般会使用“烘培”的方法，简单来说就是底层结构是低模，但是在低模的面上贴上高模的贴图，类似于「披着羊皮的狼」，达到一种看上去视觉效果很好，运行速度又快的效果。</p>
<h3 id="423-建模方式二仪器采集建模">4.2.3. 建模方式二：仪器采集建模<a hidden class="anchor" aria-hidden="true" href="#423-建模方式二仪器采集建模">#</a></h3>
<p>相比于手工建模，仪器采集建模是通过仪器扫描的方式来进行建模。该方式成本较高，目前一般用于影视特效制作等领域居多。仪器采集建模技术分为静态扫描建模和动态光场重建：</p>
<ul>
<li>静态扫描模型技术是目前的主流，可具体细分为结构光扫描重建与相机阵列扫描重建等</li>
<li>动态光场重建技术是目前重点发展的方向，不仅可以重建人物的几何模型，还可一次性获取动态的人物模型数据，并高品质重现不同视角下观看人体的光影效果，具有高视觉保真度</li>
</ul>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-10_9-22-59.png?version=1&amp;modificationDate=1681089779583&amp;api=v2" alt=""  />
<img loading="lazy" src="/download/attachments/97901833/image2023-4-10_10-21-31.png?version=1&amp;modificationDate=1681093291699&amp;api=v2" alt=""  />
</p>
<h3 id="424-建模方式三自动化建模">4.2.4. 建模方式三：自动化建模<a hidden class="anchor" aria-hidden="true" href="#424-建模方式三自动化建模">#</a></h3>
<p>自动化建模主要包含以下一些方式：</p>
<ul>
<li>图像采集建模：通过采集照片来还原人脸 3D 结构</li>
<li>AI建模：利用AI算法直接生成人脸、身体等的建模方式</li>
</ul>
<h2 id="43-绑定">4.3. 绑定<a hidden class="anchor" aria-hidden="true" href="#43-绑定">#</a></h2>
<p>    绑定技术是动态与静态联动的桥梁，建模完成后要想让冰冷的模型动起来，还需要进行一系列绑定和驱动。简单来说就是给做好的虚拟小人在关键位置打上点，方便后续通过驱动关键点来驱动小人做出各种表情与姿态。关键点的位置遍布全身，例如躯干上，手肘、手腕、膝盖、脚踝等关节就是关键点；面部的眼皮、嘴角、眉头等关键位置也要打上关键点，让虚拟小人“眉飞色舞”。</p>
<h3 id="431-骨骼绑定">4.3.1. 骨骼绑定<a hidden class="anchor" aria-hidden="true" href="#431-骨骼绑定">#</a></h3>
<p>   骨骼绑定顾名思义，就是在模型里架设一套骨骼，由每个骨骼的位移、旋转带动其附近表面的网格顶点同步运动，而关节处的网格顶点会同时受多个附近骨骼的影响。所以骨骼绑定的核心工作是确定各个顶点受各个骨骼影响的权重关系，这个过程也叫做蒙皮。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-6_16-48-11.png?version=1&amp;modificationDate=1680770891516&amp;api=v2" alt=""  />
</p>
<p>躯体部分的绑定的流程如下：</p>
<ul>
<li>创建骨骼</li>
</ul>
<p>我们看到的3D动画是这样的</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-39-26.png?version=1&amp;modificationDate=1682404768068&amp;api=v2" alt=""  />
</p>
<p>但是对于计算机来讲它是这样的。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-39-58.png?version=1&amp;modificationDate=1682404799471&amp;api=v2" alt=""  />
</p>
<p>模型是由大量顶点（Vertex）组成的，如果每一帧都要手动移动如此大量的顶点到指定位置，显然是个不可能完成的任务。于是人们借鉴了动物骨骼和皮肤的关系，为模型也设计了虚拟的骨骼。骨骼（Skeleton）有时也称作骨架（Armature），就如同人体的骨骼一样由一根根骨头（Bone）组成。<br>
下图是一个简单的人体骨骼</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-41-53.png?version=1&amp;modificationDate=1682404913844&amp;api=v2" alt=""  />
</p>
<ul>
<li>添加骨骼的联动（例如脚踝抬起时膝盖也会自然弯曲）</li>
</ul>
<p>有了骨骼，控制起来方便多了。但是我们还想在为角色摆姿势时更加方便。于是人们借鉴了机械设计的原理，设计出了一些骨骼约束，并且通过巧妙组合这些约束以及添加一些控制器，把一些复杂的可能需要移动很多骨骼才能实现的一个姿势，只移动一两个控制器就可以实现。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_15-6-35.png?version=1&amp;modificationDate=1682406396269&amp;api=v2" alt=""  />
</p>
<ul>
<li>为骨骼蒙皮</li>
</ul>
<p>蒙皮就是把骨骼和模型结合起来的过程。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-50-57.png?version=1&amp;modificationDate=1682405457233&amp;api=v2" alt=""  />
</p>
<p>这样每一个骨头控制着附近区域的顶点。当骨头移动时，就会牵引着它控制的骨头一起移动。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-51-35.png?version=1&amp;modificationDate=1682405495621&amp;api=v2" alt=""  />
</p>
<ul>
<li>调整权重（让虚拟人在运动时肌肉的形变更加自然）</li>
</ul>
<p>刚才说过，一根骨头可以控制很多顶点。同时一个顶点也可以被多根骨头控制。这时就需要我们分配这些骨头对该顶点的控制权，这个控制权就是所谓的权重。同一个骨架和同一个模型，权重的配置不同，最终生成的动画效果也会有很大差异。</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-25_14-54-57.png?version=1&amp;modificationDate=1682405697863&amp;api=v2" alt=""  />
</p>
<h3 id="432-混合变形">4.3.2. 混合变形<a hidden class="anchor" aria-hidden="true" href="#432-混合变形">#</a></h3>
<p>    骨骼带来的顶点变换很粗矿，无法完成对嘴形状的定制，因为这样一个看似简单的外表，实际上在模型中会涉及数以万计的顶点进行不同规律的变换。于是，我们就为这一组顶点变换专门设置变形器，业界一般称为“Morph Target”或“Blend Shape”。这种变换的原理是给顶点准备一个基准位置，再提供一个”极端变化“后的最大位置，之后乘以一定的”权重比例“，就能让这个顶点在基准位置和极端位置中的任一位置。但因为脸部需要局部变形的地方太多，且一个变形可能涉及数以万计的顶点，所以对实时计算压力也是不小的。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-7_11-38-50.png?version=1&amp;modificationDate=1680838730710&amp;api=v2" alt=""  />
</p>
<p>       两种技术比较来说，骨骼绑定简单高效但不够灵活，而混合变形更加自由灵活但制作和计算成本都较高。所以在实际开发中，哪些用骨骼，哪些用混合变形，这是一个权衡“效果”与“效率”的工作，需要具体去看，反复拿捏。</p>
<h3 id="433-重定位">4.3.3. 重定位<a hidden class="anchor" aria-hidden="true" href="#433-重定位">#</a></h3>
<p>骨骼层次相同，但骨骼比例不同，如何重定位？</p>
<p><strong>动画重定位</strong> 是一种允许在共用相同<a href="https://docs.unrealengine.com/5.1/zh-CN/skeletons-in-unreal-engine">骨架</a>资源但比例差异很大的角色之间复用动画的功能。在我们接触业务时，我们知道不同的骨骼比例模型，动画和服装的绑定也不同，那么通过重定位，可以防止生成动画的骨架在使用来自不同外形的角色的动画时丢失比例或产生不必要的变形。 通过动画重定位，还可以在使用 <strong>不同骨架</strong> 资源的角色之间共享动画，前提是他们使用相似的骨骼层级，并使用名为 <strong>绑定（Rig）</strong> 的共享资源在骨架之间传递动画数据。</p>
<p><strong>1、假如有多个角色，你希望在基本角色和矮壮角色和高瘦角色之间共享动画。</strong></p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-10_17-25-11.png?version=1&amp;modificationDate=1681118711838&amp;api=v2" alt=""  />
</p>
<p><strong>2、重定位前的结果</strong></p>
<p>在应用重定位前，您就可以在任何共用相同骨架资源的骨骼网格之间使用动画。但是，如果角色身材比例如上图所示有差异，就会得到一些很难看的结果。请注意矮个角色是如何被不必要地拉长的，高个角色又是如何被压短的，这都是系统为了使其符合基本角色的骨骼比例而进行的更改。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-10_17-17-2.png?version=1&amp;modificationDate=1681118222161&amp;api=v2" alt=""  />
</p>
<p><strong>3、重定位后的结果</strong></p>
<p>对角色应用重定位以后，系统就不再考虑它们的比例差异，动画会在每个角色身上正常播放。图中可查看原骨架（以米黄色显示）和当前骨架（白色）的差异。在基本角色身上，米黄色的非重定位骨骼与骨架是完美重合的。</p>
<p><img loading="lazy" src="/download/attachments/97901833/image2023-4-10_17-29-54.png?version=1&amp;modificationDate=1681118994975&amp;api=v2" alt=""  />
</p>
<h2 id="44-驱动动态">4.4. 驱动（动态）<a hidden class="anchor" aria-hidden="true" href="#44-驱动动态">#</a></h2>
<h3 id="441-驱动类型">4.4.1. 驱动类型<a hidden class="anchor" aria-hidden="true" href="#441-驱动类型">#</a></h3>
<p>    虚拟人驱动可以分为<strong>交互型、非交互型</strong>两种，他们的制作成本和灵活度区别也很大。</p>
<p>驱动类型</p>
<p>驱动方式</p>
<p>主要特点</p>
<p>成本</p>
<p>技术流程</p>
<p>技术成熟度</p>
<p>适用场景</p>
<p><strong>非交互型</strong></p>
<p>预制动作</p>
<p>不能实现实时互动</p>
<p>成熟</p>
<p>动画片、宣传视频等</p>
<p><strong>交互型</strong></p>
<p>真人驱动方法</p>
<p>真人驱动，在动作灵活度、互动效果等方面有明显优势</p>
<p>真人+动捕设备</p>
<p>使用门槛较高</p>
<p>形象设计—建模绑定—动捕设备或摄像头将基于真人的动作/表情等驱动数字虚拟人—实时渲染—完成内容录制或现场互动</p>
<p>成熟</p>
<p>虚拟偶像、大型直播、游戏等</p>
<p>智能驱动方法</p>
<p>语言表达、面部表情、具体动作将主要通过深度学习的运算结果实时或离线驱动</p>
<p>AI技术，造价成本低</p>
<p>形象设计—建模绑定—训练各类驱动的深度模型，学习模特语言、唇形、表情参数间的潜在映射关系—内容制作，基于输入的语言（或TTS），预测唇动、表情等参数，推理图片与时间戳结合—渲染并生成内容</p>
<p>技术有限</p>
<p>虚拟客服、虚拟助手等</p>
<p><strong>非交互型</strong>主要通过设置预制动作来让人物动起来，类似于动画片的原理，不能实现实时互动。</p>
<p><strong>交互型</strong>虚拟人是我们的重点。交互型虚拟人需要靠驱动技术来驱动**动作、表情、嘴形，**这样，虚拟人才能做到根据外界刺激进行反馈的效果。交互型数字人的驱动可以分为真人驱动方法和智能驱动方法。</p>
<ul>
<li><strong>真人驱动方法</strong>：需要”真人+动捕设备”来进行驱动，通过捕捉技术采集真人演员的动作和面部表情数据之后将这些<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB&spm=1001.2101.3001.7020">数据迁移</a>合成到虚拟数字人身上，<strong>光学捕捉和惯性捕捉是常见的动捕方式</strong>，使用门槛较高。而近年来基于计算机视觉的捕捉技术发展迅猛，超写实虚拟数字人的面部表情捕捉还可以通过一个景深摄像头采集真人的面部3D点阵云图，然后实时地将面部动作和表情迁移到虚拟人的身上，相比需要穿戴动捕设备、租赁动捕棚的方式，驱动虚拟人只需要一台手机，使操作流程更加方便。</li>
<li><strong>智能驱动方法</strong>：智能驱动是指通过AI技术，例如CV、ASR、TTS等来对虚拟人进行驱动，该方式造价成本低，可以无限拓展，在未来有很大的想象空间。不过现阶段AI技术有限，一般需要结合合适的场景，通过较多垂直领域的训练才能达到商业可用的效果。</li>
</ul>
<h3 id="442-主要驱动技术概况及相关公司">4.4.2. 主要驱动技术概况及相关公司<a hidden class="anchor" aria-hidden="true" href="#442-主要驱动技术概况及相关公司">#</a></h3>
<ul>
<li>**智能合成：**2D、3D数字人均已实现嘴型动作的智能合成，其他面部/身体部位的动作智能合成未能完全实现。</li>
<li>**动捕：**通过将捕捉采集的动作迁移至数字虚拟人是目前动作生成主要方式，核心技术是动作捕捉。可分为光学式、惯性式及计算机视觉动捕等。现阶段光学式和惯性式动捕占据主导。计算机视觉动捕虽然相对开发难度大，目前精度较低，但就成本/对环境要求低，可移动范围大，使用场景想象力较大，目前已有消费级应用（部分VR设备采用），成为聚焦热点。</li>
</ul>
<p>项目</p>
<p>分类</p>
<p>技术</p>
<p>优缺点</p>
<p>国外相关技术和公司</p>
<p>国内相关技术和公司</p>
<p>智能合成</p>
<p>唇形动作</p>
<p>建立输入文本到输出语言与视觉信息的相关映射，主要是对于已采集到的文本到语言（TTS）和嘴型动画的数据模型训练，得到输入任意文本都可以驱动嘴型的模型，通过模型智能合成</p>
<p>已实现智能合成</p>
<p>Reallusion公司的Craytalk技术</p>
<p>搜狗、相芯科技等</p>
<p>唇形之外的动作</p>
<p>眨眼、点头、挑眉等动画目前都是通过采用一直随机策略或某个脚本策略将预制好的3D动作进行循环播放来实现，触发策略是通过人手动配置得到</p>
<p>尚未实现智能合成，未来希望通过智能分析文本，学习人类的表达，实现制动配置</p>
<p>Ziva Dynamics</p>
<p>影眸科技</p>
<p>动作/面部捕捉</p>
<p>光学动捕</p>
<p>通过对目标上特定光点的监视和跟踪来完成运动捕捉的任务，即在真人身上粘贴能够反射红外光的马克点，通过摄像头对反光马克点的追踪，从而对真人动作进行捕捉</p>
<p>造价昂贵，捕捉精度高</p>
<p>英国的Vicon，美国的OptiTrack(NP)和魔神</p>
<p>NoKov、uSens、青瞳视觉等</p>
<p>惯性动捕</p>
<p>基于惯性测量单元IMU来完成对人体动作的捕捉，即把集成了加速度计、陀螺仪、磁力计的IMU绑定在人体特定骨骼节点上，通过算法对测量数据进行计算，从而完成动作捕捉。</p>
<p>价格相对低廉，精度较低，会睡着连续使用时间的增加产生累积误差，发生位置漂移，扛遮挡力强</p>
<p>荷兰的Xsens</p>
<p>诺亦腾（Noitom)、幻境、国承万通等。</p>
<p>计算机视觉动捕</p>
<p>由多个高速相机从不同角度对目标进行监视和跟踪</p>
<p>简单、易用、硬件成本较低</p>
<p>Leap Motion、微软Kinect</p>
<p>针对光学捕捉、惯性捕捉、视觉捕捉做更进一步的综合对比：</p>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-10_16-24-27.png?version=1&amp;modificationDate=1681115067214&amp;api=v2" alt=""  />
</p>
<h2 id="45-渲染">4.5. 渲染<a hidden class="anchor" aria-hidden="true" href="#45-渲染">#</a></h2>
<p>定义：渲染是对3D数字人或虚拟人加入几何、视点、纹理、照明和阴影等信息从而达成从模型到图像的转变，渲染决定了最终数字人的质量，而渲染引擎和GPU算力的发展推动了数字人渲染更加信息和实时化，<strong>渲染分为实时渲染和离线渲染。</strong></p>
<ul>
<li>目前离线渲染比较成熟，应用较为广泛。而实时渲染尚有提升空间，伴随GPU算力的不断提升和渲染引擎的优化，将推动实时渲染的速度和真实度，未来实时渲染技术的发展也为数字人的实时交互提供了极大助力。</li>
<li>主流的3D渲染引擎UnrealEngine和Unity3D版本不断迭代，推动数字人皮肤纹理、3D效果、质感和细节等方面渲染效果更佳，同时生产效率更高。</li>
</ul>
<p>渲染方式</p>
<p>离线渲染</p>
<p>实时渲染</p>
<p>定义</p>
<p>在计算出画面时并不显示画面，计算机根据预先定义好的光线、轨迹渲染图片，渲染完成后再将图片连续播放，实现动画效果。</p>
<p>图形数据实时计算与输出，每一帧都是针对当前实际的环境光源、相机位置和材质参数计算出的图像。</p>
<p>渲染时间</p>
<p>长、花费几十分钟甚至更长时间渲染一帧画面</p>
<p>短、每秒至少渲染30帧</p>
<p>计算资源</p>
<p>多，受时效限制有限，可临时调配更多计算资源</p>
<p>少，受限于时效要求，计算资源一般不能及时调整</p>
<p>渲染质量</p>
<p>高</p>
<p>中</p>
<p>数字人交互</p>
<p>无</p>
<p>有交互/无交互</p>
<p>优缺点</p>
<p>强调美学和视觉效果，主要优点是渲染时可以不考虑时间对渲染效果的影响；缺点是渲染画面播放时用户不能实时控制物体和场景。</p>
<p>强调“交互性和实时性”，优点是就可以实时操控；缺点是要受系统的负荷能力的限制，必要时要牺牲画面效果（模型的精细、光影的应用贴图的精细程度）来满足实时系统的要求。</p>
<p>应用领域</p>
<p>广告营销/影视等预先设计好的模式的演示</p>
<p>直播/行业数字客服/游戏等无预定脚本场景</p>
<p><img loading="lazy" src="https://pic.rmb.bdstatic.com/bjh/down/ef8bfcc9a4be1335fc1569d9b176e56f.gif" alt=""  />
</p>
<h2 id="46-换装">4.6. 换装<a hidden class="anchor" aria-hidden="true" href="#46-换装">#</a></h2>
<p>    现实生活中，衣服穿在身上它就和皮肤紧贴着或者有一定空隙，这种想法放到虚拟世界中其实非常难实现。因为皮肤和衣服其实就是Mesh（网格），当衣服穿在身上时实际上是两组网格“碰撞”在一起，于是会引出如下两个问题：</p>
<ul>
<li>    如何在身体做动作时，衣服也跟着身体“做动作”？身体是拥有骨骼的，骨骼外包裹着一层“皮肤”，用同样思路，衣服实际上也是包裹在同样骨骼上的一层“皮肤”。将身体和衣服采用同一套骨骼模板，并在渲染时实现了两份骨骼数据的实时“同步”。</li>
<li>    如何解决身体的网格穿透到衣服外？用相同骨骼的方案解决衣服“穿”在身上的效果非常巧妙，但也容易出现问题，比如某件衣服局部非常内凹，那么就很容易出现身体的皮肤突出在衣服之外的问题，俗称“穿模”。因为仔细调整道具成本实在太高了，所以我们也做了个取巧的方案：通过把人体进行“切割”，并对每件衣服遮挡的人体部位进行标记，当渲染某件衣服时，直接隐藏被遮挡部位皮肤Mesh即可。</li>
</ul>
<p><img loading="lazy" src="/download/thumbnails/97901833/image2023-4-10_16-11-15.png?version=1&amp;modificationDate=1681114275826&amp;api=v2" alt=""  />
</p>
<p>有了这两项技术后，我们就可以批量的生产衣服了，而换装仅仅只是加载不同的模型而已，无须准个特殊处理也能达到目的。</p>
<h1 id="5-思考与总结">5. 思考与总结<a hidden class="anchor" aria-hidden="true" href="#5-思考与总结">#</a></h1>
<p>1、通过了解第三方公司提供能力以及万得厨的硬件能力，大概清晰了小万的PTA形象资源应该采用的是手工建模中的次世代建模方式，让低模以更低的面数展现更多的细节，面数低的模型能够降低虚拟人对资源的占用，保证终端设备能够流畅运行，之后虚拟人形象资源的选款也应该尽量选择细节简单，褶皱少的款式，尽量将模型的面数控制在一个较低的范围内。 </p>
<p>2、目前小万使用的应该是手工建模，还无法做到我们所期望的千人千面。自动化建模技术还不算特别成熟，建模结果到直接商用还有一段距离，不过，该类技术会大大降低建模的人力成本和时间成本。目前已经出现了一些支持虚拟人创建的工具化平台，如国内唯物（杭州）科技有限公司的NextHuman、国外虚幻引擎的 MetaHuman Creator 等。尤其是2022年6月虚幻引擎发布的MetaHuman Creator ，其效果令人惊艳。这些平台的建模精度虽不足以建立超高质量的模型，但能够大幅降低虚拟人建模的成本，让普通人也能快速拥有属于自己的虚拟形象。随着技术的发展，自动化建模的效果还会变得越来越好。在未来，这也是小万PTA形象发展的方向，和元宇宙入口、虚拟分身、千人千面等概念联系起来。</p>
<p>3、目前的技术从成熟趋向于简单，建模技术由手工建模逐渐演变成通过AI技术能直接生成3D模型，面部捕捉的技术也趋向更简单的硬件、更细腻的表情，影眸科技的“一句话人脸资产生成”，使用一段描述生成/编辑3D数字形象模型与高精度PBR材质，使用单张照片生成/编辑3D数字形象模型与高精度PBR材质就极大程度的减少了用户使用门槛。</p>
<p><a href="/">Filter table data</a><a href="/">Create a pivot table</a><a href="/">Create a chart from data series</a></p>
<p><a href="/users/tfac-settings.action">Configure buttons visibility</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E4%BA%94%E8%99%9A%E6%8B%9F%E4%BA%BA%E8%B5%84%E6%BA%90/">（五）虚拟人资源</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/wiki/%E4%BA%94%E8%99%9A%E6%8B%9F%E4%BA%BA%E8%B5%84%E6%BA%90/">
    <span class="title">« 上一页</span>
    <br>
    <span>（五）虚拟人资源</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/wiki/%E5%BD%92%E6%A1%A3/">
    <span class="title">下一页 »</span>
    <br>
    <span>归档</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on x"
            href="https://x.com/intent/tweet/?text=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&amp;hashtags=%ef%bc%88%e4%ba%94%ef%bc%89%e8%99%9a%e6%8b%9f%e4%ba%ba%e8%b5%84%e6%ba%90">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&amp;title=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;summary=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f&title=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on whatsapp"
            href="https://api.whatsapp.com/send?text=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on telegram"
            href="https://telegram.me/share/url?text=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=3%e3%80%81%e8%99%9a%e6%8b%9f%e4%ba%ba%e5%bd%a2%e8%b1%a1%e4%b8%8e%e5%8a%a8%e7%94%bb%e5%88%b6%e4%bd%9c%e3%80%81%e9%aa%a8%e9%aa%bc%e7%bb%91%e5%ae%9a%e3%80%81%e5%94%87%e5%bd%a2%e8%a1%a8%e6%83%85%e5%8a%a8%e4%bd%9c%e9%a9%b1%e5%8a%a8%e6%8a%80%e6%9c%af%e8%b0%83%e7%a0%94&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fwiki%2f3%25E8%2599%259A%25E6%258B%259F%25E4%25BA%25BA%25E5%25BD%25A2%25E8%25B1%25A1%25E4%25B8%258E%25E5%258A%25A8%25E7%2594%25BB%25E5%2588%25B6%25E4%25BD%259C%25E9%25AA%25A8%25E9%25AA%25BC%25E7%25BB%2591%25E5%25AE%259A%25E5%2594%2587%25E5%25BD%25A2%25E8%25A1%25A8%25E6%2583%2585%25E5%258A%25A8%25E4%25BD%259C%25E9%25A9%25B1%25E5%258A%25A8%25E6%258A%2580%25E6%259C%25AF%25E8%25B0%2583%25E7%25A0%2594%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://beian.miit.gov.cn/">粤ICP备2023039897号-1</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
