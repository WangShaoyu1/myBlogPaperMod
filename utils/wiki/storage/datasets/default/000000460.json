{
	"title": "OpenAPI接入调查",
	"author": "王宇",
	"publishTime": "三月02,2023",
	"readTime": "12s",
	"tags": "[\"chatGPT与OpenAI相关学习资料\"]",
	"description": "chatGPT与OpenAI相关学习资料",
	"article": "模型\n\n描述\n\n模型\n\n描述\n\nGPT-3\n\n一组能够理解和生成自然语言的模型\n\nCodex\n\n一组可以理解和生成代码的模型，包括将自然语言转换为代码\n\n虽然**GPT-3**模型没有ChatGPT背后的**GPT-3.5**强大，但是用API有如下好处：\n\n**优点**\n\n*   无需注册、无需Science上网\n*   有参数可以控制输出\n*   比ChatGPT稳定\n*   速度比ChatGPT快一点\n*   可以整合到其他系统中\n\n**缺点**\n\n*   生成质量不如ChatGPT\n*   优先的上下文支持\n*   会产生费用\n\n  \n\ndemo用法\n------\n\n[?](#)\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n`// $ npm install openai`\n\n`const { Configuration, OpenAIApi } = require(``\"openai\"``);`\n\n`const configuration =` `new` `Configuration({`\n\n  `apiKey:` `'YOUR_OPENAI_API_KEY'``,`\n\n`});`\n\n`const openai =` `new` `OpenAIApi(configuration);`\n\n`const response = await openai.createCompletion({`\n\n  `model:` `\"text-davinci-003\"``,`\n\n  `prompt:` `\"Hello\"``,`\n\n  `temperature: 0,`\n\n  `max_tokens: 16,`\n\n`});`\n\n  \n\nAPI参数说明\n-------\n\n**GPT-3**和**Codex**模型支持的参数：\n\n参数名\n\n类型\n\n默认值\n\n说明\n\n参数名\n\n类型\n\n默认值\n\n说明\n\n`model`\n\nstring\n\n  \n\n模型名称（详见模型综述）\n\n`prompt`\n\nstring\n\n  \n\n输入的提示\n\n`suffix`\n\nstring\n\n`null`\n\n文本生成后在文末插入的后缀\n\n`max_tokens`\n\nint\n\n`16`\n\n文本生成时要生成的最大token数。  \n提示的token数加上`max_tokens`不能超过模型的上下文长度。  \n大多数模型的上下文长度为2048个token（最新模型支持4096 tokens）\n\n`temperature`\n\nfloat\n\n`1`\n\n采样温度。值越高意味着模型承担的风险越大。  \n对于需要创意的场景，可以尝试0.9，  \n对于答案明确的场景，建议用0（argmax采样）  \n建议不要与`top_p`同时改变。\n\n`top_p`\n\nfloat\n\n`1`\n\n核采样（温度采样的另一种方式），其中模型考虑具有`top_p`概率质量的token的结果。因此，0.1意味着只考虑包含最高10%概率质量的token  \n建议不要与`temperature`同时改变。\n\n`n`\n\nint\n\n`1`\n\n每个提示要生成多少个答案\n\n`stream`\n\nboolean\n\n`false`\n\n是否返回流传输进度。如果设置，token将在可用时以纯数据服务器端推送事件发送，流以`data:[DONE]`消息终止。\n\n`logprobs`\n\nint\n\n`nul`\n\n如果传值（最大值5）则表示包括`logprobs`个最可能的token以及所选令牌的对数概率。例如，如果`logprobs`为5，则API将返回包含5个最可能Token的列表。\n\n`echo`\n\nboolean\n\n`false`\n\n是否回传提示\n\n`stop`\n\nstring\n\n`null`\n\n最多4个序列，遇到`stop`API将停止生成。  \n返回的文本不包含停止序列。\n\n`presence_penalty`\n\nfloat\n\n`0`\n\n数值介于-2.0和2.0之间。正值将根据到目前为止新token是否出现在文本中来惩罚新token，从而增加模型谈论新主题的可能性。\n\n`frequency_penalty`\n\nfloat\n\n`0`\n\n数值介于-2.0和2.0之间。正值根据文本中新token已经出现的频率惩罚新token，从而降低模型逐字重复同一行的可能性。\n\n`best_of`\n\nint\n\n`1`\n\n在服务端生成`best_of`个完成，并返回“最佳”（每个token的log概率最高的一条）。结果无法流式传输。  \n与`n`一起使用时，`best_of`控制候选回应的数量，`n`指定要返回的数量–`best_of`必须大于等于`n`。  \n⚠注意：由于此参数生成许多回应，因此会快速消耗token配额。小心使用并确保对`max_tokens`和`stop`进行了合理的设置。\n\n`logit_bias`\n\nmap\n\n`null`\n\n修改回应种出现指定token的可能性。  \n接受一个json对象，该对象将token（由GPT tokenizer的token ID指定）映射到-100到100之间的相关偏差值。可以用 tokenizer tool 将文本转换成token ID。  \n在数学上，在采样之前，将偏差添加到模型生成的逻辑中。每个模型的确切效果会有所不同，但介于-1和1之间的值应该会降低或增加选择的可能性；像-100或100这样的值应该会导致相关token的禁用或必现。  \n例如，可以传递`｛\"50256\": -100｝`以防止生成\\`<\n\n`user`\n\nstring\n\n`null`\n\n代表终端用户的唯一标识符，OpenAI用来监控和检测滥用。\n\n[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)\n\n[Configure buttons visibility](/users/tfac-settings.action)"
}