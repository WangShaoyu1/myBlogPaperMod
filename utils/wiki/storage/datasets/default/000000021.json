{
	"title": "3.2、语音交互体验需求说明（针对科大讯飞与鱼亮科技提供的技术支持）",
	"author": "王宇",
	"publishTime": "三月10,2023",
	"readTime": "12s",
	"tags": "[\"3、语音交互体验需求\"]",
	"description": "3、语音交互体验需求",
	"article": "背景：\n===\n\n万得厨团队之前进行过语音相关的软硬件一体化性能测试：[9.1、万得厨语音部分的软硬一体化性能测试方案](/pages/viewpage.action?pageId=95556039)，主要是测试语音唤醒率、识别（响应）率、误唤率这几个方面，侧重于性能。\n\n而关于识别的准确率、语音交互的体验感等非性能方面，则另外进行了测试：[3.1、万得厨2.0虚拟人语音交互功能测评](/pages/viewpage.action?pageId=95559396)，根据测试的结果，整理出一份针对科大讯飞与鱼亮科技提供的技术支持的需求说明，希望通过技术上的改进来提高用户与虚拟人语音交互的体验感。\n\n  \n\n需求说明清单（0221细化版）：\n================\n\n![](/download/attachments/95560754/image2023-2-21_20-6-30.png?version=2&modificationDate=1678432623052&api=v2)\n\n2019年由电信终端产业协会发布的团体标准文件：\n\n[TTAF 041-2019 智能产品语音识别测评方法 第一部分车载语音交互系统.pdf](/download/attachments/95560754/TTAF%20041-2019%20%E6%99%BA%E8%83%BD%E4%BA%A7%E5%93%81%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%B5%8B%E8%AF%84%E6%96%B9%E6%B3%95%20%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E8%BD%A6%E8%BD%BD%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F.pdf?version=1&modificationDate=1677028835361&api=v2)\n\n[TTAF 043-2019 智能产品语音识别测评方法 第二部分：智能音箱.pdf](/download/attachments/95560754/TTAF%20043-2019%20%E6%99%BA%E8%83%BD%E4%BA%A7%E5%93%81%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%B5%8B%E8%AF%84%E6%96%B9%E6%B3%95%20%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E6%99%BA%E8%83%BD%E9%9F%B3%E7%AE%B1.pdf?version=1&modificationDate=1677028835541&api=v2)\n\n序号\n\n技术模块\n\n需求点\n\n需求点描述\n\n需求现状描述\n\n需求期望描述\n\n技术类指标\n\n优先级\n\n需求相关方\n\n备注\n\n**1**\n\n**唤醒**\n\n可以配置进入休眠的时间\n\n在虚拟人进入休眠前，用户只需唤醒一次就可以在一定的时间内持续对话（无需重复输入唤醒词）\n\n目前虚拟人被唤醒后，超过2s再输入下一指令，就已无法准确接收且做出反馈（也可能是受到干扰）；且每次输入新的指令都需要重复唤醒，影响用户体验，对虚拟人的运行来说也容易造成卡顿和延迟\n\n希望可以自行配置虚拟人进入休眠的时间\n\n（参考小鹏的智能语音助手小P：时间设定有4档：20s-30s-60s-120s）\n\n（各个方位的）唤醒率：≥95%\n\n（各个方位的）误唤醒率：≤5%\n\none-shot后续指令的识别率：≥95%\n\np0\n\n科大讯飞\n\n内部可进行设置不同休眠时间的用户体验测试（由鹏哥带领）\n\n**2**\n\n支持one-shot功能\n\n“唤醒词+意图识别”一体化，支持用户可以在说出唤醒词之后不作停顿\n\n现目前不支持one-shot交互，且简单指令唤醒率也不算高\n\n需要支持one-shot交互，比如：\n\n用户：“小万小万，帮我打开炉门”，\n\n小万：“好的，以为你打开炉门”\n\none-shot交互更便捷、自然，可以极大的提升了交互便利性\n\np1\n\n科大讯飞\n\n  \n\n**3**\n\n**ASR**\n\n识别速度/响应速度\n\n语音输入到响应在1s以内。\n\n识别结果响应时间低于200ms。\n\n反应很迟钝，多数情况下隔一秒钟就不能回答\n\n甚至语音唤醒识别都很迟钝，而且需要提高人声。\n\n排查具体问题，进行全链路优化，做到语音输入到响应在1s以内\n\n响应时间：≤1s\n\nWER（ASR字错率）：≤5%\n\nSER（ASR句错率）：≤5%\n\np0\n\n科大讯飞、\n\n影子虚拟人团队\n\n需要进行全链路耗时分析瓶颈进行具体优化：  \n1、ASR识别耗时\n\n2、接口请求耗时\n\n3、TTS响应耗时\n\n**4**\n\nASR识别自动校正\n\n基于相关专业领域的知识库，能够做到在用户输入的语音指令不准确时对识别内容自动进行校正\n\n当前我们采取的头部领域是菜名，通过对各种与菜名相关的问题进行测试，发现识别率不高，且兜底话术很少\n\n需要在正常环境使用过程中4米以内识别准确率需要做到>=95%，需要增加专业领域知识以及兜底话术，例如：\n\n用户：“小万小万，鱼香肉丝怎么做？”\n\n小万：“已为您找到鱼香肉丝的食谱，第一步，需要胡萝卜、木耳、肉丝少许.....”（而不会查到京酱肉丝的做法）\n\np0\n\n科大讯飞、\n\n杭州虚拟人公司、\n\n影子虚拟人团队\n\n需要补充相关专业领域知识；\n\n指令中包含实体的需要开放配置\n\n**5**\n\n全双工/连续对话\n\n在唤醒后的一段时间内语音助手都会处于收音状态，可以提供连续对话（边听边说）；\n\n当下一句指令与上一句之间完全没有联系的时候，不会对收听到的无关背景对话进行响应（即“后续query”的NLP不支持长尾词的理解）\n\n小万不支持连续对话\n\n需要做到完整的连续对话；且对于突然插入的指令，若不支持长尾词的理解，需要给出兜底话术，例如：\n\n用户：“小万小万，开始烹饪”\n\n小万：“好的主人，已为您烹饪”\n\n用户：“你会说方言吗”\n\n小万：“这个技能我不会”（此回答为兜底话术，也可用其他替代，比如：“你在说什么”）\n\np0\n\n杭州虚拟人公司\n\n需要能够联系上下文的含义\n\n**6**\n\n**NLU/NLP**\n\n语义打断\n\n在较长的播报中，可以随时发出新指令，且上下文之间语义是连贯的\n\n测试的较长播报是播报食谱，目前在播报食谱的过程中不能打断，即使已经获取到了新指令，也只有在播报结束后才会执行\n\n希望能够实现语义打断，在播报食谱的过程中可以随时打断，并且上下文连贯。比如：\n\n用户：“番茄炒蛋怎么做？”\n\n小万：“番茄炒蛋的做法是…………”\n\n用户：（打断）“那水煮肉片呢？”\n\n小万：“水煮肉片的做法是…………”（会进行水煮肉片的食谱播报，而不是调取水煮肉片的烹饪方案）\n\n语义理解准确率（查准率）： ≥ 90%\n\n召回率（查全率）：≥ 90%\n\n（能够正确识别用户话术到某个意图中，则为对应场景的召回率，召回率= 正确识别到该场景的话术/用户话术中存在该意图的话术。）\n\nF1值：≥ 90%\n\n（准确率和召回率的调和平均数；F1值越高越好）\n\n  \n\n（离线状态）语义理解准确率：≥90%\n\n（离线状态）召回率：≥90%\n\np0\n\n科大讯飞、\n\n杭州虚拟人公司\n\n需要能够联系上下文的含义\n\n与【连续对话】的区别：语义打断是用一条新的任务型指令来中断播报（通常与当前场景有联系）；而连续对话的打断是指与当前场景无上下文联系的问答\n\n**7**\n\n兜底回复\n\n在用户提问时如果没有满足条件的知识点，则进行兜底回复\n\n目前小万的兜底回复较少，且有时小万已经无法接收用户指令（间隔时间过长），但却没有响应的兜底回复，容易给用户带来困惑\n\n补充兜底回复，自行配置，例如，不能查询到相应食谱时：\n\n用户：“小万小万，麻婆豆腐怎么做？”\n\n小万：“对不起，主人，这道菜小万还没学会”\n\np0\n\n影子虚拟人内部\n\n按意图设置不同的兜底回复（比如，首先是识别到了做菜的意图，然后在未查询到用户输入的对应食谱后，给出兜底：“这道菜还没学会”）\n\n**8**\n\n离线意图识别\n\n离线识别是指在弱网或无网状态下保留基础指令\n\n目前小万不支持离线状态下的唤醒与意图识别\n\n离线识别需要做到常用简单的指令的识别，例如“开始烹饪”、“天气怎么样”等等，通过正则表达式获取关键词，匹配指令后执行。\n\np1\n\n科大讯飞、\n\n杭州虚拟人公司\n\n与烹饪相关的基础指令需要支持离线识别\n\n**9**\n\n敏感词过滤\n\n敏感词指涉黄、涉暴、涉政、涉恐或者儿童不宜的词汇。敏感过滤把敏感词屏蔽，不展示给用户。\n\n敏感词过滤默认关闭\n\n（目前后台暂未打开敏感词过滤，还未进行体验测试）\n\n能够自行设置需要屏蔽的敏感词\n\np2\n\n科大讯飞\n\n  \n\n**10**\n\n**TTS**\n\n自定义语音包（定制音色）\n\n类似百度地图可以录制专属语音包，用户录制几句标准语音即可以生成相应的导航语音包\n\n目前小万的音色不支持修改和定制\n\n在敬老爱幼等场景中，自定义语音包可以支持将用户家人的语音录入并匹配给虚拟人，从而给用户带来亲切感\n\n【采用5分制主观评分】\n\nTTS自然度：＞3分\n\n(指TTS在播报时的流畅程度，更加贴近于人在说话，而非机器音，是一个偏主观的指标。)\n\n  \n\n同理心：＞3分\n\n趣味性：＞3分\n\n人设一致性：＞4分\n\np2\n\n科大讯飞\n\n  \n\n**11**\n\nTTS个性化播报\n\n根据场景定制TTS文本\n\n当前小万不支持TTS个性化播报，无论是谁、无论何时何地、什么场景向小万发出指令或询问，得到的反馈回答都是一致的\n\n能够支持根据场景定制TTS文本，比如：\n\n用户询问“有什么美食推荐”的时候，可以根据当前的时间给出不一样的回答；\n\n通过身份识别技术识别到当前和虚拟人对话的家庭成员是谁，从而可以在TTS反馈文本中加入用户昵称等信息，体现亲切感\n\np2\n\n影子虚拟人团队、\n\n科大讯飞\n\n可能会涉及到其他有关时间空间信息、身份理解以及性别、年龄预测等技术\n\n  \n\n阿里云不支持自定义，但可以指定某个发音人\n\n[https://help.aliyun.com/document\\_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG](https://help.aliyun.com/document_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG)\n\n科大讯飞支持定制\n\n[https://www.xfyun.cn/solution/soundLibrary](https://www.xfyun.cn/solution/soundLibrary)，具体还要和科大讯飞商谈\n\nby renpeng\n\n参考链接：[https://blog.csdn.net/mingzheng114/article/details/120121572](https://blog.csdn.net/mingzheng114/article/details/120121572)\n\n[https://coffee.pmcaff.com/article/2999187665857664/](https://coffee.pmcaff.com/article/2999187665857664/)\n\n[语音量化分析指标.xmind](/download/attachments/95560754/%E8%AF%AD%E9%9F%B3%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E6%8C%87%E6%A0%87.xmind?version=1&modificationDate=1676985397974&api=v2)\n\n[万得厨”小飞小飞“语音【识别率】测试报告](/pages/viewpage.action?pageId=95561208)   by renpeng\n\n  \n\n  \n\n  \n\n  \n\n  \n\n  \n\n需求说明清单（初稿）：\n===========\n\n序号\n\n需求分类\n\n需求点\n\n需求描述\n\n需求现状描述\n\n需求期望描述\n\n优先级\n\n需求相关方\n\n备注\n\n1\n\n唤醒（听得见）\n\n可以配置进入休眠的时间\n\n可自行配置虚拟人进入休眠的时间，在进入休眠前，虚拟人只需唤醒一次就可以在一定的时间内持续对话（无需重复输入唤醒词）\n\n目前虚拟人被唤醒后，超过2s再输入下一指令，就已无法准确接收且做出反馈（也可能是受到干扰）；且每次输入新的指令都需要重复唤醒，在识别准确率与响应速度都不太高的情况下，频繁的说唤醒词不仅影响用户体验，对虚拟人的性能运行来说也容易造成卡顿和延迟\n\n希望虚拟人进入休眠的时间可以自行配置\n\n（参考小鹏的智能语音助手小P：时间设定有4档：20s-30s-60s-120s）\n\np0\n\n科大讯飞\n\n内部可进行设置不同休眠时间的用户体验测试（由鹏哥带领）\n\n2\n\n唤醒词+离线意图识别\n\n离线识别是指在弱网或无网状态下保留基础指令\n\n目前小万不支持离线状态下的唤醒与意图识别\n\n离线识别需要做到常用简单的指令的识别，例如“开始烹饪”、“天气怎么样”等等，通过正则表达式获取关键词，匹配指令后执行。\n\np1\n\n科大讯飞、\n\n杭州虚拟人公司\n\n与烹饪相关的基础指令需要支持离线识别\n\n3\n\n支持one-shot功能\n\n“唤醒词+意图识别”一体化，支持用户可以在说出唤醒词之后不作停顿，立刻说出后续需求。\n\n现目前不支持one-shot交互，且简单指令唤醒率也不算高\n\n需要支持one-shot交互，比如：\n\n用户：“小万小万，帮我打开炉门”，\n\n小万：“好的，以为你打开炉门”\n\none-shot交互更便捷、自然，可以极大的提升了交互便利性\n\np1\n\n科大讯飞\n\n  \n\n4\n\n聆听（听得清）\n\n  \n\n**识别速度/响应速度**\n\n语音输入到响应在1s以内\n\n反应很迟钝，多数情况下隔一秒钟就不能回答\n\n甚至语音唤醒识别都很迟钝，而且需要提高人声。\n\n排查具体问题，进行全链路优化，做到语音输入到响应在1s以内\n\np0\n\n科大讯飞、\n\n影子虚拟人团队\n\n需要进行全链路耗时分析瓶颈进行具体优化：  \n1、ASR识别耗时\n\n2、接口请求耗时\n\n3、TTS响应耗时\n\n5\n\n语义打断\n\n在较长的播报中，可以随时发出新指令，且上下文之间语义是连贯的\n\n测试的较长播报是播报食谱，目前在播报食谱的过程中不能打断，即使已经获取到了新指令，也只有在播报结束后才会执行\n\n希望能够实现语义打断，在播报食谱的过程中可以随时打断，并且上下文连贯。比如：\n\n用户：“番茄炒蛋怎么做？”\n\n小万：“番茄炒蛋的做法是…………”\n\n用户：（打断）“那水煮肉片呢？”\n\n小万：“水煮肉片的做法是…………”（会进行水煮肉片的食谱播报，而不是调取水煮肉片的烹饪方案）\n\np0\n\n科大讯飞、\n\n杭州虚拟人公司\n\n需要能够联系上下文的含义\n\n6\n\n全双工/连续对话\n\n在唤醒后的一段时间内语音助手都会处于收音状态，可以提供连续对话；\n\n且当下一句指令与上一句之间完全没有联系的时候，不会对收听到的无关背景对话进行响应（即“后续query”的NLP不支持长尾词的理解）\n\n小万不支持连续对话\n\n需要做到完整的连续对话；且对于突然插入的指令，若不支持长尾词的理解，需要给出兜底话术，例如：\n\n用户：“小万小万，开始烹饪”\n\n小万：“好的主人，已为您烹饪”\n\n用户：“你会说方言吗”\n\n小万：“这个技能我不会”（此回答为兜底话术，也可用其他替代，比如：“你在说什么”）\n\np0\n\n杭州虚拟人公司\n\n需要能够联系上下文的含义\n\n与【语义打断】的区别是：语义打断使用的是一条新的任务型指令（通常与当前场景有联系）；而连续对话中的打断，是指与当前场景无上下文联系的问答\n\n7\n\n理解（听得懂）\n\n**ASR识别自动校正**\n\n正常环境使用过程中4米以内ASR识别准确率需要做到>=95%\n\n当前我们采取的头部领域是菜名，通过对各种与菜名相关的问题进行测试，发现识别率不高，且兜底话术很少\n\n需要在正常环境使用过程中4米以内识别准确率需要做到>=95%，需要增加专业领域知识以及兜底话术，例如：\n\n用户：“小万小万，鱼香肉丝怎么做？”\n\n小万：“已为您找到鱼香肉丝的食谱，第一步，需要胡萝卜、木耳、肉丝少许.....”（而不会查到京酱肉丝的做法）\n\n  \n\n  \n\np0\n\n科大讯飞、\n\n杭州虚拟人公司、\n\n影子虚拟人团队\n\n需要补充相关专业领域知识；\n\n指令中包含实体的需要开放配置\n\n8\n\n播报（能反馈）\n\n兜底回复\n\n在用户提问时如果没有满足阈值的知识点，则进行兜底回复\n\n目前小万的兜底回复较少，且有时小万已经无法接收用户指令（间隔时间过长），但却没有响应的兜底回复，容易给用户带来困惑\n\n补充兜底回复，自行配置，例如，不能查询到相应食谱时：\n\n用户：“小万小万，麻婆豆腐怎么做？”\n\n小万：“对不起，主人，这道菜小万还没学会”\n\np0\n\n影子虚拟人内部\n\n是否可以按意图设置不同的兜底回复？（比如，首先是识别到了做菜的意图，然后在未查询到用户输入的对应食谱后，给出兜底：“这道菜还没学会”）\n\n9\n\n自定义语音包（定制音色）\n\n类似百度地图可以录制专属语音包，用户录制几句标准语音即可以生成相应的导航语音包\n\n目前小万的音色不支持修改和定制\n\n在敬老爱幼等场景中，自定义语音包可以支持将用户家人的语音录入并匹配给虚拟人，从而给用户带来亲切感\n\np2\n\n科大讯飞\n\n  \n\n10\n\nTTS个性化播报\n\n根据场景定制TTS文本\n\n当前小万不支持TTS个性化播报，无论是谁、无论何时何地、什么场景向小万发出指令或询问，得到的反馈回答都是一致的\n\n能够支持根据场景定制TTS文本，比如：\n\n用户询问“有什么美食推荐”的时候，可以根据当前的时间给出不一样的回答；\n\n通过身份识别技术识别到当前和虚拟人对话的家庭成员是谁，从而可以在TTS反馈文本中加入用户昵称等信息，体现亲切感\n\np2\n\n影子智能研究院、\n\n科大讯飞\n\n可能会涉及到其他有关时间空间信息、身份理解以及性别、年龄预测等技术\n\n  \n\n阿里云不支持自定义，但可以指定某个发音人\n\n[https://help.aliyun.com/document\\_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG](https://help.aliyun.com/document_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG)\n\n科大讯飞支持定制\n\n[https://www.xfyun.cn/solution/soundLibrary](https://www.xfyun.cn/solution/soundLibrary)，具体还要和科大讯飞商谈\n\nby renpeng\n\n1、细化内容（0221下午）\n\n2、补充需求相关方——识别意图前都属于科大讯飞\n\n3、识别速度与识别准确率的现状，提供数据上的体现（志川、鹏哥）\n\n[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)\n\n[Configure buttons visibility](/users/tfac-settings.action)"
}