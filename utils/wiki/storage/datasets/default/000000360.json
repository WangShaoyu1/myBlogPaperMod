{
	"title": "看了此文不要说你还不了解大模型",
	"author": "王宇",
	"publishTime": "八月21,2023",
	"readTime": "12s",
	"tags": "[\"大模型系列\"]",
	"description": "大模型系列",
	"article": "*   1 [一、名词解释](#id-看了此文不要说你还不了解大模型-一、名词解释)\n*   2[二、大模型的定义以及发展现状](#id-看了此文不要说你还不了解大模型-二、大模型的定义以及发展现状)\n    *   2.1[2.1 大模型的定义](#id-看了此文不要说你还不了解大模型-2.1大模型的定义)\n    *   2.2[2.2 大模型的发展现状](#id-看了此文不要说你还不了解大模型-2.2大模型的发展现状)\n*   3[三、大模型的能力分析](#id-看了此文不要说你还不了解大模型-三、大模型的能力分析)\n    *   3.1[3.1 大模型实现了什么技术突破](#id-看了此文不要说你还不了解大模型-3.1大模型实现了什么技术突破)\n    *   3.2[3.2 技术突破直接带来的能力是什么](#id-看了此文不要说你还不了解大模型-3.2技术突破直接带来的能力是什么)\n*   4[四、大模型的优势和不足](#id-看了此文不要说你还不了解大模型-四、大模型的优势和不足)\n*   5[五、大模型时代背后的趋势和应用思路](#id-看了此文不要说你还不了解大模型-五、大模型时代背后的趋势和应用思路)\n    *   5.1[5.1 大模型时代的AI十大趋势](#id-看了此文不要说你还不了解大模型-5.1大模型时代的AI十大趋势)\n        *   5.1.1[5.1.1 趋势一：涌现-- LLM推动人工智能快速进化到AGI阶段](#id-看了此文不要说你还不了解大模型-5.1.1趋势一：涌现--LLM推动人工智能快速进化到AGI阶段)\n        *   5.1.2[5.1.2 趋势二:  融合-- 多模态助力大模型解决复杂问题](#id-看了此文不要说你还不了解大模型-5.1.2趋势二:融合--多模态助力大模型解决复杂问题)\n        *   5.1.3[5.1.3 趋势三:  懂你-- 生成式AI带来更贴近人的交互方式](#id-看了此文不要说你还不了解大模型-5.1.3趋势三:懂你--生成式AI带来更贴近人的交互方式)\n        *   5.1.4[5.1.4 趋势四：生态-- 模型即服务(MaaS) 生态呼之欲出](#id-看了此文不要说你还不了解大模型-5.1.4趋势四：生态--模型即服务\\(MaaS\\)生态呼之欲出)\n        *   5.1.5[5.1.5 趋势五:  泛在-- 垂直领域应用是大模型的主战场](#id-看了此文不要说你还不了解大模型-5.1.5趋势五:泛在--垂直领域应用是大模型的主战场)\n        *   5.1.6[5.1.6 趋势六:  平台-- Plugin工具让大模型迎来App Store时刻](#id-看了此文不要说你还不了解大模型-5.1.6趋势六:平台--Plugin工具让大模型迎来AppStore时刻)\n        *   5.1.7[5.1.7 趋势七:  入口-- 大模型为数字人\"注入灵魂\"](#id-看了此文不要说你还不了解大模型-5.1.7趋势七:入口--大模型为数字人\"注入灵魂\")\n        *   5.1.8[5.1.8 趋势八:  赋能-- AI大模型将帮助个体成为超级生产者](#id-看了此文不要说你还不了解大模型-5.1.8趋势八:赋能--AI大模型将帮助个体成为超级生产者)\n        *   5.1.9[5.1.9 趋势九: 冲击-- 版权\"思想表达二分法\" 基石正在动摇](#id-看了此文不要说你还不了解大模型-5.1.9趋势九:冲击--版权\"思想表达二分法\"基石正在动摇)\n        *   5.1.10[5.1.10 趋势十: 向善-- 伦理和安全建设塑造负责任的AI生态](#id-看了此文不要说你还不了解大模型-5.1.10趋势十:向善--伦理和安全建设塑造负责任的AI生态)\n    *   5.2[5.2 大模型的应用思路](#id-看了此文不要说你还不了解大模型-5.2大模型的应用思路)\n*   6[六、我们应该如何更好与大模型共生](#id-看了此文不要说你还不了解大模型-六、我们应该如何更好与大模型共生)\n*   7[七、相关材料](#id-看了此文不要说你还不了解大模型-七、相关材料)\n\n一、名词解释\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n名词\n\n解释\n\n  \n\n  \n\n  \n\n名词\n\n解释\n\n  \n\n  \n\n  \n\n**大模型**\n\n一般指1亿以上参数的模型，但是这个标准一直在升级，目前万亿参数以上的模型也有了。[大语言模型](https://www.zhihu.com/search?q=%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)（Large Language Model，LLM）是针对语言的大模型。\n\n  \n\n  \n\n  \n\n**175B、60B、540B等**\n\n这些一般指参数的个数，B是Billion/十亿的意思，175B是1750亿参数，这是ChatGPT大约的参数规模。\n\n  \n\n  \n\n  \n\n**强化学习**\n\n（Reinforcement Learning）一种机器学习的方法，通过从外部获得激励来校正学习方向从而获得一种自适应的学习能力。\n\n  \n\n  \n\n  \n\n**基于****[人工反馈](https://www.zhihu.com/search?q=%E4%BA%BA%E5%B7%A5%E5%8F%8D%E9%A6%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)****的强化学习（RLHF）**\n\nReinforcement Learning from Human Feedback）构建人类反馈数据集，训练一个激励模型，模仿人类偏好对结果打分，这是GPT-3后时代大语言模型越来越像人类对话核心技术。\n\n  \n\n  \n\n  \n\n**涌现**\n\n（Emergence）或称创发、突现、呈展、演生，是一种现象。许多小实体相互作用后产生了大实体，而这个大实体展现了组成它的小实体所不具有的特性。研究发现，模型规模达到一定阈值以上后，会在多步算术、大学考试、单词释义等场景的准确性显著提升，称为涌现。\n\n  \n\n  \n\n  \n\n**泛化**\n\nGeneralization）[模型泛化](https://www.zhihu.com/search?q=%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)是指一些模型可以应用（泛化）到其他场景，通常为采用迁移学习、微调等手段实现泛化。\n\n  \n\n  \n\n  \n\n**微**\n\n（FineTuning）针对大量数据训练出来的预训练模型，后期采用业务相关数据进一步训练原先模型的相关部分，得到准确度更高的模型，或者更好的泛化。\n\n  \n\n  \n\n  \n\n **[指令微调](https://www.zhihu.com/search?q=%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)**\n\n（Instruction FineTuning），针对已经存在的预训练模型，给出额外的指令或者标注数据集来提升模型的性能。\n\n  \n\n  \n\n  \n\n **[思维链](https://www.zhihu.com/search?q=%E6%80%9D%E7%BB%B4%E9%93%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)**\n\nChain-of-Thought，CoT）。通过让大语言模型（LLM）将一个问题拆解为多个步骤，一步一步分析，逐步得出正确答案。需指出，针对复杂问题，LLM直接给出[错误答案](https://www.zhihu.com/search?q=%E9%94%99%E8%AF%AF%E7%AD%94%E6%A1%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22615074572%22%7D)的概率比较高。思维链可以看成是一种指令微调。\n\n  \n\n  \n\n  \n\n监督学习(机器学习领域)\n\n需要大量带有标签的数据来进行训练，在训练过程中，模型通过将输入与标签进行比较来学习特征表示和分类器。\n\n  \n\n  \n\n  \n\n无监督学习\n\n不需要人工标注数据的机器学习方法，它通过对未标注数据的自动学习，学习到数据的潜在结构和特征，从而完成对数据的聚类、降维、特征提取等任务。\n\n  \n\n  \n\n  \n\n自监督学习\n\n无须人工标注数据的机器学习方法，能通过从大规模无标注数据中自动学习特征表示，从而提高模型的泛化和学习效果。\n\n  \n\n  \n\n  \n\n提示词学习\n\n提示词是指一种文本片段或文本模版，用于引导和规范机器生成的文本，以便控制机器生成的文本方向，确保生成的文本满足特定的条件和要求。\n\n  \n\n  \n\n  \n\n二、大模型的定义以及发展现状\n--------------\n\n### 2.1 大模型的定义\n\n比较通用的官方定义:  **大模型是指具有庞大参数量和复杂结构的机器学习模型。这些模型通常需要大量的计算资源和数据来训练，并且能够处理更复杂、更大规模的任务。大模型通常具有更高的准确性和表现能力，但也需要更多的训练时间和计算资源来进行训练和推理。常见的大模型包括深度神经网络、BERT、GPT等。**\n\n**个人理解: 把大模型拆开来理解，那就是\"大\"和\"模型\"**\n\n**“大”: 对应的参数量到了一定量级，并朝着越来越大的方向发展。**\n\n**“模型”: 再回想模型的定义——对于某个实际问题或客观事物、规律进行抽象后的一种形式化表达方式。**\n\n### 2.2 大模型的发展现状\n\n生成式预训练大模型（Generative Pre-trained Models）目前在自然语言处理领域非常流行。这类模型使用了海量的文本数据进行预训练，然后通过微调来完成特定的任务。其中最著名的生成式预训练模型是OpenAI的GPT（Generative Pre-trained Transformer）系列，包括GPT、GPT-2和GPT-3。这些模型使用了大量的互联网文本数据进行预训练，学习了丰富的语言知识和规律。在预训练阶段，模型通过自监督学习的方式预测下一个词或掩盖的词，从而学习到了句子的上下文和语义信息。\n\n预训练后，这些模型可以根据特定任务的需求进行微调，如文本生成、文本分类、问答等。通过微调，模型可以根据任务的输入和输出数据进行优化，以提供更准确的预测和生成结果。\n\n生成式预训练大模型的优势在于其能够从大量的数据中学习到丰富的语言知识，并且能够灵活地适应不同的任务和领域。然而，这些模型通常需要庞大的计算资源和大规模的数据集进行训练，因此对于一般用户和研究者来说，使用已经预训练好的模型进行微调是更常见和可行的做法。\n\n**生成式**: 生成模型是一种可以给定的训练数据中学习数据的分布，然后生成与训练数据类似的新数据的模型，换句话说生成模型可以模拟数据的生成过程，并从中生成新的数据，与生成模型相对的是判别模型，不关心数据的分布，而是直接对数据进行分类、回归等预测。\n\n**预训练模型:** 是一种在大规模语料上进行自我训练的模型，它能够学习自然语言处理任务中的一般特征和模式。预训练模型通常使用无监督学习的方式，在没有人工标注的数据上进行训练，以获得更广泛和通用的语言知识。\n\n*   模型架构\n*   训练数据的数量\n*   训练数据的质量\n*   学习算法\n*   计算硬件算力\n\n大模型包括但不限于以下几类：\n\n*   大语言模型\n*   视觉大模型\n*   多模态大模型\n*   决策大模型\n*   机器人大模型\n\n目前大多数的大语言模型都是建立在一个叫做 **Transformer** 的基础模型之上的（下一篇会介绍）。而根据使用的 Transformer 的方式不同，LLM的构建就被分为三条研究路径：编码器-解码器结构、只使用解码器、只使用编码器。绿色标注的是开源可用的模型，部分模型后有相应的参数量（以B–十亿为单位）。\n\n大语言模型\n\n![](https://pic3.zhimg.com/v2-6510d73a14d8df6fb282ffc56929b3b2_r.jpg)  \n  \n\n多模态大模型  \n![](https://pic3.zhimg.com/v2-d4223b0aba574dae703a0f0e35b06e3a_r.jpg)\n\n三、大模型的能力分析\n----------\n\n**驱动技术发展的动力是什么？ 是创新？ 是需求？ 是利益？**\n\n### 3.1 大模型实现了什么技术突破\n\n大模型指的是能够处理大规模数据和任务的机器学习模型。这些模型通常由数十亿到数百亿个参数组成，需要运行在强大的计算资源上，如超级计算机或分布式计算系统。大模型的出现实现了以下几个技术突破：\n\n**更高的准确性**：大模型具有更多的参数和更强大的计算能力，可以学习更复杂、更精细的模式和规律，从而提高预测和分类的准确性。  \n**更好的泛化能力**：大模型可以更好地泛化到新的、未见过的数据上。通过对大量数据的学习，大模型能够提取更丰富的特征和知识，从而对新数据做出更准确的预测。  \n**更广泛的应用领域**：大模型的出现使得机器学习和人工智能可以应用于更广泛的领域和任务。例如，自然语言处理、计算机视觉、语音识别等领域都可以通过大模型来实现更高的性能和更复杂的任务。  \n**更高的效率和速度**：大模型的训练和推理需要大量的计算资源，但随着计算技术的发展和分布式计算的应用，大模型的训练和推理速度也得到了显著提升，从而可以更快地完成任务。\n\n### 3.2 技术突破直接带来的能力是什么\n\nLLM表现出了惊人的语义理解和生成能力，这种能力如何赋能企业核心业务？\n\n四、大模型的优势和不足\n-----------\n\n**优势**:\n\n 1、提高模型的泛化能力。\n\n 2、降低训练成本\n\n 3、提高模型的性能。\n\n 4、避免过拟合。\n\n 5、提高下游任务的完成效率。\n\n**不足:**\n\n大模型就像一个在读大量的书籍和文章，然后尝试回答你的问题的[超级书虫](https://www.zhihu.com/search?q=%E8%B6%85%E7%BA%A7%E4%B9%A6%E8%99%AB&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22648709272%22%7D)。但是它也有一些限制和挑战，就像下面这些：\n\n1.  **只知道训练时的知识**： 大模型就像一个只读过到训练结束那天为止的所有书籍的人。也就是说，如果在它训练结束后发生了新的事件，或者出现了新的知识，它是不会知道的。就像ChatGPT，只知道到2021年9月为止的所有信息，之后的信息是不知道的。\n2.  **无法理解复杂的情感和意图**： 尽管大模型可以生成看起来非常自然的语言，但是它并不能真正理解人的情感或意图。也就是说，如果你告诉它你很伤心，它可以给你回应，但它并不会真正理解你的感受。\n3.  **可能产生错误或者有偏见的信息**： 大模型是根据它阅读的文本生成答案的。如果这些文本中包含了错误的信息，或者有偏见的观点，那么模型也可能会生成错误或者有偏见的答案。\n4.  **需要大量的资源**： 大模型需要读大量的书籍和文章，这需要非常大的计算资源。而且，每次生成答案，也需要进行大量的计算。这就导致大模型需要非常强大的计算能力，这对于很多人来说可能难以负担。\n\n要评估一个LLM模型的水平，可以从以下几个维度进行评估：\n\n*   **理解能力**：提出一些需要深入理解长文本的问题，看模型是否能准确回答。\n*   **语言生成能力**：让模型生成一段有关特定主题的文章或故事，评估其生成的文本在结构、逻辑和语法等方面的质量。\n*   **知识面广度**：请模型回答关于不同主题的问题，以测试其对不同领域的知识掌握程度。这可以是关于科学、历史、文学、体育或其他领域的问题。一个优秀的大语言模型应该可以回答各种领域的问题，并且准确性和深度都很高。\n*   **适应性**：让模型处理各种不同类型的任务，例如：写作、翻译、编程等，看它是否能灵活应对。\n*   **多样性**：提出一个问题，让模型给出多个不同的答案或解决方案，测试模型的创造力和多样性。\n*   **情感分析**：提供一段对话或文本，让模型分析其中的情感和态度。\n*   **情感表达**：请模型生成带有情感色彩的文本，如描述某个场景或事件的情感、描述一个人物的情感状态等。一个优秀的大语言模型应该能够准确地捕捉情感，将其表达出来。\n*   **逻辑推理能力**：请模型回答需要进行推理或逻辑分析的问题，如概率或逻辑推理等。这可以帮助判断模型对推理和逻辑思考的能力，以及其在处理逻辑问题方面的准确性。\n*   **问题解决能力**：提出实际问题，例如：数学题、编程问题等，看模型是否能给出正确的解答。\n*   **道德和伦理**：测试模型在处理有关道德和伦理问题时的表现，例如：“在什么情况下撒谎是可以接受的？”\n*   **对话和聊天**：请模型进行对话，以测试其对自然语言处理的掌握程度和能力。一个优秀的大语言模型应该能够准确地回答问题，并且能够理解人类的语言表达方式。\n\n五、大模型时代背后的趋势和应用思路\n-----------------\n\n### 5.1 大模型时代的AI十大趋势\n\n#### 5.1.1 趋势一：涌现-- LLM推动人工智能快速进化到AGI阶段\n\n自2010年代初深度学习问世以来，人工智能进入到第三次高潮。而2017年Transformer算法将深度学习推向了大模型时代。OpenAl基于Transformer的Decoder部分建立起来了GPT家族。ChatGPT一经面世便风靡全球，人们惊讶于其能够进行连贯、有深度对话的同时，也惊异地发现了它涌现了推理、思维链等体现智能的能力。GPT4的能力更是进化神速，在多种能力测试中达到人类顶级水平,让人类看到了AGI的曙光。\n\n#### 5.1.2 趋势二:  融合-- 多模态助力大模型解决复杂问题\n\n多模态AI是指能够处理和理解多种类型信息的人工智能，如文本、图像、音频、视频等。这种AI不仅能够处理单一数据类型的任务，而且可以在不同数据类型间建立联系和融合，从而实现一个综合、全面的理解多模态。AI能够对各种不同类型的数据进行关联分析,为解决复杂问题提供支持。未来在诸多创新领域，多模态技术的发展将带来创新应用的蓝海。\n\n#### 5.1.3 趋势三:  懂你-- 生成式AI带来更贴近人的交互方式\n\n从使用键盘-鼠标等方式跟电脑交互，到使用手指滑动屏幕跟手机交互，再到人们用唤醒词跟智能音箱等交互，人机交互从识别机器指令，到识别人的动作，语音，不断朝着更贴近人的习惯的交互方式演进。生成式AI的发展，让人类有史以来第一次有机会用自然语言的方式，来跟机器对话，而机器也借由大模型拥有了极强的理解人类语言的能力，有望带来一场全新的交互变革。 正如历次交互变革带来从终端、到连接，到各类应用的颠覆式变革，生成式Al也必将带来产业链、价值链和生态的重塑。\n\n#### 5.1.4 趋势四：生态-- 模型即服务(MaaS) 生态呼之欲出\n\n#### 5.1.5 趋势五:  泛在-- 垂直领域应用是大模型的主战场\n\n#### 5.1.6 趋势六:  平台-- Plugin工具让大模型迎来App Store时刻\n\n#### 5.1.7 趋势七:  入口-- 大模型为数字人\"注入灵魂\"\n\n#### 5.1.8 趋势八:  赋能-- AI大模型将帮助个体成为超级生产者\n\n#### 5.1.9 趋势九: 冲击-- 版权\"思想表达二分法\" 基石正在动摇\n\n#### 5.1.10 趋势十: 向善-- 伦理和安全建设塑造负责任的AI生态\n\n### 5.2 大模型的应用思路\n\n**大模型，是AI时代一项基础设施，基础设施的价值，最终还是要靠各式各样的应用来发挥。**\n\n就像如果没有电灯、电车等这些具体的应用，电力这种强大的基础设施的价值也没办法发挥一样。\n\n如果没有能够实实在在为用户解决问题、创造价值的应用的支撑，大模型的真正价值也很难发挥出来。\n\n既然大模型最终是一项基础设施，那对于大部分的人来说，思考如何在基础设施上做创新、做应用，就比思考基础设施本身更有价值。\n\n正如互联网时代的互联网+Everything，现在正是到了大模型+Everything的时代。\n\n结合上面介绍的大模型+应用的生命周期，在大模型时代，我们要用全新的范式，来进行应用的创新了。\n\n在全新的范式下，我们思考应用创新的逻辑，大致应该是这样的：\n\n（1）首先，要明确，AI不是可以拿着锤子到处找钉子的锤子，它是一项更先进的生产力，它是为解决问题、提升生产效率而生，不是为了AI而AI；\n\n（2）其次，对于用户来说，最重要的永远是解决方案，你提供的产品/服务，最终为用户解决什么样的问题，如何解决的，这是最为关键的；\n\n（3）在合适的场景中，AI如果能够帮用户更加优雅、更加高效地解决问题，那AI就是好的选择，否则，就不是；\n\n（4）确定好场景和需求，然后再定义具体的技术需求，明确了技术需求，再去定义合适的技术方案；\n\n（5）对于大模型的使用，我们的第一选择，仍然是Prompt Engineering, 它能够帮我们快速验证我们的想法到底可行不可行；\n\n（6）想法验证可行，有了一定的数据积累之后，基于对用户体验、数据安全等的考虑，我们可以开始考虑“训练”自己的大模型；\n\n（7）训练自己的大模型，第一选择是挑选一个优质的[开源基座](https://www.zhihu.com/search?q=%E5%BC%80%E6%BA%90%E5%9F%BA%E5%BA%A7&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22647417636%22%7D)模型(Foundation Model)，然后用Prompt Tuning/Fine Tuning这些成本较为低廉的技术方案，来进行调优、适配；\n\n（8）不到万不得已，千万不要从头训练自己的基座模型，因为这一步的投入产出比，随着开源大源模型生态越来越好，会越来越低；\n\n（9）做完上述调优，再考虑是否进一步需要做跟人类反馈的对齐，即RLHF，这步是可选的，并不是所有的大模型都需要这一步；\n\n（10）用微调好的大模型，做压缩、优化，布署在自己的环境中，就可以很好地为自己的应用提供AI能力；\n\n（11）千万要记住，AI不是目的，它只是手段、工具。把AI当作做产品、做服务的工具箱中一种可选用的工具，在合适的地方使用，千万不要为了蹭热度、追风口，把AI当做目的，这样就有点本末倒置了。\n\n六、我们应该如何更好与大模型共生\n----------------\n\n**我们有一个无法摆脱的命运，那就是和技术共生。**田园牧歌的生活只会逐渐成为奢望，我们在不断被迫接受着过量的信息和超出认知的技术革新，否则就会处于被革新的尴尬境地。那么大模型也一样，我们要寻求共生之道。我在这个回答中给出了一个理想主义的答案，即在大模型等技术的支撑下，我们能够从劳作中解脱出来，可以有更多的精力探寻人存在的价值。\n\n但这终究是个过分理想的叙事角度，不难想象，在不久的将来，我们会在[高频迭代](https://www.zhihu.com/search?q=%E9%AB%98%E9%A2%91%E8%BF%AD%E4%BB%A3&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22630472611%22%7D)的AI技术的逼促下，更辛苦地劳作以赶上时代的发展，或者彻底屈服于其下做一个便宜稳定的数据源。所以看起来，路子只有一条，了解它并学会使用它，就像学会使用电脑为我们创造价值一样。\n\n现在还不晚，还来得及拥抱未来。\n\n七、相关材料\n------\n\n[十大趋势-大模型](https://mp.weixin.qq.com/s?__biz=MzI5MjkxNjEyNg==&mid=2247490784&idx=2&sn=bd3955b22fb4fb6fbd565bc3155133c7&chksm=ec7b4211db0ccb0796478438e164e94c2fc159ba8c9dd700b208f4fe5aa5d8ad684bdd8426be#rd)\n\n  \n\n  \n\n  \n\n  \n\n  \n\n[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)\n\n[Configure buttons visibility](/users/tfac-settings.action)"
}