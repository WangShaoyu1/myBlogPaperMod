{
	"title": "1、多模交互知识",
	"author": "王宇",
	"publishTime": "五月06,2023",
	"readTime": "12s",
	"tags": "[\"多模交互设计知识学习\"]",
	"description": "多模交互设计知识学习",
	"article": "《前瞻交互 从语音、手势设计到多模融合》Chap1-2\n\n*   1[第一章 多模交互的演进](#id-1、多模交互知识-第一章多模交互的演进)\n    *   1.1[（1）初步融合阶段](#id-1、多模交互知识-（1）初步融合阶段)\n    *   1.2[（2）语义理解阶段](#id-1、多模交互知识-（2）语义理解阶段)\n    *   1.3[（3）任务自适应阶段](#id-1、多模交互知识-（3）任务自适应阶段)\n*   2[第二章 多模交互包含什么](#id-1、多模交互知识-第二章多模交互包含什么)\n    *   2.1[2.1 视觉系统的特点](#id-1、多模交互知识-2.1视觉系统的特点)\n    *   2.2[2.2 听觉](#id-1、多模交互知识-2.2听觉)\n    *   2.3[2.3 触觉](#id-1、多模交互知识-2.3触觉)\n    *   2.4[2.4 嗅觉、味觉和五感理论](#id-1、多模交互知识-2.4嗅觉、味觉和五感理论)\n    *   2.5[2.5 主动交互](#id-1、多模交互知识-2.5主动交互)\n    *   2.6[2.6 无须主动交互的生物识别技术](#id-1、多模交互知识-2.6无须主动交互的生物识别技术)\n        *   2.6.1[指纹识别](#id-1、多模交互知识-指纹识别)\n        *   2.6.2[人脸识别](#id-1、多模交互知识-人脸识别)\n        *   2.6.3[虹膜识别](#id-1、多模交互知识-虹膜识别)\n        *   2.6.4[声纹识别](#id-1、多模交互知识-声纹识别)\n        *   2.6.5[心率检测](#id-1、多模交互知识-心率检测)\n        *   2.6.6[肌肉电信号](#id-1、多模交互知识-肌肉电信号)\n        *   2.6.7[脑电波](#id-1、多模交互知识-脑电波)\n\n· 随着技术的进步，信息逐渐从单个屏幕延展到多个屏幕，深智延展到用户所处的空间，基于鼠标、键盘和触摸屏的近场交互方式已经不能较好地满足人们在空间中快捷获取信息的需求，以语音、肢体交互为代表的远场交互方式成为人工智能和人机交互的研究重点，将多种交互方式自然地融合在一起更为关键。\n\n第一章 多模交互的演进\n===========\n\n· 在过去的70年里，计算机经历了大型机计算、桌面计算、移动计算三个发展阶段。人机交互方式从穿孔卡片（Punched Card）发展到命令行界面（Command-Line Interface，CLI），再到图形用户界面（Graphical User Interface，GUI），现在逐渐往以自然交互为主的自然用户界面（Natural User Interface，NUI）发展，每一种界面都比前一种界面更为自然和直观。\n\n· 卡内基梅隆大学的路易斯-菲利普·莫伦西（Louis-Phillippe Morency）教授在2017年对模态是这样论述的：每一种信息的来源或形式，都可以成为一种模态。\n\n· 人工智能非常依赖深度学习（Deep Learning）算法，例如卷积神经网络（Convolutional Neural Network，CNN）、循环神经网络（Recurrent Neural Network，RNN）、生成式对抗网络（Generative Adversarial Network，GAN）等。……各个互联网公司先后开源了自家的深度学习框架，例如谷歌的TensorFlow、Facebook的PyTorch和百度的PaddlePaddle，\n\n· 实现完整的多模融合需要三个阶段，分别为初步融合阶段、语义理解阶段和任务自适应阶段。\n\n（1）初步融合阶段\n---------\n\n· 从多个并行的交互模态中获取用户要传达的信息，关键在于每个模态的信息整合问题。模态之间的关系可以分为：唯一性、等效性、互斥性和互补性。\n\n*   唯一性：某个模态是完成某个特定交互任务的唯一途径。唯一性还有一种解释，就是当前存在多种交互方式可供选择，但用户或系统总是倾向或优先使用一种交互方式（比如：唱歌只能使用语音；切换焦点有使用键盘和使用鼠标两种方式，但用户总是优先使用鼠标）。\n*   等效型：在完成交互任务时至少有两种以上模态具有相同的表达。这样的好处是两个输入模态之间的信息可以相互转化，比如微信聊天时的语音消息转文字。如果是激活一个智能机器人，除了语音交互唤醒和触碰机器人之外，用户还可以向它挥手或盯着它一段时间，也能够理解为是用户在尝试激活它（地铁站里的yoyo智能机器人，当用户靠近屏幕的时候就会被自动唤醒）。\n*   互斥性：单个模态具有互斥性，比如人不能同时聚精会神地听两件事情。这里的互斥性更多的是指每个模态产生的信息会在流程上相互抵消，比如智能音箱如果将语音交互激活/取消设置为开关按键，当用户用语音唤醒了智能音箱，这时按语音按键就会退出语音交互，这两个行为是互斥的。——**相同的表达有可能同时带来等效性和互斥性**。\n*   互补性：当单个模态不能提供充分的任务信息时，可以由其他模态进行补充。多模交互的互补性充分发挥了各个模态的优点，同时也弥补了各个模态之间的缺点。\n\n· 从时间维度上来说，可以将多个模态的整合分为并行和串行两种状态。输入模态的五种感官是并行的关系；输入模态和任意一种输出模态融合也是并行的关系（比如用户可以边看屏幕边用鼠标和键盘）。多个输出模态的融合更多术语串行，因为用户很少能同时一心二用。\n\n· 多模融合的前提是选中对象，多个模态围绕该对象进行交互，因此多模态交互初步融合的关键是对齐焦点，也就是寻找相同的参照物，用同一个参照物实现多个模态之间的信息互通。\n\n· 语音交互里的交互对象如何与图形界面中的焦点融合将是一个好问题。由于误差的原因，语音在一定程度上不能被系统很好地识别焦点被定位到哪个对象上。当代词的理解发生错误时，交互任务的焦点自然会发生错误，焦点的错误切换会直接导致后续的交互流程全部错误。所以，在多模融合里，焦点的正确管理是保证用户体验的基础、关键点和难点。\n\n· 多模交互框架（Multimodal Interaction Framework），核心思想是将整个多模交互系统分解为一个交互管理器（Interaction Manager）和多个模态组件（Modality Components），为单个设备或跨设备交互提供更丰富的交互方式。设计原则如下：\n\n*   多模交互框架不应对组件的内部结构做出任何假设。\n*   多模交互框架应允许组件分散或耦合。\n*   各种交互方式应该彼此独立，尤其是添加新的交互方式不会对现有的交互方式产生影响。\n*   多模交互框架不应假设如何及何时将这些模态进行组合。\n*   总体控制流程和模式协调应与用户交互分开。\n\n· 上述5条设计原则里，前4条设计原则强调了责任分离，比如，语音交互组件不应该尝试控制GUI组件中发生的事情，而GUI组件应该负责视觉的输入和图形界面的输出，不用担心语音模态中发生的事情，这样的解耦有助于后续模态的增加和删除，这也意味着交互任务的完成与交互模态无关。最后一条设计原则可以看出交互模态不会和业务逻辑耦合，多模交互框架会有一个单独的控制模块来负责不同模态之间的协调——交互管理器，它不会直接与用户交互，主要负责控制流程，跟踪应用程序的总体进度，收集信息并决定下一步要做什么，然后协调各种模态组件，由模态组件负责与用户进行交互（比如，用户需要通过语音交互来填写图形界面上的信息，则交互管理器将负责启动语音模态组件，然后从该组件中获取语音结果并传递给图形界面）。\n\n· 如果某些模态组件需要彼此紧密耦合，而它们之间的协调性较差，则多模交互框架中允许多个模态组件加上一个交互管理器形成一个更大的模态组件，再由另外一个交互管理器进行管理。\n\n· （作者认为）在初步融合阶段更多的是基于触控、语音和隔空手势的GUI和VUI融合，后续当表情识别、视线追踪和指向定位等交互技术成熟后可作为补充融入进来。\n\n（2）语义理解阶段\n---------\n\n· 图形界面的语义由容器和响应事件两部分组成，比如，开关和滑块属于不同的容器，轻按和滚动属于不同的响应事件，两者结合起来可以产生简单的操作指令“打开/关闭开关”和“将XX设置为YY”。\n\n· 语音交互的语义可以理解为一句指令包含的意思，但不同语气也有不同的语义。\n\n· 肢体语言的语义复杂很多，因为涉及人因学、社会学等多个学科（比如，OK这个手势在美国、中国、英国的文化中代表事情妥妥的，而在土耳其、希腊、巴西等国的部分地区，OK是一种极具侮辱的冒犯性手势），加上表情会更复杂（表情识别技术）。\n\n· 场景也包含语义信息，家里、商场有着截然不同的信息，但目前移动设备只能通过GPS、WiFi等方式获取定位信息，摄像头和麦克风因为隐私和成本问题不能覆盖所有区域，因此场景内的语义信息基本上是缺失的。\n\n· 语音交互设计师、手势交互设计师做的工作都是在定义每一句话和每一个手势背后的语义是什么，这是产品经理和设计师在人机交互领域能尝试的新机会。\n\n（3）任务自适应阶段\n----------\n\n· 任务自适应阶段指计算机可以根据多个因素动态调整任务内容及各个输出模态的分配状态，这些因素包括：用户的认知负荷、隐私性、安全性、模态容量、信息重要程度、交互历史和交互距离等。\n\n*   认知负荷：人的注意力资源有限，过度的多任务交互会引发人们的紧张情绪，甚至降低人们的任务绩效。目前我们还不能很好地量化人类的认知资源和认知负荷，导致多模交互融合过程中，信息无法被合理运用到各个模态中，只能人为设计一个认知资源和认知负荷的阈值。\n    *   在工程心理学中，对多模交互研究最有帮助的理论模型目前有两个（作者推荐）：工作记忆模型（Work Memory Theory）和多重资源理论（Multiple Resource Theory）。多重资源理论是由美国著名工程心理学教授Christopher D.Wickens提出来的注意力模型，强调注意力不是局限在单一的认知资源中，而是存在多个认知资源的概念，不同感官与不同信息处理阶段都可能有各自独立的认知资源。但无论学者怎样努力完善这个模型，还是有很多问题无法解答，也导致现在的心理学家从研究人类行为逐渐转向研究人类大脑的活动情况（《快思考，慢思考》）。\n\n![](/download/attachments/101812282/image2023-4-20_17-33-58.png?version=1&modificationDate=1681983238665&api=v2)\n\n*   隐私性：当有多个用户在同一空间内时，每个用户的隐私都应该受到保护，如何通过各种交互模态有效保护用户的隐私是空间交互中的重中之重。比如，在驾驶过程中，当驾驶员接到一个紧急且比较隐私的电话时，如果车内只有驾驶员一人，则会使用车内扬声器接听电话；如果车内有其他乘客，则要么使用蓝牙耳机要么直接挂断电话。再比如，观看文字、图片可以通过角度、距离及遮挡避免其他用户看到。\n*   安全性：用户和信息的交互在特定场景下需要考虑安全性。比如，在驾驶过程中，车内的信息不能全部集中到中控屏幕上，可以通过HUD展示给驾驶员，同时对视觉信息做减法，多采用通过听觉通道接收信息的语音交互方式。\n*   模态容量：指一定时间内（如1秒），可以通过一种或多种方式传输的信息量。计算机主要通过文字、图片、音频、视频4中载体和用户交互，不同载体能承载的容量是不一样的，什么时候用什么载体和用户交互是一个值得深思的问题。作者认为，这个容量和大脑的工作机制有一定的关系，因此在确定模态容量前，只能提前人为定义好那些时候要用哪些模态或容器进行人机交互。\n    *   有研究表明，指尖震动的理论容量为10²bit/s，听觉的理论容量为10^4bit/s，而视觉的理论容量为10^6bit/s。\n*   信息重要程度和交互历史：信息应该判断自身的重要程度及当前交互任务的重要程度，并且根据用户的认知负荷进行自适应调整。比如，在驾驶过程中，驾驶员好友发来一条文字信息，但系统检测到汽车正在高速路上行驶，则会自动屏蔽该信息。除重要度外，还要考虑信息的输入和输出尽量和交互历史保持一致，优先选用用户交互频次高的模态作为输入和输出模态，更符合用户的心理预期。\n*   交互距离：人和机器之间的距离会成为是否开启人机对话的重要线索。美国人类学家Edward·Hall在经典著作《近体行为的符号体系》中将人类的空间区域距离分为亲密距离、个人距离、社交距离和公共距离。当前被用于检测用户和设备之间的距离，除了利用计算机视觉技术外，UWB（Ultra-Wide Band，超宽带）、TOF（Time of Flight，飞行时间）等传感技术也能够准确地检测用户的移动和未知状态。由于这类传感器无法获取到外界的真实画面，因此能够大大地减少广大用户对隐私泄露的担忧。\n    *   亲密距离（0~46㎝）：其语义为“热烈、亲密”，0~15㎝为近位亲密距离，16~46㎝为远位亲密距离；\n    *   个人距离（46~120㎝）：其语义为“亲切、友好”，46~75㎝为近位个人距离，76~120㎝为远位个人距离；\n    *   社交距离（1.2~3.6m）：其语义为“严肃、庄重”；\n    *   公共距离（＞3.6m）：其语义为“自由、开放”。\n\n· 目前已商用的交互技术：基于外设或触控的图形用户界面、语音交互、眼动追踪、单手手势识别、指纹识别、人脸识别和心率检测，目前表情识别技术、基于计算机视觉的手势识别技术等还处在发展阶段，准确率不高。\n\n  \n\n第二章 多模交互包含什么\n============\n\n· 在人机交互过程中，机器主要通过摄像头、麦克风以及各种传感器获取外界信息，通过文字、图片、音频和视频4种载体输出信息。相应的，人类通过视觉、听觉、触觉、嗅觉和味觉5个输入模态获取外界信息，然后通过语言、肢体动作、面部表情向机器表达自己的意图。除此之外，人类的生理信号也是输出模态中的一种。\n\n2.1 视觉系统的特点\n-----------\n\n· 人类通过视觉接收的信息占所有感官通道接收的信息的83%。\n\n· 图中展示了人的双眼视觉区域在垂直方向和水平方向的范围，有助于我们更合理地将信息和内容放在合适的区域，同时更好地理解为什么有些技术瓶颈会给用户体验带来影响：\n\n*   MR设备HoloLens 2的水平视野只有43°，远低于用户的可视范围190°，导致用户的沉浸式体验和接收信息的效率严重下降；\n*   在100%沉浸式的VR中需要考虑信息的展示范围，将文本面积控制在用户的字母识别区域内有助于用户更高效地阅读；\n*   由于汽车仪表盘和HUD在司机坐着时的自然视线范围内，所以将重要信息放在仪表盘和HUD能够让司机在驾驶过程中更安全地获取关键信息。\n\n![](/download/attachments/101812282/image2023-4-20_17-38-15.png?version=1&modificationDate=1681983495401&api=v2)\n\n· 在视神经盘颞（niè）侧约3.5毫米处，是大部分视锥细胞（能感知颜色，光敏感性差，但视敏度高）的聚集地，叫做中央凹，是视网膜中视觉（辨色力、分辨力）最敏锐的区域。离中央凹越远，图像越不清晰，对颜色的分辨也越来越模糊。那我们是如何看清楚周围的信息的？这依赖于我们的眼球运动来聚焦事物——眼球每秒会以3~4次的频率从一个地方跳到另一个地方，有助于我们专注于多个区域并看清一切。每次眼跳之间都有短暂的停顿让我们注视信息，在这个过程中信息会进入中央凹视野，然后被大脑提取。这是一个重要的信息过滤机制，如果大量信息瞬间集中在一起被大脑提取，我们的大脑就会因为信息超载而超出负荷。鉴于此，设计师在设计信息架构和排版时，应该将相关信息靠的近一点，同时排版的顺序要迎合眼睛习惯的运动方向，这样才能提升用户接收信息的效率。\n\n· 饶培伦教授在《人因工程：基础与实践》一书中提到人类的双眼运动有以下规律：\n\n1.  眼睛的运动方向通常是从左到右、从上到下的，较习惯于顺时针方向运动；\n2.  眼睛的水平运动快于垂直运动；人类对水平方向尺寸的估计要优于对垂直方向尺寸的估计；\n3.  当眼睛不在中心点时，在象限内观察到的顺序依次为左上象限↖、右上象限↗、左下象限↙、右下象限↘；\n4.  视觉更容易接受直线轮廓，而不是曲线轮廓；\n5.  当运动目标的角速度大于1-2rad/s时，个体无法识别其具体的运动状态；\n6.  注视时间在0.07-0.3s时，人们才能看清楚目标；当光线昏暗时，注视时间需要相对延长。\n\n· 人有两种视觉：（1）通过中央凹和周围的副中央凹获取信息的视觉区域，叫做中央视觉，用于直视事物和观察细节；（2）其余区域叫做周边视觉或边缘视觉，用于展示视野中的其他区域。人对场景的认知都来自周边视觉而非中央视觉，因为周边视觉能让人察觉到周围的动静，并作出反应。\n\n  \n\n· 视觉适应：环境中的光线亮度发生变化，视觉的感受能力也跟着发生变化，这种现象叫做视觉适应。从黑暗环境突然进入明亮环境，需要大约几十秒的适应期后才能看清，叫做明适应（Light Adaptation）；从明亮环境进入黑暗环境，需要30~40分钟的适应期才能看清，叫做暗适应（Dark Adaptation）。频繁的视觉适应会导致视觉迅速疲劳。\n\n· 深色模式可以减轻弱光环境下眼睛的疲劳，但是在强光环境下会使眼睛更疲劳。深色底浅色字能有效减少弱光环境下明适应带来的影响，降低眼睛的疲劳度，同时深色模式可以节省带有OLED或AMOLED显示器的设备的能耗。但深色模式对散光患者来说，增大了阅读的困难——因为白色反映了可见光谱的所有波长，眼睛上的虹膜不需要扩大即可吸收更多的光，此时的瞳孔仍然很窄，所以无须花太多精力就可以聚焦当前对比鲜明的黑色字体；而面对黑底白字时，瞳孔需要变大才能吸收更多的光线，且白色字体会渗入黑色背景，这种现象叫做“光晕”效果，对散光患者来说更甚。建议人们在光线较暗的环境中使用深色模式。\n\n· 从信息获取的角度来说，视觉范围决定了人只能从前方获取信息，明适应和暗适应在一定程度上会直接影响人接收信息的效率。如果信息分布在人的周围，而且需要人及时接收，那么这时人通过视觉通道来接收信息不是最好的方式，可以优先考虑听觉、嗅觉甚至触觉通道上的设计。\n\n2.2 听觉\n------\n\n· 听觉传递的信息具有以下几个特点：\n\n*   迫听性：环境中的声音信号总会传入人的耳中，引起人的注意和快速的朝向反射或惊跳反射；相比起视觉信号，听觉信号不容易被遗漏，适用于传递紧急的告警信息。\n*   全方位性：相比视觉的190°视场，听觉可以接收360°空间中的声音。结合迫听性的特点，可以在相关方向播放声音吸引用户的注意。\n*   变化敏感性：由于声音信号是一种时间序列信号，因此人耳对声音信号随时间的变化特别敏感，时间解析度（Temporal Resolution）较高，声音的变化敏感性适用于传递紧急的告警信息。\n*   绕射性与穿透性：声音传播的绕射性（衍射性）、折射性及反射性等特性时的声音信号可以不受空间阻隔的限制进行远距离传送，且声音不受照明条件的限制，具有穿透烟雾等障碍的特点。\n\n· 通过听觉通道传递信息有以下不足之处：\n\n*   声音信号难以避免对无关人群形成侵扰；\n*   听觉信道容量低于视觉信道容量；\n*   听觉对复杂信息模式的工作记忆保持时间较短；\n*   声音信号的瞬态特性决定了信号不可持久存在。\n\n· 听觉 VS 视觉：\n\n*   时间与空间的结合决定了信息的大小，其中时间对接收信息的多少起着决定性的作用，它是单向的、线性的以及不能停止的；而空间中的信息主要由两个因素决定：动态或静态、三维空间或二维空间，在没有其他参照物对比的情况下，事物的静止不动可以模拟时间上的静止，但静止的事物也可以传递信息。\n*   视觉与听觉最本质的区别是传递信息的维度：视觉信息由时间、空间方位和空间位置决定，听觉信息则只能由时间维度决定（虽然耳朵能觉察声音的方向和频率，但不是决定性因素——当听到一个声音后，让你说出刚才这个声音来自哪个方向，是无法准确判断的）。\n*   耳朵在很短的时间内接收的信息是非常有限的，在静止的时间内声音是无法传播的，即不存在信息；但眼睛可以在很短的时间内从空间中获取大量的信息。\n*   视觉接收的信息量远高于听觉，由于无法准确知道大脑每秒通过眼睛和耳朵接收的信息上限是多少（大脑是如何工作的对于研究人员来说仍然是未解之谜），但可以用以下方法简单判断——在不考虑超出理解范围的情况下，人阅读文字的速度可以达到每分钟500~1000字，说话时语速可以达到每分钟200~300字。\n\n2.3 触觉\n------\n\n· 触觉感知是我们在真实环境下感受实体的唯一途径。\n\n· 在物理按键上，如果增加纹理或刻度， 通过触觉通道就能感知和操作对象，久而久之形成肌肉记忆，无须视觉通道的介入就可以以更快的速度摸到按键并完成相关操作。随着电阻屏、电容屏、多点触控屏的引入，物理按键逐渐被触控屏上的虚拟按键和手势取代，手指上的感受器无法在触控屏上获得更多特征，导致人们在操控触控屏时必须看屏幕。\n\n· 人类对物体形状的触觉感知取决于手指受到的作用力，因此，为了让触摸屏拥有更多的触觉反馈信号，出现了以下集中触觉反馈技术：\n\n（1）振动式触觉反馈技术：苹果公司的触觉反馈技术一直处于全球领先地位：Apple Watch的Force Touch、iPhone 6s的3D Touch、iPhone 11的Haptic Touch（Force Touch的核心原理与3D Touch差不多，关于3D Touch与Haptic Touch的对比：[https://baijiahao.baidu.com/s?id=1732071528213998327&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1732071528213998327&wfr=spider&for=pc)），这些触觉体验都与线性振动马达有关。还有一个案例就是2018年出现的一款名叫Gloveone的VR手套，内置了多了小型线性振动马达和多个传感器，通过不同频率和强度的振动模拟出真实的触摸体验。  \n（2）静电式触觉反馈技术：当用干燥的手指在覆有绝缘物质的通电金属表面滑动时，手指会有“胶质”触感，这是由于手指触到屏幕时产生的静电可以调节手指与屏幕之间的摩擦。2006年Disney Research研究了一款名为TeslaTouch的技术，能让用户产生一种仿真纹理的感受，主要通过编程的方式改变手指与触摸面板之间的摩擦力，模拟不同的纹理和材质的感觉。Senseg薄膜也利用了类似的原理，把它贴在电容、光学和电阻式触摸屏上就能使用，消除电子和物理世界的鸿沟同时产生极其微小的噪音。  \n（3）超声波触觉反馈技术：由日本的富士通公司在2014年发布的超声波振动技术（Ultrasonic Vibrations），利用超声波来振动触摸板，可以让用户在原本冷硬的触摸板上感受到平滑与粗糙的触感。英国Ultrahaptics公司研究出另一种超声波触觉反馈技术（如左图），采用一组超声波扬声器阵列，当不同频率的超声波脉冲振动皮肤时，会让用户产生不同的感觉，目前多用于汽车上。\n\n![](/download/attachments/101812282/image2023-5-6_9-22-31.png?version=1&modificationDate=1683336152171&api=v2)![](/download/thumbnails/101812282/image2023-5-6_9-22-46.png?version=1&modificationDate=1683336166405&api=v2)![](/download/attachments/101812282/image2023-5-4_9-14-45.png?version=1&modificationDate=1683162886147&api=v2)\n\n· 将触觉的感知和操作融合在一起称为TUI（Tangible User Interface，可触式用户界面），融合了物理表示（空间内可操作的物理对象）和信息展示（图形和音频），典型的TUI案例是盲文笔记本电脑DotBook（[https://baijiahao.baidu.com/s?id=1645377749612427156&wfr=spider&for=pc](https://baijiahao.baidu.com/s?id=1645377749612427156&wfr=spider&for=pc)）。迪士尼和卡内基梅隆大学共同研究的一款名叫Wall++的大型内容感知传感系统，给墙面刷上Wall++特制的导电涂层，白墙就会变成一块大型的TUI（[https://baike.baidu.com/tashuo/browse/content?id=dfe1324092405f68bfb97f1c](https://baike.baidu.com/tashuo/browse/content?id=dfe1324092405f68bfb97f1c)）。\n\n![](/download/attachments/101812282/image2023-5-4_9-15-8.png?version=1&modificationDate=1683162909099&api=v2)\n\n2.4 嗅觉、味觉和五感理论\n--------------\n\n· 嗅觉不止是简单地检测周围环境中的化学刺激，还是人类复杂的情绪系统的一部分。嗅觉是最直接并能唤起人类本能行为和情绪记忆的感官。\n\n· 味觉和嗅觉结合在一起可以形成一个预警系统，对于哪些有可能对我们造成伤害的事件或情况进行警告。\n\n· 嗅觉和味觉的记忆比视觉记忆更长久。\n\n· 一些案例：英国希斯洛机场与香水制造商Design in Scent共同打造了一个“香氛地球仪”；杭州“气味王国”公司推出的仅有火柴盒大小的气味模块TY2.0和气味编辑器Scent Magic。\n\n· 气味检索：气味扫描仪又被成为电子鼻，用特定的传感器和模式识别系统快速提供被测样品的整体信息。英特尔神经拟态芯片Loihi可以识别10种有害气体；IBM研究院的科学家研发的“电子舌头”能在1分钟内识别多种液体，用于食品安全检测等。\n\n2.5 主动交互\n--------\n\n· 人与人在交流时，一半依赖语音，另一半依赖肢体语言，即肢体动作、手势、面部表情和眼神。\n\n· 智能情绪识别：ASR仅是把语音音频转换成文本，但无法真正确认人的意图。根据人们说话的方式和音域的变化，分析出愤怒、焦虑、幸福或满足的情绪。\n\n· 肢体动作和手势通常是借助计算机视觉系统来实现的。为了避免全部手势识别都依赖计算机视觉技术，谷歌还设计了一款名叫Soli的微型雷达芯片，首次应用于Pixel 4和Pixel 4L上，用户可以通过在Pixel 4上方做手势，从而控制音量、导航菜单等，无须触摸显示屏。\n\n![](/download/attachments/101812282/image2023-5-6_9-23-8.png?version=1&modificationDate=1683336188206&api=v2)\n\n· 面部表情在人机交互上，除了可以理解为用户的情感反馈之外，还可以作为对话中发言的轮换管理，比如机器看到用户表情瞬间变为愤怒，需要考虑流程是否还要继续下去。\n\n2.6 无须主动交互的生物识别技术\n-----------------\n\n· 生物识别技术（Biometric Identification Technology）：利用人的生物特征进行身份认证的一种技术。目前主流的生物识别技术有：指纹识别、人脸识别、虹膜识别、声纹识别。除了身份认证，还能用于健康和运动领域的监测。\n\n#### 指纹识别\n\n· 门禁系统是最早应用指纹识别技术的商业产品，而真正让人们享受指纹识别技术带来的便利性的产品是苹果公司在2013年退出的iPhone 5S，用户通过Home键进行指纹识别，完成手机解锁、账号登录和购物支付。2018年vivo退出全球第一台屏下指纹解锁手机。\n\n#### 人脸识别\n\n· 人脸识别有两种方式：\n\n（1）通过人脸关键点的检测算法来实现，比如眉毛、眼睛、鼻子、嘴巴和脸部轮廓等，人脸识别算法会计算人脸关键点之间的关系来判定两张人脸图像是否一样。一般而言关键点数量越多，识别准确率越高，目前用的最多的关键点数量是72点或106点；  \n（2）通过结构光技术来实现，即向人脸投射光，再读取人脸表面的光信息来确定人脸形状。\n\n· 结构光技术最先应用于苹果公司的iPhone X的Face ID，通过使用Truedepth摄像机系统里面的传感器和点阵投影仪，投射出3万多个点，就能形成一张完整的3D“脸谱”来识别用户脸部。同时，经过不断被训练的神经网络建模识别人脸，当用户的iPhone X识别用户脸部的次数越多，就对用户越熟悉（无论是换发型、留胡子、戴帽子都能认出用户）。\n\n#### 虹膜识别\n\n· 虹膜识别的误识别率可低至150万分之一，是当前应用最方便和最精确的生物识别技术。具有高度独特性、稳定性（可以保持数十年没有大的变化）、不可更改的特点。\n\n· 虹膜识别技术因为设备复杂、扫描距离短（一般要求在7英寸范围内），以及使用者心理上对健康的担心，未能在民用市场大量应用，主要应用在一些高科技公司的门禁和考勤系统上。\n\n#### 声纹识别\n\n· 声纹识别技术通过语音波形中反映说话人生理和行为特征的语音参数来翻遍说话人的身份。\n\n· 无论是密码输入、指纹识别、人脸识别、虹膜识别，还是声纹识别，如果有人千方百计想获取你的宝贵数据，可以通过社会工程、多次密码输入，以及指纹、人脸、虹膜、声纹的造假方式获取，因此身份认证技术的安全性是相对的。\n\n#### 心率检测\n\n· 监测心跳速率和心脏活动是用心电图（Electrocardiography ，ECG）设备来完成的。由于使用心电图设备需要在人体皮肤表面贴上电极才能记录心脏的电位变化，因此可以使用便捷的PPG（Photoplethysmography，光体积描记法），利用光电传感器来监测经过人体血液吸收后的反射光的强度，从而计算出当前心率是多少（智能手环上的绿色LED灯）。\n\n· PPG可以通过记录心率的变化评估运动的强度和效果，同时还能监测用户的睡眠质量——华为的TruSleep睡眠监测技术。\n\n#### 肌肉电信号\n\n· 当人们收缩肌肉时，皮肤表面会产生肌电信号。肌电图（Electromyography，EMG）用户描绘肌电信号的变化。\n\n· 2014年加拿大公司Thalmic Labs推出的手势识别硬件——MYO腕带，安装了8个EMG，通过表面肌电图SEMG就能识别用户手臂肌肉收缩时肌电信号的变化，同时，通过内置的陀螺仪监测手臂的运动速度，于EMG结合可以对手臂的空间运动和手势进行识别，并将其转换成操作命令。\n\n![](/download/attachments/101812282/image2023-5-4_16-54-55.png?version=1&modificationDate=1683190495164&api=v2)\n\n· 与用计算机视觉实现的手势识别相比，EMG不存在摄像头隐私泄露的问题，且不会受到角度、光照、遮挡的影响，也不需要很高的算力就能单独完成计算。\n\n#### 脑电波\n\n· 脑电图（Electroencephalography,EEG）是一种使用电生理指标记录大脑活动的方法，基于EEG的脑机接口（Brain Computer Interface，BCI）是人机交互和神经工程领域最活跃的研究方向之一。\n\n![](/download/thumbnails/101812282/image2023-5-4_16-55-17.png?version=1&modificationDate=1683190517357&api=v2)\n\n· 分为非侵入式和侵入式：美国NeuroSky公司研发的非侵入式脑电波玩具MindFlex，是一款便携性高的可穿戴式设备。虽然非侵入式装置方便佩戴，但是由于颅骨对信号的衰减作用和对神经元发出的电磁波的分散和模糊效应，使得记录到信息的分辨率并不高，也很难确定发出信号的脑区或相关的单个神经元。埃隆·马斯克的公司研发的Neuralink的入侵式装备Link，植入到了一只名叫Gertrude的猪的颅骨，可以通过1024个穿透脑细胞的薄电极与脑细胞交流，然后将脑细胞产生的信号通过蓝牙传输到手机上。\n\n· 除了ECG、PPG、EMG、EEG外，生理信号皮肤点活动（Electro Dermal Activity，EDA）可用于情感识别，也可用于心理负荷、压力和沮丧的测量。\n\n  \n\n  \n\n  \n\n[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)\n\n[Configure buttons visibility](/users/tfac-settings.action)"
}