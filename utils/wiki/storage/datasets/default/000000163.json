{
	"title": "文本/语音驱动2D数字人模型服务调研",
	"author": "王宇",
	"publishTime": "九月03,2024",
	"readTime": "12s",
	"tags": "[\"马冬颖\"]",
	"description": "马冬颖",
	"article": "1\\. 概述\n======\n\n1.1. 定义\n-------\n\n虚拟形象语音动画合成技术，可以让用户输入文本或语音，通过一定规则或者深度学习算法生成对应的虚拟形象人脸表情系数，从而完成3D虚拟形象的口型和面部表情的精准驱动。\n\n1.2. 驱动方式\n---------\n\n### 1.2.1. 语音驱动\n\n①语音作为驱动源头。将语音输入到深度模型，预测嘴型和面部微表情系数。\n\n②该方法不受限于不同人、国家，但是受到语音特性（音色、强度、噪声等）影响较大，较难提升模型的[泛化能力](https://www.zhihu.com/search?q=%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22575651991%22%7D)。\n\n### 1.2.2. 文本驱动\n\n①文本作为驱动源头。将文本时间序列转换成音素时间序列，并输入到深度模型，预测嘴型和面部微表情系数。\n\n②该此方法只与文本内容相关，不受语音特性变换影响。但是模型受限于不同国家的文本语言；同一文本内容、不同类型的合成声音，最后合成的口型及面部表情相似度高，缺乏风格和特性。\n\n### 1.2.3. 语音和音素融合驱动\n\n①语音和音素同时作为驱动源头。\n\n②该方法融合语音和文本两个模态的信息，驱动系数更准确，效果更好，但模型更复杂。\n\n1.3. 应用场景\n---------\n\n虚拟主持人（新闻播报）、虚拟客服、虚拟教师（教学视频）、影视制作等\n\n2\\. 技术路线\n========\n\n2.1. 多模型组合\n----------\n\n### 2.1.1. 组合结构\n\n文本到语音-语音驱动表情和口型-动作迁移-声音与视频合成\n\n文本生成语音: [https://github.com/weineng-zhou/text2voice](https://github.com/weineng-zhou/text2voice)\n\n语音驱动表情和嘴型：[https://github.com/YuanxunLu/LiveSpeechPortraits](https://github.com/YuanxunLu/LiveSpeechPortraits)\n\n动作迁移: [https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model)\n\n### 2.1.2. 相关服务\n\n#### 2.1.2.1. [Wav2Lip](https://blog.csdn.net/u012193416/article/details/130152327)\n\nWav2Lip是一款基于深度学习的唇语同步技术，它能够将任意音频与视频中的口型进行精准匹配，生成高度逼真的唇语同步效果。\n\n**基本原理：**Wav2Lip模型通过提取输入视频中的人物口型特征以及音频中的语音特征，利用深度学习算法将这些特征进行匹配，最终生成一段新的视频，其中人物的口型与音频中的语音完全匹配。即使用语音信号和人脸图像来训练一个生成器网络，该网络可以将输入的语音信号转换为对应的唇形。\n\n**模型组成：**基于生成对抗网络（GAN），这种网络由两部分组成：生成器和判别器。生成器的任务是依据输入的音频波形生成逼真的面部动画，而判别器则负责区分生成的动画与真实的面部动画\n\n**输入输出：W**av2Lip的输入为一段音频和一段视频，输出为与音频相匹配的视频。\n\n**代码地址：**[https://github.com/Rudrabha/Wav2Lip](https://github.com/Rudrabha/Wav2Lip)\n\n**体验地址：**[https://synclabs.so/](https://synclabs.so/)  （需github或者Google登录）\n\n#### 2.1.2.2. [GFPGAN](https://qianfanmarket.baidu.com/article/detail/184189)\n\nGFPGAN是一种基于生成对抗网络的图像生成与复原技术，旨在将低分辨率或低质量的面部图像转换为高分辨率、高质量的图像。通过深度学习的方法，能够识别并修复老旧照片中的损坏部分，同时生成高清、逼真的面部图像。\n\n**基本原理：**使用低分辨率的图像作为输入，通过生成器网络将其转换为高分辨率的图像。 \n\n**模型组成：**该模型基于生成对抗网络（GAN），包括两个子网络：一个是生成器网络，用于将低分辨率图像转换为高分辨率图像；另一个是判别器网络，用于评估生成的图像是否逼真。\n\n**输入输出：**GFPGAN的输入为低分辨率或低质量的面部图像，输出为警告过修复和增强的高分辨率高质量面部图像。\n\n**代码地址：** [https://github.com/TencentARC/GFPGAN](https://github.com/TencentARC/GFPGAN)\n\n#### 2.1.2.3. WAV2Lip-GFPGAN\n\nWAV2Lip-GFPGAN 是一种基于生成对抗网络（GAN）的模型，旨在将音频波形直接转换为面部动画，尤其关注于唇部动作的生成与同步，并进一步利用GFPGAN对生成的面部图像进行超分辨率处理，以提升画质。\n\n**基本原理：**由两个模型共同完成，WAV2Lip负责人物与口型匹配并生成对应的视频，GFPGAN负责将视频的每一帧图片进行高清修复，最后再将高清修复的图片合成视频。 ![](/download/thumbnails/129198201/image2024-8-20_11-6-31.png?version=1&modificationDate=1724123191352&api=v2)\n\n**输入输出：**Wav2Lip-GFPGAN模型的输入包括音频和视频，输出是经过超分辨率处理的、与输入音频同步的高质量面部动画。\n\n**代码地址：**[https://github.com/ajay-sainy/Wav2Lip-GFPGAN](https://github.com/ajay-sainy/Wav2Lip-GFPGAN)\n\n#### 2.1.2.4. [SadTalker](https://developer.baidu.com/article/details/3268719)\n\nSadTalker是通过隐式3D系数调制来驱动风格化音频，生成具有人脸动作的视频。可以根据一张图片、一段音频，合成面部说这段语音的视频。\n\n**基本原理：**SadTalker模型的基本原理是通过从音频信号中学习并生成三维运动系数，然后利用这些系数通过一个三维面部渲染器生成逼真的数字人视频。主要分为几个步骤。首先，它生成一个三维的脸部模型（3DMM），接下来利用三维面部渲染器来生成[视频](https://cloud.baidu.com/product/mct.html)，最后将生成的3DMM系数映射到面部渲染器的三维关键点空间，以生成最终的视频。\n\n**输入输出：**输入为一张真实人物图片和一个音频文件，输出为与音频相匹配的会说话有表情动作的数字人视频\n\n**代码地址：**[https://github.com/Winfredy/SadTalker](https://github.com/Winfredy/SadTalker)\n\n2.2. 一个模型大力出奇迹\n--------------\n\n### 2.2.1. 驱动类型\n\n#### 2.2.1.1. 音频驱动\n\n**定义：**是指仅将语音作为驱动源，使用输入音频信号和参考图像或视频生成说话头部视频。\n\n主要分为两个方面：\n\n  \n\n①视频中嘴型匹配：输入一段**视频**和一段**驱动音频**，无需**额外训练**，来修改原视频中的嘴型，同时保持原有视频其他内容不变。![](https://pic4.zhimg.com/v2-727ab83ec41a35bd725da1847a1eb357_r.jpg)\n\n②单张[图片生成](https://www.zhihu.com/search?q=%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22665109013%22%7D)视频：输入一张**图片**和一段**驱动音频**，无需额外训练来驱动原图像，得到**风格化**的**嘴型准确**驱动结果。\n\n**劣势：**与语音相关性较强，受语音特性（音色、强度、噪声等）影响较大，可能由于音频信号相对较弱而导致驱动上的不稳定。\n\n#### 2.2.1.2. 面部关键点驱动\n\n**定义：**是指使用参考图像和关键点控制来生成说话头部视频\n\n**劣势：**对面部关键点信息的过度控制，可能会导致生成的视频不自然\n\n#### 2.2.1.3. 音频+面部点关键点驱动\n\n**定义：**音频 + 选定关键点驱动是指使用输入音频信号、参考图像和选定关键点控制来生成说话头部视频。\n\n### 2.2.2. 相关产品\n\n#### 2.2.2.1. [微软 GAIA](https://heehel.com/aigc/gaia-microsoft-groundbreaking-technology.html)（Generative AI for Avatar）\n\nGAIA技术能够仅凭一张面部照片及语音或文字指令生成虚拟人物视频，而且人物的表情、动作都是可以通过文字进行控制的，如“张嘴”“伤心”“惊讶”等指令\n\n**基本原理：**GAIA的工作原理分为以下几个步骤：分离运动和外观表示（模型能够区分视频中由于说话而产生的运动和那些保持不变的外观部分）、使用变分自编码（这个过程帮助模型学习如何准确地捕捉和再现人物的面部特征和表情）、基于语音的运动序列生成（模型可以根据给定的语音输入生成相应的面部运动）、在推理过程中的应用（生成的这些运动序列被解码成视频，展示虚拟头像的说话和表情动作）、控制和文本指令的应用（允许根据文本指令生成虚拟头像的视频剪辑)\n\n**输入输出：**①输入为一张面部图片和音频，输出与音频相匹配的虚拟人物视频；②输入一张面部图片和视频，输出一个基于输入图片的肖像模仿参考视频的新视频。[GAIA 允许控制头部动作和微表情\\_哔哩哔哩\\_bilibili](https://www.bilibili.com/video/BV1KG411v77p/?spm_id_from=333.976.0.0&vd_source=5b9bc5847fee194b4c1b23a8f3cb5642)\n\n#### 2.2.2.2. [腾讯ID-Animator](https://www.jiqizhixin.com/articles/2024-05-18-7)\n\nID-Animator能够根据单张参考面部图像生成具有高度个性化特征的视频内容，同时保留图像中的人物身份特征，并能根据文本提示词调整视频内容。\n\n**基本原理：**通过结合预训练的文本到视频扩散模型和轻量级面部适配器，有效地从可学习的面部潜在查询中编码出与身份相关的嵌入信息，实现了高效的视频生成，且无需针对特定身份进行额外的训练。\n\n**主要功能：**\n\n1.  能够利用文本引导，动态调整角色的多维度属性，包括发型、服饰、性别，执行特定的动作和改变角色所处环境等。\n2.  能够将不同身份的特征以不同比例混合，从而生成结合两种身份特征的视频。\n3.  与 ControlNet 结合：用户能通过上传特定的引导图像或图像序列，精确指引视频中每一帧的生成细节。\n4.  ID-Animator 能够和 Civitai 社区的多种模型兼容，即使在未曾训练过的模型权重上也能保持有效性，能够生成多种风格的视频。\n\n**项目地址：**[https://github.com/ID-Animator/ID-Animator](https://github.com/ID-Animator/ID-Animator)\n\n#### 2.2.2.3. [阿里echoMimic](https://blog.csdn.net/xiaobing259/article/details/140594415)\n\nEchoMimic是一款开源AI数字人项目，旨在通过先进的深度学习技术，将静态图像转化为具有动态语音和表情的数字人像。\n\n**基本原理：**同时使用音频和面部标志点进行训练。运用复杂的深度学习模型，结合音频特征提取和面部标记点定位来预测和生成与语音同步的面部表情和口型变化。\n\n**输入输出：**输入为一段音频和一张清晰的面部图像，输出为口型与音频相匹配的肖像视频。\n\n**项目地址：** [https://github.com/BadToBest/EchoMimic](https://github.com/BadToBest/EchoMimic)\n\n**整合包：**[https://deepface.cc/thread-278-1-1.html](https://deepface.cc/thread-278-1-1.html)（电脑配置不足，需要16G内存与至少6GN卡显存，网页版没有权限体验）\n\n#### 2.2.2.4. 其他模型\n\n[Hallo模型](https://blog.csdn.net/nulifancuoAI/article/details/139844496)：基于语音音频输入来驱动生成逼真且动态的肖像图像视频，采用分层音频驱动视觉合成模块，通过交叉注意力机制建立音频与视觉特征之间的精确对应关系。\n\n[VASA-1模型](http://VASA-1：一张图片和一段语音生成逼真说话视频 - AIHub | AI导航)：能够将单一静态图像和一段语音音频转换成逼真的对话面部动画。支持逼真的面部动画、自然头部动作以及实时视频生成，并且具备多语言支持和技术泛化能力\n\n[Speech2Vedio框架](https://zhuanlan.zhihu.com/p/575651991)：通过从语音音频输入合成虚拟人全身运动（包括头、口、臂等）视频\n\n[AniPortrait框架](http://数字人解决方案——AniPortrait音频驱动的真实肖像动画合成-CSDN博客)：专门用于生成由参考肖像图像和音频样本驱动的高质量动画\n\n[X-Portrait](https://wiki.yingzi.com/x-portrait~~~video驱动的面部视频生成 - 哔哩哔哩 \\(bilibili.com\\)、x-portrait: Expressive Portrait Animation with Hierarchical Motion Attention \\(yiyibooks.cn\\))：能够将一张静态肖像图通过一段驱动视频生成一组头部运动的序列，同时保留原本肖像图的身份特征和背景内容。\n\n3\\. 体验反馈\n========\n\n3.1. wav2lip ( [https://synclabs.so/](https://synclabs.so/) )  \n---------------------------------------------------------------\n\n### 3.1.1. 测验内容\n\n测验1\n\n*   输入文件：[三字经1.mp3](/download/attachments/129198201/%E4%B8%89%E5%AD%97%E7%BB%8F1.mp3?version=1&modificationDate=1725346370249&api=v2)、[视频.mp4](/download/attachments/129198201/%E8%A7%86%E9%A2%91.mp4?version=1&modificationDate=1725346388799&api=v2)\n*   输出文件：[生成视频1.mp4](/download/attachments/129198201/%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%911.mp4?version=1&modificationDate=1725347330919&api=v2)\n\n测验2\n\n*   输入文件：[播报视频2.mp4](/download/attachments/129198201/%E6%92%AD%E6%8A%A5%E8%A7%86%E9%A2%912.mp4?version=1&modificationDate=1725351218770&api=v2)、[第七套广播体操.mp3](/download/attachments/129198201/%E7%AC%AC%E4%B8%83%E5%A5%97%E5%B9%BF%E6%92%AD%E4%BD%93%E6%93%8D.mp3?version=1&modificationDate=1725351248968&api=v2)\n*   输出文件：[生成视频2.mp4](/download/attachments/129198201/%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%912.mp4?version=1&modificationDate=1725351766891&api=v2)\n\n测验3\n\n*   输入文件：[播报视频2.mp4](/download/attachments/129198201/%E6%92%AD%E6%8A%A5%E8%A7%86%E9%A2%912.mp4?version=1&modificationDate=1725351218770&api=v2)、[观沧海-音频.mp3](/download/attachments/129198201/%E8%A7%82%E6%B2%A7%E6%B5%B7-%E9%9F%B3%E9%A2%91.mp3?version=1&modificationDate=1725351216420&api=v2)\n*   输出文件：[生成文件3.mp4](/download/attachments/129198201/%E7%94%9F%E6%88%90%E6%96%87%E4%BB%B63.mp4?version=1&modificationDate=1725357347052&api=v2)\n\n### 3.1.2. 测试结果\n\n体验总结：总共进行了12次测试，仅有3次成功，其他都失败了（未找到原因，用了跟第二次测验相同的素材，依然是失败）。基于三个测验来看，整体的体验效果较差，口型、画质、表情动作的效果都不太好。\n\n#### 3.1.2.1. 视觉逼真度\n\n**口型匹配：**很差，口型跟播报的内容完全不匹配，仅少量几个字能勉强对的上，音频刚开始只有背景音乐，还未有文字播报时，生成的视频就已经有口型了。\n\n**时间对齐：**良好，口型跟音频的时间对齐比较准确，没有延迟性\n\n**面部表情：**较差，面部表情不自然部分帧节口部变形较明显\n\n**人物动作：**一般，头部和肢体动作都是按照输入视频的人物头部和肢体动作循环播放，如输入的视频为10s，音频为30s，则生成的视频循环三次输入视频的人物动作。没法根据音频的内容来做相应的适配。\n\n![](/download/thumbnails/129198201/image2024-9-3_15-33-50.png?version=1&modificationDate=1725348830511&api=v2)![](/download/thumbnails/129198201/image2024-9-3_15-36-14.png?version=1&modificationDate=1725348975786&api=v2)![](/download/thumbnails/129198201/image2024-9-3_17-57-12.png?version=1&modificationDate=1725357432975&api=v2)![](/download/thumbnails/129198201/image2024-9-3_17-59-45.png?version=1&modificationDate=1725357585486&api=v2)\n\n#### 3.1.2.2. 图像质量\n\n**清晰度：**较差，生成出来的视频口部和下巴区域的清晰度明显降低（下面图片对比，左图为导入视频画质，右图为生成视频画质）\n\n**稳定性：**一般，画面不够稳定，当进行新一轮的人物动作循环时，衔接不流畅，会有目前的卡壳感觉\n\n![](/download/thumbnails/129198201/image2024-9-3_15-16-19.png?version=1&modificationDate=1725347779815&api=v2)vs![](/download/thumbnails/129198201/image2024-9-3_15-16-58.png?version=1&modificationDate=1725347818900&api=v2)、![](/download/thumbnails/129198201/image2024-9-3_18-0-52.png?version=1&modificationDate=1725357652339&api=v2)vs![](/download/thumbnails/129198201/image2024-9-3_18-1-17.png?version=1&modificationDate=1725357677300&api=v2)\n\n#### 3.1.2.3. 生成效率\n\n**处理速度：**不太稳定，时快时慢，从视频和音频文件上传到新视频的生成，第一个视频耗时6分钟，第二个视频耗时12分钟\n\n**生成异常：**异常频率较高，生成失败的情况较频繁，总共测试了12次，仅有3次成功\n\n![](/download/attachments/129198201/image2024-9-3_17-24-19.png?version=1&modificationDate=1725355459575&api=v2)\n\n  \n\n  \n\n[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)\n\n[Configure buttons visibility](/users/tfac-settings.action)"
}