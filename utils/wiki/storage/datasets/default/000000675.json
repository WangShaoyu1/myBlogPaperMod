{
	"title": "3DAIGC探索",
	"author": "王宇",
	"publishTime": "十一月02,2023",
	"readTime": "12s",
	"tags": "[\"（四）美术资源\"]",
	"description": "（四）美术资源",
	"article": "**一、背景**\n--------\n\nFTT系统能力的本质是形象动画资源、驱动形象的智能驱动算法和知识库、前端渲染能力。\n\n2023年2月正式与合作伙伴启动对接后，影子虚拟人项目内部聚焦的点在结合应用的工程落地、知识库基本能力的学习应用。关于角色形象资源的工作，在5月进行了少量调研，详见[3、虚拟人形象与动画制作、骨骼绑定、唇形表情动作驱动技术调研](/pages/viewpage.action?pageId=97901833)，但调研工作未深入延续，也未结合实际应用开展更具体工作以了解技术细节。近期结合新形象资源制作需求，我们重新捡起形象资源的相关工作，并针对项目初期领导层提出的千人千面的思路，结合当下热点的AIGC，进一步开展一些技术调研与拓展讨论。\n\n**二、3D资源适配FTT的流程介绍**\n--------------------\n\n#### 1、在[卡通风格模型与动画资源标准及工作流程](/pages/viewpage.action?pageId=109729662)中，整体介绍了卡通形象资源的美术标准与流程。在此进行部分信息补充：\n\n（1）为了优化模型与动画文件在前端的运行效率，虚拟人应用采用了类似游戏引擎的处理，因此模型与动画文件有模型面数、文件格式等要求，以便于适配渲染引擎；  \n（2）口型对齐的实现，需要按照一定帧数间隔制作不同音节口型的动画文件并上传系统，通过口型对齐算法，将虚拟人口型与拉取的TTS内容对齐；  \n（3）动作表情的智能驱动，通过配置动画文件与行为情绪标签的映射关系实现；目前支持8种情绪标签（愤怒、怀疑、兴奋、坚定、快乐、喜爱、得意、沮丧）与47种行为标签，通过语义分析获取交互内容的标签后，控制相应的动画文件播放。\n\n#### 2、在与FTT第一阶段合作中，基于影子方提出的千人千面的业务需求，交付的写实型虚拟人形象生成的解决方案情况如下：\n\n（1）制作写实型虚拟人的形象模板文件（模型拆分为部件制作，包含头、头发、上装、下装、鞋子五个部件）；  \n（2）制作多个服饰、发型部件模型，通过替换部件模型，实现角色形象服饰的自定义设置；  \n（3）将用户上传的形象照片进行一定处理后，作为贴图附在头部模型表面，以获得一定近似程度的形象模型；这种贴图的方式，无法改变模型原有形状，处理不了脸型、侧脸轮廓、耳朵等面部特征、相似效果比较有限。\n\n**三、3D AIGC相关技术**\n-----------------\n\n#### 1、3D资源数据\n\n3D资源包括模型、骨骼、贴图、动画文件等，先从最基础的3D模型开始讨论。3D模型的数据表示，可以通过**网格（mesh）、点云（point cloud）、神经场、立体像素（voxel）、空间分布函数（SDF）、多视角图片**等。\n\n3D资源数据表示方式多样，但如果应用到渲染管线中，通常只有**网格mesh**这种主要方式，因为网格具备足够的灵活性（可以通过调整顶点的位置和连接关系创建各种形状和曲面）和可编辑性（可以添加、删除或移动顶点、面和边，从而改变物体的形状），并且可以和现有渲染技术及工具便捷继承。  \n**点云**通常是无序的，进行渲染与处理时需要额外的算法来处理点之间的连续性和拓扑关系，且占用较多存储空间。  \n**神经场**是由神经网络完全或部分参数化的域，内存消耗低于其他表示；NeRF 是神经场 3D 重建的最新新兴表示，可实现高分辨率与逼真场景，但需要大量计算资源和时间进行训练，处理大规模场景和复杂光照条件困难，无法结构化数据。  \n**体素**对于复杂形状表达能力弱，难以精确处理细节与曲面，也需要大量的内存和计算资源来储存与处理其数据。\n\n网格\n\n点云\n\n神经场\n\n体素\n\n网格\n\n点云\n\n神经场\n\n体素\n\n![](/download/thumbnails/109729702/image2023-11-2_9-54-22.png?version=1&modificationDate=1698890062360&api=v2)\n\n![](/download/thumbnails/109729702/image2023-11-2_9-54-54.png?version=1&modificationDate=1698890094212&api=v2)\n\n![](/download/attachments/109729702/image2023-11-2_9-55-49.png?version=1&modificationDate=1698890149625&api=v2)\n\n![](/download/thumbnails/109729702/image2023-11-2_9-56-8.png?version=1&modificationDate=1698890168575&api=v2)\n\n#### 2、自动化建模\n\n在前期调研文档中，提及目前建模方式包含手工建模、相机扫描、光场扫描等，但这些的前提均是有一定的物体参照，而非完全的智能生成的建模；在这些工作流程中，可能涉及到的相关技术包括\n\n（1）**同步定位与建图**（SLAM，基于相机位置与连续视图）、**三维重建**（SFM，基于多角度视图），基于传感器与硬件资源的处理方式，广泛用在建筑设计、文化遗产保护、虚拟现实、游戏开发、机器人导航和医学图像处理等领域，是XR的关键技术，生成的是点云的模型；  \n（2）工业建模软件的插件中提供的**自动重拓扑、自动展UV、自动生成纹理贴图**等中间环节处理的算法，当然效果仍然无法和手动的结果相比；  \n（3）基于模板模型匹配的建模，包括基于与图片相似度计算的卡通/写实模板模型匹配、捏脸等。\n\n#### 3、3D资源AIGC生成\n\n近几年该领域相关工作很多，以下仅进行概述，不再额外进行过多详述，感兴趣的可以参考相关文档。\n\n**（1）训练和推理都基于3D数据的路线**\n\n[AutoSDF](https://arxiv.org/abs/2203.09516)：将 3D 形状上的分布建模为 3D 形状的离散化、低维、符号网格状潜在表示上的非顺序自回归分布，提出了一种用于 3D 的自回归先验来解决多模态 3D 任务，例如3D形状制作、重建和生成。  \n[MeshDiffusion](https://meshdiffusion.github.io/)：用可变形的四面体网格表示网格，然后在这个直接的参量化上建立扩散模型。  \n[GET3D](https://nv-tlabs.github.io/GET3D/)：基于可微表面建模、可微渲染以及2D 生成对抗网络，生成具有复杂拓扑结构、丰富的几何细节和高保真纹理的显式纹理3D 网格的生成模型。\n\n        生成速度快，但是也有比较直接的问题，3D 的数据集（**ShapeNet** 有 51 K 模型、**Objaverse** 有 800+K、商业模型网站 **SketchFab** 有 5M）和 2D 的 LAION-5B（Stable diffusion所用的开源数据集）差了至少三个数量级，这一类方法比较难实现数据多样性。Stable diffusion使用了大概20多亿张带文字的图片来训练，这20多亿图片代表人类从单视角描绘世间万物的内容；SD根据提示词指引，通过概率模型来创造出一副关于提示词描述内容的图片，用提示词生成图片的内容特征，也就在这20多亿图片包含的内容里了。要想用类似的方法生成高质量的3Dmesh内容，可以认为也需要差不多数量级已经制作好的3D内容及对应文字标识来训练。\n\n  \n\n**（2）借助2D-AIGC的先验知识驱动3D生成**\n\na. [Point·E](https://arxiv.org/pdf/2212.08751.pdf)（openAI）：以文字为输入，用 2D diffusion 模型GLIDE生成一张图片，然后用 3D 点云的 diffusion 模型基于输入图片生成点云，在单个 GPU 上仅 1-2 分钟生成3D模型，质量上没有达到3D生成任务上当前最优效果，但能比其他方法快一到两个数量级；  \nb. Shap-E（openAI）：Point·E的升级版本，收敛速度更快、生成质量更好（但实际效果仍较差）；  \nc. [DreamFusion](https://dreamfusion3d.github.io/)（谷歌）：支持将文本生成3D。通过 2D 生成模型(如Imagen)生成多个视角的 3D 视图，然后用NeRF重建。可生成模型多样性强，但需要在NeRF 和 Imagen 间来回迭代，时间与资源耗费量大；  \nd. [Magic3D](https://research.nvidia.com/labs/dir/magic3d/)\\-（英伟达）：谷歌DreamFields 的升级版本，通过将重建过程分为两步（先仅用NeRF进行比较粗糙的模型重建，然后采用一个光栅化渲染器对其进行优化生成高分辨率渲染图像），解决了NeRF优化慢和低分辨率的两个问题；比DreamFusion快两倍、分辨率高八倍，但在使用8块英伟达 A100 GPU情况下，仍需要40分钟才能完成一次渲染；  \ne. [ProlificDreamer](https://ml.cs.tsinghua.edu.cn/prolificdreamer/)（清华大学）：采用NeRF优化和纹理网格微调的两个阶段进行生成，较DreamFusion在过饱和、过平滑、缺少细节等问题上表现更好，但也需要数个小时；  \nf. [TextMesh](https://arxiv.org/abs/2304.12439)（谷歌近期论文）：通过扩展NeRF，采用SDF骨干，改进了3D 网格提取，提出一种新的方法来调整网格纹理，消除高饱和度的影响，并改善输出的三维网格的细节；  \ng. [Zero-1-to-3](https://arxiv.org/abs/2303.11328)（哥伦比亚大学）：利用了大规模扩散模型对自然图像学习的几何先验；给定一张RGB图像，视点条件扩散方法可以用于单幅图像的三维重建任务；  \nh. [One-2-3-45](https://www.163.com/dy/article/I9RL591G0514CVVN.html)（加州大学圣迭戈分校等机构）：在Zero-1-to-3基础上开展的工作，先利用 2D 扩散模型生成多视角图像，然后希望利用这些多视角图像来重建 3D 模型，将不太一致的多视图像提升到三维空间（mesh或SDF形式）；质量较逐物体优化的模型（c\\\\d\\\\e）差，但生成速度快（约45s可生成），多样性、一致性表现较好。\n\n这一路线的瓶颈在于算法，生成512x512的2D图片的计算资源如果尚能接受，基于2D升维3D，效果比较好的在路径上多是通过多尺度的数据扩充来逐级优化，如果不进行算法创新，直接将2D暴力扩充到512x512x512的3D，计算工作量非常巨大。且业内目前也少人研究AI建模实现合理布线等更高模型质量要求的工作。\n\n  \n\n以上两类项目合辑可参考：[https://github.com/threestudio-project/threestudio](https://github.com/threestudio-project/threestudio)，该页面也包含各项目重现效果。\n\n  \n（3）[影眸科技](https://deemos.com/)做的 [ChatAvatar](https://www.bilibili.com/video/BV1oo4y1h7a9/?vd_source=9d4b42484c42453fccdfc6c1078cdb3f)、微软亚洲研究院的 [Rodin](https://3d-avatar-diffusion.microsoft.com/)，聚焦处理**特定业务场景下的生成任务**，也是比较有代表性的工作。\n\n#### 4、应用于影子虚拟人场景，可行的路线\n\n瓶颈：  \n3D领域缺乏大量高质量的数据集，数量与细节质量待提升；可控程度低。  \n  \n\n建模与驱动过程中，AI暂时无法参与的阶段：  \n（1）模型纹理材料调整；  \n（2）骨骼绑定；  \n（3）动作动画；\n\n除了基于模型模板的模型生成方式，落地到实际应用，AIGC生成3D模型可能还是需要多步组合，通过AIGC参与部分步骤以提高工作效率。可行的技术路线：  \n（1）由文本生成形象设计图，使用Stable Diffusion和Midjourney（Stable Diffusion的内容可控程度更高，可用于细节调整）；  \n（2）文本或图形到3D模型（Shap-E/openAI，生成速度与准确度高）；  \n（3）3D模型优化（Magic 3D/英伟达或ProlificDreamer/清华，生成质量高、耗时、不同角度一致性差）；  \n（4）人工调整。\n\n在实现通用基础模型的AIGC之外，可以考虑类似于影眸科技的ChatAvatar的框定业务范围的模型生成，或者应用三.2.2提及的自动生成贴图的技术，来基于指定基础模型生成贴图并渲染（[Meshy](https://www.meshy.ai/)、[DreamTexture](https://github.com/carson-katri/dream-textures)等）\n\n**四、驱动**\n--------\n\n可以发现目前3D AIGC的存在低质量、不可控、低速度的问题，主要受限于相关技术瓶颈（包括3D数据、算法、硬件算力等）；而这些瓶颈的解决，是依赖技术驱动（基础模型突破、通用人工智能的实现），还是商业驱动（3D资产最大的消费行业在游戏，什么样的商业需求能够推动解决其技术瓶颈是未知的，但如果类似Apple Vision Pro的产品能够真的带来拓展现实的iphone时刻，3D资产的消费群体，也许真的能够从集中在游戏产业拓宽到大众消费群体），目前尚不可知。但从近两年的发展趋势看，依赖于底层硬件、底层模型以及终端硬件的突破，新的交互形态初见雏形，3D AIGC的方向仍然值得探索，期望未来能站在巨头的肩膀上开展更多创新工作。"
}