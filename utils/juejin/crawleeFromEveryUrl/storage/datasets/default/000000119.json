{
	"title": "Web Audio在音频可视化中的应用",
	"author": "",
	"publishTime": "2019-09-26",
	"readTime": "阅读10分钟",
	"tags": "[\"数据可视化中文技术社区\",\"前端开发社区\",\"前端技术交流\",\"前端框架教程\",\"JavaScript 学习资源\",\"CSS 技巧与最佳实践\",\"HTML5 最新动态\",\"前端工程师职业发展\",\"开源前端项目\",\"前端技术趋势\"]",
	"description": "本文有两个关键词：音频可视化和Web Audio。前者是实践，后者是其背后的技术支持。 Web Audio 是很大的知识点，本文会将重点放在如何获取音频数据这块，对于其 API 的更多内容，可以查看 MDN。 另外，要将音频数据转换成可视化图形，除了了解 Web Audio 之…",
	"article": "本文有两个关键词：`音频可视化`和`Web Audio`。前者是实践，后者是其背后的技术支持。 Web Audio 是很大的知识点，本文会将重点放在**如何获取音频数据**这块，对于其 API 的更多内容，可以查看 [MDN](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWeb_Audio_API \"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API\")。\n\n另外，要将音频数据转换成可视化图形，除了了解 Web Audio 之外，还需要对 Canvas （特指2D，下同），甚至 WebGL （可选）有一定了解。如果读者对它们没有任何学习基础，可以先从以下资源入手：\n\n*   [Canvas Tutorial](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FCanvas_API%2FTutorial \"https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Tutorial\")\n*   [WebGL Tutorial](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FWebGL_API%2FTutorial%2FGetting_started_with_WebGL \"https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API/Tutorial/Getting_started_with_WebGL\")\n\n什么是音频可视化\n========\n\n> 通过获取频率、波形和其他来自声源的数据，将其转换成图形或图像在屏幕上显示出来，再进行交互处理。\n\n云音乐有不少跟音频动效相关的案例，但其中有些过于复杂，又或者太偏业务。因此这里就现找了两个相对简单，但有代表性的例子。\n\n第一个是用 Canvas 实现的音频柱形图。\n\n![普通版](/images/jueJin/16d6b27242fcdcc.png)\n\n[↑点击播放↑](https://link.juejin.cn?target=https%3A%2F%2Fd1.music.126.net%2Fdmusic%2F1ed3%2F1d0d%2Fc41c%2F6ad74ba6a06eb3657f16415a813a988a.mp4 \"https://d1.music.126.net/dmusic/1ed3/1d0d/c41c/6ad74ba6a06eb3657f16415a813a988a.mp4\")\n\n第二个是用 WebGL 实现的粒子效果。\n\n![升级版](/images/jueJin/16d6b272431165c.png)\n\n[↑点击播放↑](https://link.juejin.cn?target=https%3A%2F%2Fd1.music.126.net%2Fdmusic%2Fce94%2Ff211%2F37cb%2F51d9ea5598f2583db38440cd6c927bf2.mp4 \"https://d1.music.126.net/dmusic/ce94/f211/37cb/51d9ea5598f2583db38440cd6c927bf2.mp4\")\n\n在具体实践中，除了这些基本图形（矩形、圆形等）的变换，还可以把音频和自然运动、3D 图形结合到一起。\n\n![其他效果](/images/jueJin/16d6b2729526c96.png)\n\n[点击查看：pinterest上的一些视觉效果](https://link.juejin.cn?target=https%3A%2F%2Fwww.pinterest.com%2Fmastercassawon%2Faudio-visualizer%2F \"https://www.pinterest.com/mastercassawon/audio-visualizer/\")\n\n什么是 Web Audio\n=============\n\n> Web Audio 是 Web 端处理和分析音频的一套 API 。它可以设置不同的音频来源（包括`<audio>`节点、 ArrayBuffer 、用户设备等），对音频添加音效，生成可视化图形等。\n\n接下来重点介绍 Web Audio 在可视化中扮演的角色，见下图。\n\n![Web Audio工作流](/images/jueJin/16d6b2724343117.png)\n\n简单来说，就是**取数据** + **映射数据**两个过程。我们先把“取数据”这个问题解决，可以按以下5步操作。\n\n1\\. 创建 AudioContext\n-------------------\n\n在音频的任何操作之前，都必须先创建 AudioContext 。它的作用是关联音频输入，对音频进行解码、控制音频的播放暂停等基础操作。\n\n创建方式如下：\n\n```\nconst AudioContext = window.AudioContext || window.webkitAudioContext;\n\nconst ctx = new AudioContext();\n```\n\n2\\. 创建 AnalyserNode\n-------------------\n\nAnalyserNode 用于获取音频的频率数据（ FrequencyData ）和时域数据（ TimeDomainData ）。从而实现音频的可视化。\n\n它只会对音频进行读取，而不会对音频进行任何改变。\n\n```\nconst analyser = ctx.createAnalyser();\nanalyser.fftSize = 512;\n```\n\n关于 fftSize ，在 [MDN](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.mozilla.org%2Fen-US%2Fdocs%2FWeb%2FAPI%2FAnalyserNode%2FfftSize \"https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/fftSize\") 上的介绍可能很难理解，说是快速傅里叶变换的一个参数。\n\n可以从以下角度理解：\n\n**1\\. 它的取值是什么？**\n\nfftSize 的要求是 2 的幂次方，比如 256 、 512 等。数字越大，得到的结果越精细。\n\n对于移动端网页来说，本身音频的比特率大多是 128Kbps ，没有必要用太大的频率数组去存储本身就不够精细的源数据。另外，手机屏幕的尺寸比桌面端小，因此最终展示图形也不需要每个频率都采到。只需要体现节奏即可，因此 512 是较为合理的值。\n\n**2\\. 它的作用是什么？**\n\nfftSize 决定了 frequencyData 的长度，具体为 fftSize 的一半。\n\n至于为什么是 1 / 2，感兴趣的可以看下这篇文章：[Why is the FFT “mirrored”?](https://link.juejin.cn?target=https%3A%2F%2Fdsp.stackexchange.com%2Fquestions%2F4825%2Fwhy-is-the-fft-mirrored \"https://dsp.stackexchange.com/questions/4825/why-is-the-fft-mirrored\")\n\n3\\. 设置 SourceNode\n-----------------\n\n现在，我们需要将音频节点，关联到 AudioContext 上，作为整个音频分析过程的输入。\n\n在 Web Audio 中，有三种类型的音频源：\n\n*   **MediaElementAudioSourceNode** 允许将`<audio>`节点直接作为输入，可做到流式播放。\n*   **AudioBufferSourceNode** 通过 xhr 预先将音频文件加载下来，再用 AudioContext 进行解码。\n*   **MediaStreamAudioSourceNode** 可以将用户的麦克风作为输入。即通过`navigator.getUserMedia`获取用户的音频或视频流后，生成音频源。\n\n这 3 种音频源中，除了 MediaStreamAudioSourceNode 有它不可替代的使用场景（比如语音或视频直播）之外。 MediaElementAudioSourceNode 和 AudioBufferSourceNode 相对更容易混用，因此这里着重介绍一下。\n\n### MediaElementAudioSourceNode\n\nMediaElementAudioSourceNode 将`<audio>`标签作为音频源。它的 API 调用非常简单。\n\n```\n// 获取<audio>节点\nconst audio = document.getElementById('audio');\n\n// 通过<audio>节点创建音频源\nconst source = ctx.createMediaElementSource(audio);\n\n// 将音频源关联到分析器\nsource.connect(analyser);\n\n// 将分析器关联到输出设备（耳机、扬声器）\nanalyser.connect(ctx.destination);\n```\n\n### AudioBufferSourceNode\n\n有一种情况是，在安卓端，测试了在`Chrome/69`（不含）以下的版本，用 MediaElementAudioSourceNode 时，获取到的 frequencyData 是全为 0 的数组。\n\n因此，想要兼容这类机器，就需要换一种预加载的方式，即使用 AudioBufferSourceNode ，加载方式如下：\n\n```\n// 创建一个xhr\nvar xhr = new XMLHttpRequest();\nxhr.open('GET', '/path/to/audio.mp3', true);\n\n// 设置响应类型为 arraybuffer\nxhr.responseType = 'arraybuffer';\n\n    xhr.onload = function() {\n    var source = ctx.createBufferSource();\n    \n    // 对响应内容进行解码\n        ctx.decodeAudioData(xhr.response, function(buffer) {\n        \n        // 将解码后得到的值赋给buffer\n        source.buffer = buffer;\n        \n        // 完成。将source绑定到ctx。也可以连接AnalyserNode\n        source.connect(ctx.destination);\n        });\n        };\n        \n        xhr.send();\n```\n\n如果将 AnalyserNode 类比中间件，会不会好理解一些？\n\n可以对比一下常规的`<audio>`播放，和 Web Audio 中的播放流程：\n\n![两种播放逻辑对比](/images/jueJin/16d6b2724463aaf.png)\n\n4\\. 播放音频\n--------\n\n对于`<audio>`节点，即使用 MediaElementAudioSourceNode 的话，播放相对比较熟悉：\n\n```\naudio.play();\n```\n\n但如果是 AudioBufferSourceNode ，它不存在 play 方法，而是：\n\n```\n// 创建AudioBufferSourceNode\nconst source = ctx.createBufferSource();\n\n// buffer是通过xhr获取的音频文件\nsource.buffer = buffer;\n\n// 调用start方法进行播放\nsource.start(0);\n```\n\n5\\. 获取 frequencyData\n--------------------\n\n到此，我们已经将音频输入关联到一个 AnalyserNode ，并且开始播放音频。对于 Web Audio 这部分来说，它只剩最后一个任务：获取频率数据。\n\n关于频率， Web Audio 提供了两个相关的 API，分别是：\n\n1.  `analyser.getByteFrequencyData`\n2.  `analyser.getFloatFrequencyData`\n\n两者都是返回 TypedArray ，唯一的区别是精度不同。\n\ngetByteFrequencyData 返回的是 0 - 255 的 Uint8Array 。而 getFloatFrequencyData 返回的是 0 - 22050 的 Float32Array 。\n\n相比较而言，如果项目中对性能的要求高于精度，那建议使用 getByteFrequencyData 。下图展示了一个具体例子：\n\n![getByteFrequencyData](/images/jueJin/16d6b27243e1923.png)\n\n关于数组的长度（ 256 ），在上文已经解释过，它是 fftSize 的一半。\n\n现在，我们来看下如何获取频率数组：\n\n```\nconst bufferLength = analyser.frequencyBinCount;\nconst dataArray = new Uint8Array(bufferLength);\n\nanalyser.getByteFrequencyData(dataArray);\n```\n\n需要注意的是， getByteFrequencyData 是对已有的数组元素进行赋值，而不是创建后返回新的数组。\n\n它的好处是，在代码中只会有一个 dataArray 的引用，不用通过函数调用和参数传递的方式来重新取值。\n\n可视化的两种实现方案\n==========\n\n在了解 Web Audio 之后，已经能用 getByteFrequencyData 取到一个 Uint8Array 的数组，暂时命名为 dataArray 。\n\n从原理上讲，可视化所依赖的数据可以是音频，也可以是温度变化，甚至可以是随机数。所以，接下来的内容，我们只需要关心如何将 dataArray 映射为图形数据，不用再考虑 Web Audio 的操作。\n\n（为了简化 Canvas 和 WebGL 的描述，下文提到 Canvas 特指 `Canvas 2D`。）\n\n1\\. Canvas 方案\n-------------\n\n[点击查看：第1个示例的源码](https://link.juejin.cn?target=https%3A%2F%2Fcodepen.io%2Fjchenn%2Fpen%2FLYPdjWN \"https://codepen.io/jchenn/pen/LYPdjWN\")\n\nCanvas 本身是一个序列帧的播放。它在每一帧中，都要先清空 Canvas ，再重新绘制。\n\n![Canvas工作流程](/images/jueJin/16d6b2726df5ee1.png)\n\n以下是从示例代码中摘取的一段：\n\n```\n    function renderFrame() {\n    requestAnimationFrame(renderFrame);\n    \n    // 更新频率数据\n    analyser.getByteFrequencyData(dataArray);\n    \n    // bufferLength表示柱形图中矩形的个数\n        for (var i = 0, x = 0; i < bufferLength; i++) {\n        // 根据频率映射一个矩形高度\n        barHeight = dataArray[i];\n        \n        // 根据每个矩形高度映射一个背景色\n        var r = barHeight + 25 * (i / bufferLength);\n        var g = 250 * (i / bufferLength);\n        var b = 50;\n        \n        // 绘制一个矩形，并填充背景色\n        ctx.fillStyle = \"rgb(\" + r + \",\" + g + \",\" + b + \")\";\n        ctx.fillRect(x, HEIGHT - barHeight, barWidth, barHeight);\n        \n        x += barWidth + 1;\n    }\n}\n\nrenderFrame();\n```\n\n对于可视化来说，核心逻辑在于：如何把频率数据映射成图形参数。在上例中，只是简单地改变了柱形图中每一个矩形的高度和颜色。\n\nCanvas 提供了丰富的绘制API，仅从 2D 的角度考虑，它也能实现很多酷炫的效果。类比 DOM 来说，如果只是`<div>`的组合就能做出丰富多彩的页面，那么 Canvas 一样可以。\n\n2\\. WebGL 方案\n------------\n\n[点击查看：第2个示例的源码](https://link.juejin.cn?target=https%3A%2F%2Fcodepen.io%2Fjchenn%2Fpen%2FWNezZdN \"https://codepen.io/jchenn/pen/WNezZdN\")\n\nCanvas 是 CPU 计算，对于 for 循环计算 10000 次，而且每一帧都要重复计算， CPU 是负载不了的。所以我们很少看到用 Canvas 2D 去实现粒子效果。取而代之的，是使用 WebGL ，借助 GPU 的计算能力。\n\n在 WebGL 中，有一个概念相对比较陌生——着色器。它是运行在 GPU 中负责渲染算法的一类总称。它使用 GLSL（ OpenGL Shading Language ）编写，简单来说是一种类 C 风格的语言。以下是简单的示例：\n\n```\nvoid main()\n    {\n    gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);\n}\n```\n\n关于着色器更详细的介绍，可以[查看这篇文章](https://link.juejin.cn?target=http%3A%2F%2Fzhangwenli.com%2Fblog%2F2017%2F02%2F24%2Fwhat-is-a-shader%2F \"http://zhangwenli.com/blog/2017/02/24/what-is-a-shader/\")。\n\nWebGL 的原生 API 是非常复杂的，因此我们使用`Three.js`作为基础库，它会让业务逻辑的编写变得简单。\n\n先来看下整个开发流程中做的事情，如下图：\n\n![WegGL开发流程](/images/jueJin/16d6b2726fef527.png)\n\n在这个过程中， uniforms 的类型是简单 Object ，我们会将音频数组作为 uniforms 的一个属性，传到着色器中。至于着色器做的事情，可以简单理解为，它将 uniforms 中定义的一系列属性，映射为屏幕上的顶点和颜色。\n\n顶点着色器和片元着色器的编写往往不需要前端开发参与，对于学过 Unity3D 等技术的游戏同学可能会熟悉一些。读者可以到 [ShaderToy](https://link.juejin.cn?target=https%3A%2F%2Fwww.shadertoy.com%2F \"https://www.shadertoy.com/\") 上寻找现成的着色器。\n\n![ShaderToy](/images/jueJin/16d6b27270ef358.png)\n\n然后介绍以下3个 Three.js 中的类：\n\n**1\\. THREE.Geometry**\n\n可以理解为形状。也就是说，最后展示的物体是球体、还是长方体、还是其他不规则的形状，是由这个类决定的。\n\n因此，你需要给它传入一些顶点的坐标。比如三角形，有3个顶点，则传入3个顶点坐标。\n\n当然， Three.js 内置了很多常用的形状，比如 BoxGeometry 、 CircleGeometry 等。\n\n**2\\. THREE.ShaderMaterial**\n\n可以理解为颜色。还是以三角形为例，一个三角形可以是黑色、白色、渐变色等，这些颜色是由 ShaderMaterial 决定的。\n\nShaderMaterial 是 Material 的一种，它由顶点着色器和片元着色器进行定义。\n\n**3\\. THREE.Mesh**\n\n定义好物体的形状和颜色后，需要把它们组合在一起，称作 Mesh （网格）。有了 Mesh 之后，便可以将它添加到画布中。然后就是常规的 requestAnimationFrame 的流程。\n\n同样的，我们摘取了示例中比较关键的代码，并做了标注。\n\ni. 创建 Geometry （这是从 THREE.BufferGeometry 继承的类）：\n\n```\n    var geometry = ParticleBufferGeometry({\n    // TODO 一些参数\n    });\n```\n\nii. 定义 uniforms ：\n\n```\n    var uniforms = {\n        dataArray: {\n        value: null,\n        type: 't' // 对应THREE.DataTexture\n        },\n        // TODO 其他属性\n        };\n```\n\niii. 创建 ShaderMaterial ：\n\n```\n    var material = new THREE.ShaderMaterial({\n    uniforms: uniforms,\n    vertexShader: '', // TODO 传入顶点着色器\n    fragmentShader: '', // TODO 传入片元着色器\n    // TODO 其他参数\n    });\n```\n\niv. 创建 Mesh ：\n\n```\nvar mesh = new THREE.Mesh(geometry, material);\n```\n\nv. 创建 Three.js 中一些必须的渲染对象，包括场景和摄像头：\n\n```\nvar scene, camera, renderer;\n\n    renderer = new THREE.WebGLRenderer({\n    antialias: true,\n    alpha: true\n    });\n    \n    camera = new THREE.PerspectiveCamera(45, 1, .1, 1e3);\n    \n    scene = new THREE.Scene();\n```\n\nvi. 常规的渲染逻辑：\n\n```\n    function animate() {\n    requestAnimationFrame(animate);\n    \n    // TODO 此处可以触发事件，用于更新频率数据\n    \n    renderer.render(scene, camera);\n}\n```\n\n小结\n==\n\n本文首先介绍了如何通过 Web Audio 的相关 API 获取音频的频率数据。\n\n然后介绍了 Canvas 和 WebGL 两种可视化方案，将频率数据映射为图形数据的一些常用方式。\n\n另外，云音乐客户端上线鲸云动效已经有一段时间，看过本文之后，有没有同学想尝试实现一个自己的音频动效呢？\n\n![云音乐鲸云动效](/images/jueJin/16d6b27272209a9.png)\n\n最后附上文中提到的两段 codepen 示例：\n\n1.  [codepen.io/jchenn/pen/…](https://link.juejin.cn?target=https%3A%2F%2Fcodepen.io%2Fjchenn%2Fpen%2FLYPdjWN \"https://codepen.io/jchenn/pen/LYPdjWN\")\n2.  [codepen.io/jchenn/pen/…](https://link.juejin.cn?target=https%3A%2F%2Fcodepen.io%2Fjchenn%2Fpen%2FWNezZdN \"https://codepen.io/jchenn/pen/WNezZdN\")\n\n> 本文发布自 [网易云音乐前端团队](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fx-orpheus \"https://github.com/x-orpheus\")，文章未经授权禁止任何形式的转载。我们一直在招人，如果你恰好准备换工作，又恰好喜欢云音乐，那就 [加入我们](mailto:grp.music-fe@corp.netease.com \"mailto:grp.music-fe@corp.netease.com\")！",
	"selfDefined": "likes:199,comments:11,collects:266,likes:11194"
}