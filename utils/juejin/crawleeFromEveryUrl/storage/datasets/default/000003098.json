{
	"title": "一文教你在MindSpore中实现A2C算法训练",
	"author": "华为云开发者联盟",
	"publishTime": "2024-06-07",
	"readTime": "阅读5分钟",
	"tags": "[\"人工智能\",\"强化学习\",\"算法中文技术社区\",\"前端开发社区\",\"前端技术交流\",\"前端框架教程\",\"JavaScript 学习资源\",\"CSS 技巧与最佳实践\",\"HTML5 最新动态\",\"前端工程师职业发展\",\"开源前端项目\",\"前端技术趋势\"]",
	"description": "文中的配置定义了 Actor-Critic 算法在 MindSpore 框架中的具体实现，包括 Actor 和 Learner 的设置、策略和网络的参数，以及训练和评估环境的配置。",
	"article": "本文分享自华为云社区[《MindSpore A2C 强化学习》](https://link.juejin.cn?target=https%3A%2F%2Fbbs.huaweicloud.com%2Fblogs%2F428508%3Futm_source%3Djuejin%26utm_medium%3Dbbs-ex%26utm_campaign%3Dother%26utm_content%3Dcontent \"https://bbs.huaweicloud.com/blogs/428508?utm_source=juejin&utm_medium=bbs-ex&utm_campaign=other&utm_content=content\")，作者：irrational。\n\nAdvantage Actor-Critic (A2C)算法是一个强化学习算法，它结合了策略梯度（Actor）和价值函数（Critic）的方法。A2C算法在许多强化学习任务中表现优越，因为它能够利用价值函数来减少策略梯度的方差，同时直接优化策略。\n\n### A2C算法的核心思想\n\n*   **Actor**：根据当前策略选择动作。\n*   **Critic**：评估一个状态-动作对的值（通常是使用状态值函数或动作值函数）。\n*   **优势函数（Advantage Function）**：用来衡量某个动作相对于平均水平的好坏，通常定义为A(s,a)=Q(s,a)−V(s)。\n\n### A2C算法的伪代码\n\n以下是A2C算法的伪代码：\n\n```perl\nInitialize policy network (actor) π with parameters θ\nInitialize value network (critic) V with parameters w\nInitialize learning rates α_θ for policy network and α_w for value network\n\nfor each episode do\nInitialize state s\nwhile state s is not terminal do\n# Actor: select action a according to the current policy π(a|s; θ)\na = select_action(s, θ)\n\n# Execute action a in the environment, observe reward r and next state s'\nr, s' = environment.step(a)\n\n# Critic: compute the value of the current state V(s; w)\nV_s = V(s, w)\n\n# Critic: compute the value of the next state V(s'; w)\nV_s_prime = V(s', w)\n\n# Compute the TD error (δ)\nδ = r + γ * V_s_prime - V_s\n\n# Critic: update the value network parameters w\nw = w + α_w * δ * ∇_w V(s; w)\n\n# Compute the advantage function A(s, a)\nA = δ\n\n# Actor: update the policy network parameters θ\nθ = θ + α_θ * A * ∇_θ log π(a|s; θ)\n\n# Move to the next state\ns = s'\nend while\nend for\n\n```\n\n### 解释\n\n1.  **初始化**：初始化策略网络（Actor）和价值网络（Critic）的参数，以及它们的学习率。\n2.  **循环每个Episode**：在每个Episode开始时，初始化状态。\n3.  **选择动作**：根据当前策略从Actor中选择动作。\n4.  **执行动作**：在环境中执行动作，并观察奖励和下一个状态。\n5.  **计算状态值**：用Critic评估当前状态和下一个状态的值。\n6.  **计算TD误差**：计算时序差分误差（Temporal Difference Error），它是当前奖励加上下一个状态的折扣值与当前状态值的差。\n7.  **更新Critic**：根据TD误差更新价值网络的参数。\n8.  **计算优势函数**：使用TD误差计算优势函数。\n9.  **更新Actor**：根据优势函数更新策略网络的参数。\n10.  **更新状态**：移动到下一个状态，重复上述步骤，直到Episode结束。\n\n这个伪代码展示了A2C算法的核心步骤，实际实现中可能会有更多细节，如使用折扣因子γ、多个并行环境等。\n\n代码如下：\n\n```ini\nimport argparse\n\nfrom mindspore import context\nfrom mindspore import dtype as mstype\nfrom mindspore.communication import init\n\nfrom mindspore_rl.algorithm.a2c import config\nfrom mindspore_rl.algorithm.a2c.a2c_session import A2CSession\nfrom mindspore_rl.algorithm.a2c.a2c_trainer import A2CTrainer\n\nparser = argparse.ArgumentParser(description=\"MindSpore Reinforcement A2C\")\nparser.add_argument(\"--episode\", type=int, default=10000, help=\"total episode numbers.\")\nparser.add_argument(\n\"--device_target\",\ntype=str,\ndefault=\"CPU\",\nchoices=[\"CPU\", \"GPU\", \"Ascend\", \"Auto\"],\nhelp=\"Choose a devioptions.device_targece to run the ac example(Default: Auto).\",\n)\nparser.add_argument(\n\"--precision_mode\",\ntype=str,\ndefault=\"fp32\",\nchoices=[\"fp32\", \"fp16\"],\nhelp=\"Precision mode\",\n)\nparser.add_argument(\n\"--env_yaml\",\ntype=str,\ndefault=\"../env_yaml/CartPole-v0.yaml\",\nhelp=\"Choose an environment yaml to update the a2c example(Default: CartPole-v0.yaml).\",\n)\nparser.add_argument(\n\"--algo_yaml\",\ntype=str,\ndefault=None,\nhelp=\"Choose an algo yaml to update the a2c example(Default: None).\",\n)\nparser.add_argument(\n\"--enable_distribute\",\ntype=bool,\ndefault=False,\nhelp=\"Train in distribute mode (Default: False).\",\n)\nparser.add_argument(\n\"--worker_num\",\ntype=int,\ndefault=2,\nhelp=\"Worker num (Default: 2).\",\n)\noptions, _ = parser.parse_known_args()\n```\n\n首先初始化参数，然后我这里用cpu运行：options.device\\_targe = “CPU”\n\n```ini\nepisode=options.episode\n\"\"\"Train a2c\"\"\"\nif options.device_target != \"Auto\":\ncontext.set_context(device_target=options.device_target)\nif context.get_context(\"device_target\") in [\"CPU\", \"GPU\"]:\ncontext.set_context(enable_graph_kernel=True)\ncontext.set_context(mode=context.GRAPH_MODE)\ncompute_type = (\nmstype.float32 if options.precision_mode == \"fp32\" else mstype.float16\n)\n    config.algorithm_config[\"policy_and_network\"][\"params\"][\n    \"compute_type\"\n    ] = compute_type\n    if compute_type == mstype.float16 and options.device_target != \"Ascend\":\n    raise ValueError(\"Fp16 mode is supported by Ascend backend.\")\n    is_distribte = options.enable_distribute\n    if is_distribte:\n    init()\n    context.set_context(enable_graph_kernel=False)\n    config.deploy_config[\"worker_num\"] = options.worker_num\n    a2c_session = A2CSession(options.env_yaml, options.algo_yaml, is_distribte)\n```\n\n设置上下文管理器\n\n```python\nimport sys\nimport time\nfrom io import StringIO\n\nclass RealTimeCaptureAndDisplayOutput(object):\ndef __init__(self):\nself._original_stdout = sys.stdout\nself._original_stderr = sys.stderr\nself.captured_output = StringIO()\n\ndef write(self, text):\nself._original_stdout.write(text)  # 实时打印\nself.captured_output.write(text)   # 保存到缓冲区\n\ndef flush(self):\nself._original_stdout.flush()\nself.captured_output.flush()\n\ndef __enter__(self):\nsys.stdout = self\nsys.stderr = self\nreturn self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\nsys.stdout = self._original_stdout\nsys.stderr = self._original_stderr\n\n\nepisode=10\n# dqn_session.run(class_type=DQNTrainer, episode=episode)\nwith RealTimeCaptureAndDisplayOutput() as captured_new:\na2c_session.run(class_type=A2CTrainer, episode=episode)\n\n\nimport re\nimport matplotlib.pyplot as plt\n\n# 原始输出\nraw_output = captured_new.captured_output.getvalue()\n\n# 使用正则表达式从输出中提取loss和rewards\nloss_pattern = r\"loss=(\\d+\\.\\d+)\"\nreward_pattern = r\"running_reward=(\\d+\\.\\d+)\"\nloss_values = [float(match.group(1)) for match in re.finditer(loss_pattern, raw_output)]\nreward_values = [float(match.group(1)) for match in re.finditer(reward_pattern, raw_output)]\n\n# 绘制loss曲线\nplt.plot(loss_values, label='Loss')\nplt.xlabel('Episode')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\nplt.show()\n\n# 绘制reward曲线\nplt.plot(reward_values, label='Rewards')\nplt.xlabel('Episode')\nplt.ylabel('Rewards')\nplt.title('Rewards Curve')\nplt.legend()\nplt.show()\n\n```\n\n展示结果：  \n![image.png](/images/jueJin/171746978705644.png)\n\n![image.png](/images/jueJin/171746977848661.png)\n\n下面我将详细解释你提供的 MindSpore A2C 算法训练配置参数的含义：\n\n### Actor 配置\n\n```python\n    'actor': {\n    'number': 1,\n    'type': mindspore_rl.algorithm.a2c.a2c.A2CActor,\n        'params': {\n        'collect_environment': PyFuncWrapper<\n        (_envs): GymEnvironment<>\n        >,\n        'eval_environment': PyFuncWrapper<\n        (_envs): GymEnvironment<>\n        >,\n        'replay_buffer': None,\n        'a2c_net': ActorCriticNet<\n        (common): Dense<input_channels=4, output_channels=128, has_bias=True>\n        (actor): Dense<input_channels=128, output_channels=2, has_bias=True>\n        (critic): Dense<input_channels=128, output_channels=1, has_bias=True>\n        (relu): LeakyReLU<>\n        >},\n        'policies': [],\n    'networks': ['a2c_net']\n}\n```\n\n*   `number`: Actor 的实例数量，这里设置为1，表示使用一个 Actor 实例。\n*   `type`: Actor 的类型，这里使用 `mindspore_rl.algorithm.a2c.a2c.A2CActor`。\n*   `params`: Actor 的参数配置。\n    *   `collect_environment` 和 `eval_environment`: 使用 `PyFuncWrapper` 包装的 `GymEnvironment`，用于数据收集和评估环境。\n    *   `replay_buffer`: 设置为 `None`，表示不使用经验回放缓冲区。\n    *   `a2c_net`: Actor-Critic 网络，包含一个公共层、一个 Actor 层和一个 Critic 层，以及一个 Leaky ReLU 激活函数。\n*   `policies` 和 `networks`: Actor 关联的策略和网络，这里主要是 `a2c_net`。\n\n### Learner 配置\n\n```ini\n    'learner': {\n    'number': 1,\n    'type': mindspore_rl.algorithm.a2c.a2c.A2CLearner,\n        'params': {\n        'gamma': 0.99,\n        'state_space_dim': 4,\n        'action_space_dim': 2,\n        'a2c_net': ActorCriticNet<\n        (common): Dense<input_channels=4, output_channels=128, has_bias=True>\n        (actor): Dense<input_channels=128, output_channels=2, has_bias=True>\n        (critic): Dense<input_channels=128, output_channels=1, has_bias=True>\n        (relu): LeakyReLU<>\n        >,\n        'a2c_net_train': TrainOneStepCell<\n        (network): Loss<\n        (a2c_net): ActorCriticNet<\n        (common): Dense<input_channels=4, output_channels=128, has_bias=True>\n        (actor): Dense<input_channels=128, output_channels=2, has_bias=True>\n        (critic): Dense<input_channels=128, output_channels=1, has_bias=True>\n        (relu): LeakyReLU<>\n        >\n        (smoothl1_loss): SmoothL1Loss<>\n        >\n        (optimizer): Adam<>\n        (grad_reducer): Identity<>\n        >\n        },\n    'networks': ['a2c_net_train', 'a2c_net']\n}\n```\n\n*   `number`: Learner 的实例数量，这里设置为1，表示使用一个 Learner 实例。\n*   `type`: Learner 的类型，这里使用 `mindspore_rl.algorithm.a2c.a2c.A2CLearner`。\n*   `params`: Learner 的参数配置。\n    *   `gamma`: 折扣因子，用于未来奖励的折扣计算。\n    *   `state_space_dim`: 状态空间的维度，这里为4。\n    *   `action_space_dim`: 动作空间的维度，这里为2。\n    *   `a2c_net`: Actor-Critic 网络定义，与 Actor 中相同。\n    *   `a2c_net_train`: 用于训练的网络，包含损失函数（SmoothL1Loss）、优化器（Adam）和梯度缩减器（Identity）。\n*   `networks`: Learner 关联的网络，包括 `a2c_net_train` 和 `a2c_net`。\n\n### Policy and Network 配置\n\n```go\n    'policy_and_network': {\n    'type': mindspore_rl.algorithm.a2c.a2c.A2CPolicyAndNetwork,\n        'params': {\n        'lr': 0.01,\n        'state_space_dim': 4,\n        'action_space_dim': 2,\n        'hidden_size': 128,\n        'gamma': 0.99,\n        'compute_type': mindspore.float32,\n            'environment_config': {\n            'id': 'CartPole-v0',\n            'entry_point': 'gym.envs.classic_control:CartPoleEnv',\n            'reward_threshold': 195.0,\n            'nondeterministic': False,\n            'max_episode_steps': 200,\n            '_kwargs': {},\n            '_env_name': 'CartPole'\n        }\n    }\n}\n```\n\n*   `type`: 策略和网络的类型，这里使用 `mindspore_rl.algorithm.a2c.a2c.A2CPolicyAndNetwork`。\n*   `params`: 策略和网络的参数配置。\n    *   `lr`: 学习率，这里为0.01。\n    *   `state_space_dim` 和 `action_space_dim`: 状态和动作空间的维度。\n    *   `hidden_size`: 隐藏层的大小，这里为128。\n    *   `gamma`: 折扣因子。\n    *   `compute_type`: 计算类型，这里为 `mindspore.float32`。\n    *   `environment_config`: 环境配置，包括环境 ID、入口、奖励阈值、最大步数等。\n\n### Collect Environment 配置\n\n```arduino\n    'collect_environment': {\n    'number': 1,\n    'type': mindspore_rl.environment.gym_environment.GymEnvironment,\n    'wrappers': [mindspore_rl.environment.pyfunc_wrapper.PyFuncWrapper],\n        'params': {\n            'GymEnvironment': {\n            'name': 'CartPole-v0',\n            'seed': 42\n            },\n            'name': 'CartPole-v0'\n        }\n    }\n```\n\n*   `number`: 环境实例数量，这里为1。\n*   `type`: 环境的类型，这里使用 `mindspore_rl.environment.gym_environment.GymEnvironment`。\n*   `wrappers`: 环境使用的包装器，这里是 `PyFuncWrapper`。\n*   `params`: 环境的参数配置，包括环境名称 `CartPole-v0` 和随机种子 `42`。\n\n### Eval Environment 配置\n\n```arduino\n    'eval_environment': {\n    'number': 1,\n    'type': mindspore_rl.environment.gym_environment.GymEnvironment,\n    'wrappers': [mindspore_rl.environment.pyfunc_wrapper.PyFuncWrapper],\n        'params': {\n            'GymEnvironment': {\n            'name': 'CartPole-v0',\n            'seed': 42\n            },\n            'name': 'CartPole-v0'\n        }\n    }\n```\n\n*   配置与 `collect_environment` 类似，用于评估模型性能。\n\n总结一下，这些配置定义了 Actor-Critic 算法在 MindSpore 框架中的具体实现，包括 Actor 和 Learner 的设置、策略和网络的参数，以及训练和评估环境的配置。这个还是比较基础的。\n\n![cke_66080.png](/images/jueJin/285008600051998.png)\n\n### HDC 2024，6月21日-23日，东莞松山湖，期待与您相见！\n\n**更多详情请关注官网：**\n\n中文：[developer.huawei.com/home/hdc](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.huawei.com%2Fhome%2Fhdc \"https://developer.huawei.com/home/hdc\")\n\n英文：[developer.huawei.com/home/en/hdc](https://link.juejin.cn?target=https%3A%2F%2Fdeveloper.huawei.com%2Fhome%2Fen%2Fhdc \"https://developer.huawei.com/home/en/hdc\")\n\n[**点击关注，第一时间了解华为云新鲜技术~**](https://link.juejin.cn?target=https%3A%2F%2Fbbs.huaweicloud.com%2Fblogs%3Futm_source%3Djuejin%26utm_medium%3Dbbs-ex%26utm_campaign%3Dother%26utm_content%3Dcontent \"https://bbs.huaweicloud.com/blogs?utm_source=juejin&utm_medium=bbs-ex&utm_campaign=other&utm_content=content\")",
	"selfDefined": "likes:1,comments:0,collects:0,likes:317"
}