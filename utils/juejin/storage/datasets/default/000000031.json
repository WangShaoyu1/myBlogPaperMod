{
	"title": "解决目标网站封爬虫的3步方法",
	"author": "小白学大数据",
	"publishTime": "2022-05-13",
	"readTime": "阅读1分钟",
	"tags": "[\"算法\"]",
	"description": "作为爬虫工作者，在我们日常获取数据的过程中难免发生ip被封和403错误等等，这都是网站检测出你是爬虫而进行反爬措施，所以今天在这里为大家总结一下怎么用IP代理防止被封的方法。1、设置等待时间，种是显",
	"article": "作为爬虫工作者，在我们日常获取数据的过程中难免发生ip被封和403错误等等，这都是网站检测出你是爬虫而进行反爬措施，所以今天在这里为大家总结一下怎么用IP代理防止被封的方法。\n\n1、设置等待时间，种是显性等待时间（强制停几秒），一种是隐性等待时间。\n\n2、修改请求头：识别你是机器人还是人类浏览器浏览的重要依据就是User-Agent。\n\n3、采用代理ip/建代理ip池，直接看代码。利用动态ip代理，可以强有力地保障爬虫不会被封，能够正常运行。以下为使用代理ip的实际示例，在使用IP量上需要看自己的需求，大型项目是必须用大量ip的。\n\nini\n\n 代码解读\n\n复制代码\n\n`#! -*- encoding:utf-8 -*-     import requests     import random     # 要访问的目标页面     targetUrl = \"http://httpbin.org/ip\"     # 要访问的目标HTTPS页面     # targetUrl = \"https://httpbin.org/ip\"     # 代理服务器(产品官网 www.16yun.cn)     proxyHost = \"t.16yun.cn\"     proxyPort = \"31111\"     # 代理验证信息     proxyUser = \"username\"     proxyPass = \"password\"     proxyMeta = \"http://%(user)s:%(pass)s@%(host)s:%(port)s\" % {         \"host\" : proxyHost,         \"port\" : proxyPort,         \"user\" : proxyUser,         \"pass\" : proxyPass,     }     # 设置 http和https访问都是用HTTP代理     proxies = {         \"http\"  : proxyMeta,         \"https\" : proxyMeta,     }     #  设置IP切换头     tunnel = random.randint(1,10000)     headers = {\"Proxy-Tunnel\": str(tunnel)}     resp = requests.get(targetUrl, proxies=proxies, headers=headers)     print resp.status_code     print resp.text`\n\n只有我们在采集数据过程中做好以上3个步骤，大致爬虫的运行就不成问题了。但是这些都是基本的反爬措施，现在有很多大型网站做设置的反爬机制更严，对技术要求更高，所以需要我们随时学习更深的爬虫技术。"
}