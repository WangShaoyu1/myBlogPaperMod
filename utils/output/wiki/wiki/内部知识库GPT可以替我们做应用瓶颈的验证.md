---
author: "王宇"
title: "内部知识库GPT可以替我们做应用瓶颈的验证"
date: 七月18,2023
description: "唐玮"
tags: ["唐玮"]
ShowReadingTime: "12s"
weight: 322
---
企业内部智能知识库应用开发项目 可以替我们验证与收集目前大模型应用在虚拟人身上的**瓶颈**有哪些。识别瓶颈后，即可以更客观的探讨技术方案。
----------------------------------------------------------------------

#### 一、我们的客户是谁？我们解决他们的什么问题？

1.  按当前的业务：我们的第一个客户是万得厨，我们辅助万得厨，完成万得厨“人人都有专享厨师”的使命。

1.  万得厨宣传场景。虚拟人与客户沟通，宣传万得厨，或许可担任一些品牌宣传等工作
2.  万得厨烹饪场景。推荐食谱、指导食材处理、推荐烹饪方案等

#### 二、我们现在及未来的业务有哪些？

1.  当前辅助万得厨。
2.  未来要让人能够把自己的技能赋予给虚拟人，然后让虚拟人服务于他人。

#### 三、大模型能够给我们的业务带来什么？（换句话：大模型在哪些方面，相比我们现有能力能够更好的解决问题？）

1.  辅助食材处理。因现在还没有此类可以直接调用的技能。
2.  FAQ工作量减少。通过向量数据库等技术可以实现接入万得厨APP的食谱数据库，或者其他的营养数据库，实现本地知识库的对话。这里不需要去编写大量的相似问，因大模型本身即具备一定的推理能力。大模型依据的是推理而不是相似度，所以不需要写相似问，相比工作量及工作时间就减少。
3.  用户个性化。通过在prompt中插入食客信息，提供食客定制化的语音风格、个性推荐、历史记忆对话（例如：虚拟男/女友聊天软件replike，通过增加历史记忆的功能，让大模型能够基于这些历史记忆聊天）
4.  多轮对话。大模型具备多轮对话能力。
5.  让生产型虚拟人能够更简单的获取能力。例如相比用FAQ库写相似问的办法，大模型大大减轻工作量，减少了时间。

#### 四、大模型要能够让我们部署到产品中至少得达成什么条件？

1.  （重要）本地/云端计算资源。资源的成本、效率需要综合考虑

1.  微调训练需要耗费GPU资源
2.  用户调用需要消耗资源

3.  （重要）大模型从输入到输出的整体运算速度。以万得厨为例，假设大模型部署于云端，因用户是和虚拟人交互，所以从输入到输出，可能必须得控制在1S、0.5S以内，甚至更短的时间。否则用户体验会很差。

1.  需要评估大模型自身的推理速度和推理需要耗费的资源
2.  需要评估使用向量数据库时的计算效率
3.  云端传输的时间效率

5.  大模型接入PTA、FTT平台可能需进行沟通与工作。大模型需要和已有平台能力进行融合
6.  token数量。即能够输入的prompt长度。主要用于知识拼接。目前以chatGLM能够支持最长长度32K个字来看是能够满足当前烹饪场景的。

#### 五、希望在搭建企业内部智能知识库GPT应用时，可以附带一些调研、测试、数据收集与验证工作：

希望能够在开发的时候埋好数据点，做一些数据的收集，最好能够生成一些报告。

1.  **资源耗费**

1.  监测与收集每次用户调用时需要耗费的资源数量。
2.  多用户调用时需要耗费的资源数量与服务器压力。
3.  微调资源耗费、时间耗费。
4.  微调模型时测试和生产环境如何稳定切换的方案

3.  **运算速度**

1.  大模型推理速度、耗费的资源
2.  使用向量数据库计算相似度时的速度、资源耗费
3.  云端传输的时间效率

5.  **推理能力**

1.  需要评估推理能力是否满足虚拟人场景的推理需求。

7.  **对接API的能力（即GPT可以直接操控炉端或其他能力的能力）**

  

[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)

[Configure buttons visibility](/users/tfac-settings.action)