---
author: "王宇"
title: "RAG技术实践及应用中的探索与思考"
date: 六月27,2024
description: "知识插件"
tags: ["知识插件"]
ShowReadingTime: "12s"
weight: 702
---
*   1 [前言](#RAG技术实践及应用中的探索与思考-前言)
*   2[一、RAG 技术介绍](#RAG技术实践及应用中的探索与思考-一、RAG技术介绍)
    *   2.1[1. 概述](#RAG技术实践及应用中的探索与思考-1.概述)
    *   2.2[2\. 知识库的构建](#RAG技术实践及应用中的探索与思考-2.知识库的构建)
        *   2.2.1[2.1 文档拆分与处理](#RAG技术实践及应用中的探索与思考-2.1文档拆分与处理)
        *   2.2.2[2.2 知识向量化入库](#RAG技术实践及应用中的探索与思考-2.2知识向量化入库)
    *   2.3[3\. 检索召回](#RAG技术实践及应用中的探索与思考-3.检索召回)
        *   2.3.1[3.1 关键词检索](#RAG技术实践及应用中的探索与思考-3.1关键词检索)
        *   2.3.2[3.2 向量检索](#RAG技术实践及应用中的探索与思考-3.2向量检索)
        *   2.3.3[3.3 召回重排序](#RAG技术实践及应用中的探索与思考-3.3召回重排序)
    *   2.4[4\. 第三方平台能力概要](#RAG技术实践及应用中的探索与思考-4.第三方平台能力概要)
*   3[二、RAG 技术实践](#RAG技术实践及应用中的探索与思考-二、RAG技术实践)
    *   3.1[1. 短文本类问答实践](#RAG技术实践及应用中的探索与思考-1.短文本类问答实践)
        *   3.1.1[1.1 知识库的构建](#RAG技术实践及应用中的探索与思考-1.1知识库的构建)
        *   3.1.2[1.2 检索应用](#RAG技术实践及应用中的探索与思考-1.2检索应用)
        *   3.1.3[1.3 思考总结](#RAG技术实践及应用中的探索与思考-1.3思考总结)
    *   3.2[2\. 长文档类问答实践](#RAG技术实践及应用中的探索与思考-2.长文档类问答实践)
        *   3.2.1[2.1 知识库的构建](#RAG技术实践及应用中的探索与思考-2.1知识库的构建)
        *   3.2.2[2.2 探索总结](#RAG技术实践及应用中的探索与思考-2.2探索总结)
    *   3.3[3\. 表格类问答实践](#RAG技术实践及应用中的探索与思考-3.表格类问答实践)
        *   3.3.1[3.1 知识库的构建](#RAG技术实践及应用中的探索与思考-3.1知识库的构建)
        *   3.3.2[3.2 探索总结](#RAG技术实践及应用中的探索与思考-3.2探索总结)
*   4[三、思考与总结](#RAG技术实践及应用中的探索与思考-三、思考与总结)

前言
===================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

传统自然语言处理（NLP）算法和大规模语言模型（Large Language Models, LLMs）是自然语言处理领域的两大技术分支，各有其特点和适用场景。下面是两者优缺点的对比：

对比项

传统自然语言算法

大语言模型

对比项

传统自然语言算法

大语言模型

**优势**

*   **可解释性强：**传统 NLP 算法往往基于明确的规则或特征工程，其工作原理直观且可解释性强。
    
*   **资源需求较低****：**相比大型语言模型，传统算法在训练和部署时对计算资源的需求较低，更适合资源受限的环境。
    
*   **针对性强****：**针对特定任务设计的算法往往在该任务上表现良好，如命名实体识别、关键词提取等，具有高度定制化的解决方案。
    
*   **稳定性较好****：**因为依赖于明确的规则，传统算法的输出相对稳定，对于输入变化的敏感性较低。
    

*   **自适应性强****：**通过自监督从大量文本中学习，无需人工设计特征，能自动捕捉语言的复杂结构和语境。
    
*   **泛化能力好****：**LLMs 在训练过程中接触到广泛的语言现象，能较好地泛化到未见过的任务和数据上，表现出较强的生成和理解能力。
    
*   **上下文理解强****：**具备长序列建模能力，能更好地理解文本的前后文关系，生成连贯、有逻辑的文本。
    
*   **多功能性好****：**通过微调，同一个基础模型可以在多种 NLP 任务上达到优秀水平，减少模型开发的成本。
    

**劣势**

*   **泛化能力差****：**面对未见过的文本或语言变化，传统算法可能无法很好地处理，需要不断调整和优化。
    
*   **特征工程耗时****：**手工设计特征和规则耗时费力，难以覆盖语言的复杂性和多样性。
    
*   **缺乏上下文理解****：**除了一些复杂的模型，传统算法往往缺乏对长距离依赖和上下文的理解能力。
    
*   ****局限性明显**：**在处理文本生成、上下文理解、复杂推理等高级语言任务时，传统算法往往力不从心。
    

*   **资源消耗巨大****：**训练大型语言模型需要大量的计算资源和数据，成本高昂。
    
*   **不透明性****：**模型内部决策过程复杂，难以解释。
    
*   **数据偏见**和伦理问题******：**模型可能受到训练数据中偏见和错误的影响，产生不恰当或有害的输出，带来伦理和法律风险。
    
*   **稳定性问题****：**对于某些输入，LLMs 可能产生不稳定或不一致的结果，特别是在边缘案例或对抗性攻击下。
    

一、RAG 技术介绍
==========

1. 概述
-----

所谓 RAG（Retrieval Augmented Generation, 检索增强生成）技术，指的是**大模型（LLM）+ 知识召回（Knowledge Retrieval）**的问答实现路线。其基本思路是把私域知识文档进行切片，然后向量化导入知识库，再通过检索召回与用户问题相关的知识片段，最终作为上下文输入给大语言模型进行归纳总结。这种在提示词中引入领域知识的方式较好地利用了大模型的指令遵循和推理能力，能整合相关信息直接给出答案，而不是单纯的给出相关知识或链接。RAG 方法使得开发者不必为每一个特定的任务重新训练整个大模型，只需要外挂上不同的知识库，即可为模型提供额外的信息输入，提高其回答的准确性和专业性。

![](/download/attachments/129174321/image2024-6-25_10-47-34.png?version=1&modificationDate=1719283654580&api=v2)

在 LLM 已经具备了较强能力的基础上，仍然需要 RAG ，主要有以下几点原因：

*   **幻觉问题：**LLM 文本生成的底层原理是基于概率的，因此会不可避免地产生“一本正经的胡说八道”的情况。通过 RAG 技术，大模型在回答问题时可以参考相关知识片段进行推断和回答，减少产生幻觉的可能性。
*   **时效性问题：**LLM 的规模越大，大模型训练的成本越高，周期也就越长。那么具有时效性的数据也就无法参与训练，所以也就无法直接回答时效性相关的问题，例如“帮我推荐几部热映的电影？”。通过 RAG 技术，可以快速补充这些知识，从而确保回答的准确性和时效性。 
*   **数据安全问题：**通用的 LLM 没有企业内部数据和用户数据，那么企业想要在保证安全的前提下使用 LLM，最好的方式就是把数据全部放在本地，企业数据的业务计算全部在本地完成，而在线的大模型仅仅完成一个归纳总结的功能。

2\. 知识库的构建
----------

### 2.1 文档拆分与处理

知识向量化的前置步骤是进行知识的拆分，**语义完整性的保持是最重要的考量。由于拆分的片段后续需要通过向量化模型进行推理，所以必须考虑向量化模型的最大长度限制，超出这个限制会出现截断，导致语义不完整。**

目前使用较多的基础方式是采用 Langchain 中的 RecursiveCharacterTextSplitter，属于是 Langchain 的默认拆分器。它采用多级分隔字符列表 \[“\\n\\n”， “\\n”， ” “， “”\] 来进行拆分，默认先按照段落做拆分，如果拆分结果的 chunk\_size （文档块大小，其字符数受限于向量化模型）超出，再继续利用下一级分隔字符继续拆分，直到满足 chunk\_size 的要求。

但这种做法相对来说还是比较粗糙，可能会造成一些关键内容被拆开。对于一些特定的文档格式可以有一些更细致的做法。

### 2.2 知识向量化入库

切分后的文档知识需要向量化后再导入知识库，这对于后续的知识检索召回起关键作用。知识的向量化需要借助深度模型的语义化能力，不仅仅需要基于场景数据来评估选用合适的向量模型，确定向量维度，还存在向量模型微调、部署&推理等额外步骤。

真实的业务场景中，文档的规模在百到百万这个数量级之间。按照冗余的多级召回方式，对应的知识条目最高可能达到亿的规模。由于整个离线计算的规模很大，所以必须并发进行，否则无法满足知识新增和向量检索效果迭代的要求。

3\. 检索召回
--------

知识召回在基于大语言模型的知识问答中是非常关键的步骤，它决定了大语言模型的输入，对后续回答的可靠性以及回复质量影响非常大。目前基于语义向量召回是用的比较多的方式，但在实际的生产实践中，关键词召回方式也非常实用，它具备精确匹配、索引效率和可解释的优势。

### 3.1 关键词检索

这种召回策略在很多场景下非常有效，特别是一些对领域专词非常敏感的场景。但它的缺点在于对语义信息的捕捉能力较弱，用户的输入更多表现为完整的句子而不是搜索关键词，所以一般把它作为向量召回的一种必要补充。同时，现实中多词一义是非常普遍的，除了向量检索能改善这个问题以外，也可以通过构建同义词表去匹配用户输入的关键词。

### 3.2 向量检索

*   **对称召回：**一般是指通过两个**同类句子**的相似性来召回，比如输入的 query 匹配相似的另一个 question(query->question)，也可以是输入的陈述句匹配相似的另一个陈述句。
*   **非对称召回：**一般是检索数据， 通常是一个**相对短的问句和一个相对长的答案**（一般为一个文本段落）, 一些 Question Answering 类的任务都是基于这种数据集。

一般来说，对称召回任务比非对称召回任务要简单，特别在很多垂直领域，向量模型对领域知识的理解有限，对称召回很多时候理解了字面意思就能满足要求。

在一些专业领域，通用的向量模型可能无法很好的理解一些专有词汇，导致检索性能不足,  且正例和负例相似度的值域有比较多的重叠，无法通过阈值判断该召回内容是否属于相关信息。过多的无关信息可能会在下游误导大语言模型，所以需要使用领域数据对向量模型进行微调，微调的主要目标一方面是为了提高 topk 召回的准确率，另外一方面是想要拉开正例和负例的相似度分值分布，从而得到更好的分隔阈值。

### 3.3 召回重排序

在知识混合检索场景中，如何结合关键词和向量召回的结果是一个问题，因为两者不在一个评分体系下。

*   在基于大语言模型的知识问答场景下， 可以简单采用关键词召回和向量召回各取 topk 然后**去重取并集**的方式。
*   可以借鉴 ElasticSearch 混合检索（KNN+BM25）分值（Score）的计算方式：**knn\_score** **\*** **boost** **\+ bm25\_score **\* （1-boost）****。****
*   另外一个思路是再训练一个**排序模型**的来打分，这种方式可以更好的结合关键词和向量召回的结果。

4\. 第三方平台能力概要
-------------

当前大模型厂商（百度、讯飞等）其实也都提供了对应的 RAG 服务构建流程，包括文件上传、文档检索和问答一整套流程，但他们提供的能力比较通用且局限性较高（比如：文档分块方式固定、向量化模型和检索策略固定，检出率也不方便优化，通常与自有大模型绑定提供服务），无法满足企业个性化定制的需求。

二、RAG 技术实践
==========

1. 短文本类问答实践
-----------

### 1.1 知识库的构建

之前做的“春节活动”和“扬翔企业知识”问答服务，都是把人工处理好的短文本（最长723个字符）作为基础数据，其处理方式相对比较简单。向量化模型采用开源预训练的模型就能达到较好的效果。

![](/download/attachments/129174321/image2024-6-27_9-42-17.png?version=1&modificationDate=1719452537773&api=v2)

### 1.2 检索应用

使用120条用例测试发现，纯向量检索的模型问答效果不太好，具体测试结果如下：

![](/download/attachments/129174321/image2024-2-18_18-46-35.png?version=1&modificationDate=1719400202762&api=v2)

通过引入关键词召回，并调整相关权重和阈值，再次使用“春节活动”测试用例测试后，模型问答效果有较大提升：

![](/download/attachments/129174321/image2024-2-18_18-59-10.png?version=1&modificationDate=1719400202817&api=v2)

由此可见，检索方案的确定并非一蹴而就的，需要基于具体的场景数据来测评。首先要初选向量化模型，敲定检索方式后，还需要多次测试来确定具体的权重和阈值。

### 1.3 思考总结

实践中采用的是多次测试调整后的提示词和讯飞星火认知大模型完成的知识问答，从测试结果来看，回答错误的用例中，有部分检索到了对应的知识，但大模型回答错误，可能与检索知识的顺序和无关内容的影响有关，由此可见检索效果对问答结果影响较大，但大模型本身的能力也很重要。所以后续大模型的选择、提示词的确认都需要基于具体的场景需求和数据来测评抉择。

2\. 长文档类问答实践
------------

### 2.1 知识库的构建

目前在做的“养猪专家”和“营养健康助手”问答服务，都使用了大量的教材类长文档数据（达千万级字数），数据的获取和处理非常的复杂耗时。

*   **文档拆分：**需要分多级（段落、各章节）进行拆分。
*   **数据增强：**文本拆分后可能会失去一部分关键的上下文信息，需要使用标题做增强。
*   **文本摘要：**按段落和章节的拆分方式，会出现文本长度超出限制的情况，需要使用大模型做摘要处理。大模型摘要受到上下文长度和通用能力的限制，其效果较难保证。对于长度较短的文本，可直接做摘要；但是长文本则有很多做摘要的技巧，比如先重叠拆分，对每一块都做摘要后再汇总（map\_reduce）做出最终的摘要；或按顺序依次对每一块做摘要，并把前一次摘要的结果并入下一次做摘要的内容中，把最后一次摘要作为长文本的最终摘要（refine）。其实，因为摘要输出长度限制，长文本摘要会失去很多关键信息，导致摘要效果不佳，故后面探索出了新的路子，让大模型做**分知识点摘要**，最终将生成的每个知识点拆分后再入库，可达到较好的效果。因为引入了标题增强，所以存在许多章节文本既有父标题，又有子标题，做摘要的同时还需要提取标题之间的关系，这个难度相对较高。
*   **数据去重：**多级拆分带来知识冗余，需要进行去重处理（simhash、minhash等）。

### 2.2 探索总结

对于长文本知识库的构建，尝试了许多方法优化文本拆分和摘要的效果。对于文本拆分，直接按长度重叠拆分是最简单的实现方法，但拆分最重要的考虑是**语义完整性**，所以最开始尝试了 NLTK, Spacy、bert 系列（语义拆分或下一句判断），效果都不好。还不如按段落拆分，但对文本质量要求较高，且也会存在语义相关的内容被拆分的情况，所以需要多级冗余拆分，才能达到一个相对较好的效果。而摘要主要考虑的是分块长度限制，也试过传统的 nlp 算法（比如 bert、bart、mt5、pegasus、textrank），更适用于关键词/句提取，标题生成，但摘要效果远不如大模型。

![](/download/attachments/129174321/image2024-6-27_15-28-22.png?version=1&modificationDate=1719473302701&api=v2)

当然大模型摘要也存在许多问题，并不是一蹴而就或一个模型就能解决所有问题的，只能是在现有能力的情况下选择最优方案而已。具体测评情况总结如下：

![](/download/attachments/129174321/image2024-6-27_15-5-38.png?version=1&modificationDate=1719471938079&api=v2)

3\. 表格类问答实践
-----------

### 3.1 知识库的构建

表格知识库构建的目的是将结构化的表格内容转化为文本描述，方便检索对应的知识供大模型参考。其对格式要求是最高的，光是提取表的标题、备注、表头和数据都需要耗费大量的人力。

![](/download/attachments/129174321/image2024-6-27_11-38-2.png?version=1&modificationDate=1719459482297&api=v2)

*   **表格拆分：**需要根据字符数限制按行、列拆分为多个子表。
*   **内容增强：**表格数据一般应用于查表问答或提供计算参数，但是单独的表格数据缺少相关描述信息，导致检索难度非常大，需要使用大模型生成一些拓展信息。
*   **基础数据统计：**由于最终知识库中展示的每一块内容都是子表的信息，缺少全局全表的信息，有时候会误导大模型，故需要提供全表的一个基础数据统计（比如：最大/小值、均值、高频数等），这样也能应对一部分全表数据问答的情况。

### 3.2 探索总结

最开始是想要在文档中结合上下文知识将表格内容一起给大模型做摘要，但这个表格上下文的提取难度很高，而且不适用于表格内容较多的情况，故最后直接将表格数据单独摘了出来处理。目前的处理方式其实还存在许多问题，比如表格数据的排版并非自然语言描述，故向量检索不太适用，更多的是依靠关键词检索。若想要提升向量检索的效果，则需要将表格中的每一条数据都转化为自然语言描述，但这个没有万能的模板，用大模型做的效果也并不好，还是需要人工去针对具体的表格确定转化模板，才能达到较好的效果。

三、思考与总结
=======

RAG 大模型问答应用构建的过程不只是数据的获取和处理耗时耗力，还有检索、提示词和模型参数的调整，这涉及检出率的统计和问答效果评估分析等一系列繁琐的工作。

![](/download/thumbnails/129174321/image2024-6-27_14-16-30.png?version=1&modificationDate=1719468990735&api=v2)

从目前做的工作来看，RAG 技术能在一定程度上提升通用大模型问答的正确率和质量（相关性、完整度），当然也面临通用大模型能力不断迭代升级的影响，要想构建出领域较优的问答服务，还有许多更细致深入的工作要做。

![](/download/attachments/129174321/image2024-6-27_14-48-46.png?version=1&modificationDate=1719470926948&api=v2)

  

[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)

[Configure buttons visibility](/users/tfac-settings.action)