---
author: "掘金酱"
title: "掘力计划第23期 - 开源中文大模型Linly LLaMA-2带来人工智能新技术新应用"
date: 2023-09-15
description: "9月9日，掘力计划第 23 期线上技术沙龙邀请到吴碧珠博士做客，为大家带来题为《Linly Chinese LLaMA-2中文开源大模型方案分享》的演讲，吴博士是宁波诺丁汉大学和深圳大学联合培养的在读"
tags: ["人工智能中文技术社区","前端开发社区","前端技术交流","前端框架教程","JavaScript 学习资源","CSS 技巧与最佳实践","HTML5 最新动态","前端工程师职业发展","开源前端项目","前端技术趋势"]
ShowReadingTime: "阅读7分钟"
weight: 1
selfDefined:"likes:0,comments:0,collects:0,views:374,"
---
![](/images/jueJin/519ad5e7e218498.png)

9月9日，掘力计划第 23 期线上技术沙龙邀请到吴碧珠博士做客，为大家带来题为《Linly Chinese LLaMA-2中文开源大模型方案分享》的演讲，吴博士是宁波诺丁汉大学和深圳大学联合培养的在读博士生，主要从事自监督学习、基于人体骨架点视频序列的动作识别和大语言模型方面的研究，师从深圳大学计算机学院沈琳琳教授。本次分享吴博士详细介绍了沈琳琳教授团队自主研发的Linly系列中文开源大模型的技术方案，包括项目背景、模型架构、训练数据以及模型应用等方面。

直播回放地址：[juejin.cn/live/jpower…](https://juejin.cn/live/jpowermeetup23 "https://juejin.cn/live/jpowermeetup23")

近年来，以 ChatGPT 为代表的大规模预训练语言模型获得了广泛的关注。这类模型依赖大量文本训练，可以完成阅读理解、文本生成等多种自然语言处理任务。但目前许多流行的大规模语言模型，例如 LLaMA、Falcon 等，其训练数据以英文为主，直接应用于中文场景的效果并不理想。为此，深圳大学沈琳琳教授团队发起了 Linly 项目，目标是基于英文底座模型，通过大规模中英文增量训练对齐模型的跨语言表示，将语言能力迁移到中文上来获得性能强大的中文预训练模型。

Linly 项目基于 TencentPretrain 框架完成模型训练，该框架模块化实现了主流模型架构，支持快速迭代不同规模的模型。目前 Linly 已在 GitHub 开源了不同模型架构、不同参数量级、不同序列长度的中文大模型，代码库的周访问量达万级，stars 近 3000，影响力日益扩大。这些模型直接在主流大模型基础之上，扩充了中文词汇表，并增加中文训练数据来进行中文化适配。相比起直接将国外主流大模型的英文回答结果翻译成中文的方式，Linly 模型更符合中文语义表达习惯，同时也保留了原大模型强大的英文问答能力。模型支持多种量化精度，可在不同硬件设备上部署。

### 一、Linly项目亮点

![](/images/jueJin/810ada0351874fe.png)

作为开源可商用的中文大模型项目，Linly 有以下突出优点:

1.  **完全** **开源**：模型、代码、数据集、训练方案均公开，可供自由使用及再现。
2.  **支持多种推理硬件**：提供了 CPU、GPU 等部署版本，降低应用门槛。
3.  **兼容主流模型应用生态**：可无缝应用于 LLaMA 等模型的下游任务场景。
4.  **性能强劲**：在匿名大模型对战平台的综合测评结果中，Linly 名列前茅。

### 二、大语言模型基础知识

![](/images/jueJin/ea57889389714b8.png)

相比于传统的监督学习方法，大模型训练采用的是预训练-微调的范式。先利用大规模无标注语料进行无监督预训练，得到通用语言理解能力，然后使用少量标注数据进行微调以适应不同的具体下游任务。这种方法可以减少人工标注数据的工作量。

常见的无监督预训练模型主要有掩码语言模型（代表作：Bert），单向语言模型（代表作：GPT系列）和去噪语言模型（代表作：BART）。目前大部分大模型都采用单向语言模型的预训练方式，其预训练目标是续写，即根据前文生成后续文本，也称单向语言模型。预训练阶段，模型根据前面的信息，预测下一个 token 最有可能是词汇表中的哪一个，用分类任务中常用的交叉熵损失函数来约束模型的训练，从而获得语言生成能力。微调阶段，使用少量标注数据使模型适应特定任务，该阶段的训练方式和前一阶段一样，也做续写任务的训练，不过只计算完成任务部分 token 的损失值。

### 三、Linly方案设计

![](/images/jueJin/b0741ccfa2e848f.png)

Linly 项目使用自主研发的 TencentPretrain 框架实现模型训练，该框架通过模块化设计提取了不同模型的共性部分，可以轻松实现新模型以及训练算法的构建。

1.  模型选型

鉴于 GPT 类模型在大模型中广泛应用，Linly 项目选择在 LLaMA-2 的基础上进行中文化改造。LLaMA-2 作为通用语言模型，中文化后可在中文任务上达到不错性能。

2.  数据构建

由于 LLaMA-2 原词汇表仅包含 700 多个中文汉字，直接在此基础上用中文数据训练的效率不高。Linly 项目团队进一步扩充中文词汇表，多支持了 8000 多个汉字、中文符号和 20000 多个词组，以提高在中文数据上训练的效率及性能。

训练数据包含中文语料、英文语料、中文指令数据、英文指令数据和英文翻译成中文的平行语料数据。采用课程学习的策略，即在训练过程中逐步减少英文数据所占的比例，增加中文数据所占的比例，实现在不降低英文能力的前提下，同时将英文能力丝滑迁移至中文数据上的效果。

3.  模型训练

训练 Linly 大模型时，针对大模型最长输入文本长度的限制，采用了 full-sentence 的技巧，即当句子长度不足最长输入文本长度时，在接上分隔符并接上新文本时，选择长度加和尽可能接近最长输入文本长度的文本。这样可以提升训练的效率，同时也尽量避免了由于新接入句子长度过长而不得不将其剩下部分当成新句子的“截断”现象。模型并行训练方面，使用 DeepSpeed 实现数据、张量和流水线三个维度的并行，加速收敛。

### 四、Linly应用

![](/images/jueJin/aeb47f79d5af463.png)

在大模型的应用方面，Linly 也进行了大量探索。基于Linly大模型微调的中文金融知识问答大模型“聚宝盆”就取得了不错的效果。Linly 数字人结合最新语音驱动人类生成的工作，达到将与大模型交互过程可视化的效果。Hugging Face 平台也提供了Linly模型的在线体验。可见 Linly 模型具备与国际先进水平接轨的语言理解与生成能力，在中文场景具有广阔的应用前景。在未来的工作中，Linly项目成员还将探索大模型在AI智能体、具身智能和虚拟人等场景的应用。

本次分享内容丰富，从多个维度全面介绍了 Linly 项目的技术方案及应用情况，让听众对这一开源中文大模型有了更为清晰和立体的理解。演讲中大量案例让观众直观感受大模型带来的人工智能新技术新应用，也让业界看到了中文大模型弥补国际开源模型中文能力不足的可能性。可以预见，在强大的学术团队和开源社区的支持下，Linly 系列模型会不断壮大，在更多领域产生重要影响，推动我国自主可控人工智能技术向纵深发展。

**掘力计划**

掘力计划由稀土掘金技术社区发起，致力于打造一个高品质的技术分享和交流的系列品牌。聚集国内外顶尖的技术专家、开发者和实践者，通过线下沙龙、闭门会、公开课等多种形式分享最前沿的技术动态。