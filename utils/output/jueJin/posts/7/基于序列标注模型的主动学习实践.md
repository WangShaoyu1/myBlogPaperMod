---
author: "字节跳动技术团队"
title: "基于序列标注模型的主动学习实践"
date: 2022-12-07
description: "主动学习是通过模型自己选择对模型贡献最大的样本作为训练语料，以达到有效地降低标注成本的同时迅速提高模型的识别能力。本文介绍几种基于序列标注模型的主动学习实践。"
tags: ["算法","深度学习中文技术社区","前端开发社区","前端技术交流","前端框架教程","JavaScript 学习资源","CSS 技巧与最佳实践","HTML5 最新动态","前端工程师职业发展","开源前端项目","前端技术趋势"]
ShowReadingTime: "阅读8分钟"
weight: 1
selfDefined:"likes:12,comments:0,collects:5,views:6817,"
---
> 我们来自字节跳动飞书商业应用研发部(Lark Business Applications)，目前我们在北京、深圳、上海、武汉、杭州、成都、广州、三亚都设立了办公区域。我们关注的产品领域主要在企业经验管理软件上，包括飞书 OKR、飞书绩效、飞书招聘、飞书人事等 HCM 领域系统，也包括飞书审批、OA、法务、财务、采购、差旅与报销等系统。欢迎各位加入我们。
> 
> 本文作者：飞书商业应用研发部 阳云

> 欢迎大家关注[**飞书技术**](https://juejin.cn/user/712139266595784 "https://juejin.cn/user/712139266595784")，每周定期更新飞书技术团队技术干货内容，想看什么内容，欢迎大家评论区留言~

背景
==

模型训练的一大难点是数据标注，一般是靠标注大规模数据来提升模型效果，但相应的会消耗大量人力和时间成本，因此一些缩减标注数据规模但不减模型效果的方法应运而生，通过主动学习挑选训练样本就是其中比较热门的方法。

主动学习（Active Learning）通过设计合理的样本挑选策略，不断从未标注的数据中挑选出优质数据加入标注后放入训练集。简单来说，主动学习的思想是通过模型自己选择对模型贡献最大的样本作为训练语料，以达到有效地降低标注成本的同时迅速提高模型的识别能力。

样本选择的原则是优先选择对模型“信息量大”的样本，在通常情况下，信息量大被诠释为不确定性较高和多样性高。

**基于不确定性**：从模型预测结果来看，挑选模型预测不准的，即当前模型较难解决的部分样本。

**基于多样性**：从样本间相似性来看，从未标注样本中挑选与已标注样本差异大的，覆盖问题信息比较多的样本。

策略介绍
====

如果挑选样本，有很多策略，这里主要介绍目前几种主流的策略。

下面都是基于序列标注模型介绍，样本的每个文字都会预测输出多个标签，每个标签有一个概率值(可以理解成预测为该标签的置信度)

1 NLC-每字最高分的平均值
---------------

基于置信度-平均分(NLC)

公式：

![](/images/jueJin/c127d74fa4184d9.png)

y\*表示字符串x的最大概率的标签

策略：取每个字预测最高分的平均值作为筛选分

优劣势：该方法是基础策略，简单实用，但效果不是很好

2 去掉高分后平均值
----------

基于置信度-平均分(去掉高分)

策略：考虑到一般大部分字的预测分数很高，只有重点个别字分数较低，按第一种策略平均后容易导致整体分数较高，看不出个别字差的问题，因此舍弃高分字后再按第一种策略筛选

优劣势：效果并没有明显改善

3 MNLP-最高分取log后的平均值
-------------------

基于置信度-平均分(MNLP)

Maximum Normalized Log-Probability (MNLP)

公式：

![](/images/jueJin/d31149a78ed54ce.png)

策略：相比第一种策略，对预测分取了log，变化不大

优劣势：效果并没有明显改善

4 每字信息熵的平均值
-----------

基于置信度-平均分(entropy)

![](/images/jueJin/8431a41ea0714d0.png)

ŷ是x所有可能的标签

策略：每个字按上面公式计算信息熵，一个样本再取每个字信息熵的平均值作为筛选分

优劣势：利用到的信息更多，计算更全面，但效果还是没有明显改善

5 LTP-每字最高分中的最低分
----------------

基于置信度-最低分(LTP)

Lowest Token Probability(LTP)

2020年最新选取策略 [arxiv.org/pdf/2001.02…](https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2001.02524.pdf "https://arxiv.org/pdf/2001.02524.pdf")

公式：

![](/images/jueJin/8e9065d946c0468.png)

y\*表示字符串x的最大概率的标签

策略：假设句子中每个字都该有极高的预测分，置信度取决于句子预测分最低的字，取最大概率序列中概率最小值

优劣势：比平均分区分度更高，实际效果好不少

6 LTP + 未登陆实体概率
---------------

基于置信度+多样性:LTP+unknown entity

公式：

![](/images/jueJin/33450740244d462.png)

LTP是指前面的策略5，e1 和 e2 是权重，实验中分别取 0.95、0.05 ，P(x)=未出现实体数/N (未出现实体数上限值取N,N在实验中取值10)

策略：考虑到NER实际应用中多数预测错误是训练语料中未出现实体，因此这里将置信度和多样性结合用

优劣势：考虑了更多特征，效果更好，但不够明显，劣势是策略更加复杂

7 其他策略
------

其他策略相比最优策略没有提升，这里就没有做更深入的实验：由LTP衍生来的取最小的两个(三个)概率值平均，LTP+基于分词的词相似性，最大概率值，第二小的概率值

实验
==

通过实验评估上述策略的优劣

实验准备
----

用线下数据做模拟实验，将已标注的数据划分为labeled set和unlabeled set

对比从unlabeled set随机选取数据去打标和active learning选取数据去打标的效果，起始 labeled set占总样本的 5%，unlabeled set 占总样本的95%

*   随机选取：从unlabeled set随机选取数据，labeled set逐渐增加到5%,10%，15%,20%，30%，......100%
*   active learning：用 labeled set训练模型，最初labeled set采用跟随机选取的5%一致的数据，训练出的模型通过active learning策略从unlabeled set选取5%数据加入labeled set，得到10%的训练数据，重新训练，再从unlabeled set选取数据加入labeled set，重复该过程到labeled set为100%数据
*   实验模型：基于3层bert的ner模型
*   测试指标：F1
*   数据：公开数据集msra (train数据49000+dev数据1658，测试数据4631)

实验数据
----

rate

count

随机

策略1

策略2

策略3

策略4

策略5

策略6

5%

2500

70.14%

\-

\-

\-

\-

\-

\-

10%

5000

76.68%

84.88%

82.00%

84.92%

84.59%

86.65%

86.73%

15%

7500

79.79%

88.08%

87.12%

87.84%

87.81%

88.60%

89.02%

20%

10000

83.79%

88.93%

88.36%

89.50%

89.72%

89.90%

89.89%

25%

12500

85.26%

90.08%

89.73%

89.71%

89.90%

90.08%

90.36%

30%

15000

86.78%

90.51%

90.48%

89.96%

90.20%

90.77%

90.45%

40%

20000

88.23%

90.87%

90.57%

90.52%

90.57%

90.70%

90.16%

50%

25000

88.55%

90.64%

90.04%

90.56%

90.74%

90.55%

90.21%

60%

30000

88.89%

90.68%

90.30%

90.65%

90.59%

90.56%

90.46%

70%

35000

88.39%

90.21%

90.21%

90.24%

90.21%

90.41%

90.24%

80%

40000

89.12%

89.66%

90.11%

89.58%

89.97%

90.11%

89.62%

90%

45000

89.08%

89.63%

90.15%

89.69%

89.99%

90.34%

89.66%

100%

49000

89.33%

\-

\-

\-

\-

\-

\-

![](/images/jueJin/0e78372003ba4b8.png)

![](/images/jueJin/e9a9aa58ddbc4fa.png)

实验分析
----

*   到30%数据时，策略5、6达到指标高峰
*   整体来看，策略5和6是第一梯队，策略6相比5稍好，说明基于未登录词的多样性对提升效果是有效的
*   策略5，效果比前四种句子平均分明显好不少，原因是平均分掩盖句子局部问题，大部分句子只是个别局部存在信息增益
*   策略1和3、4是第二梯队，效果差别极小，原因是这三种策略整体都是取平均分，差别仅在于每个字的得分计算公式不一样
*   策略2相比策略1，剔除高分，并没有带来收益，反而效果差点,原因是高分也是有差别的，剔除高分等于少了一项判断句子有效性的因子
*   策略高峰指标比全量数据指标还高，原因主要是受训练步数影响，这里为了加快评测速度统一用了1万步，实际3万步更合适，数据多了，就会受步数少训练不充分的影响，30%时数据较少，受步数影响较小，而策略用了最优质数据，因此反而比全量数据时指标还高

实验结论
----

采用策略6，效果最佳，但相比策略5提升不明显，考虑到实际应用复杂度，一般选择用策略5即可，策略5、6都能做到只用30%数据达到全量数据效果

实验总结
----

*   采用合适的主动学习能大幅降低标注成本的同时迅速提高模型效果(上面实验只用了30%数据)
*   主动学习效率提升程度取决于选择的策略，因此需要结合具体的模型选择合适的策略，目前序列标注比较适合策略5/6，但不代表其他类型的模型也是这样
*   基于目前最优策略结合多样性可探索更优策略，但需权衡复杂性和提升幅度，最好的不一定是最实用的，需结合自己使用场景考虑

参考
==

*   【领域报告】主动学习年度进展|VALSE2018 [zhuanlan.zhihu.com/p/38029108?…](https://link.juejin.cn?target=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F38029108%3Fspm%3Data.13261165.0.0.2d44475dTEjQpi "https://zhuanlan.zhihu.com/p/38029108?spm=ata.13261165.0.0.2d44475dTEjQpi")
*   DEEP ACTIVE LEARNING FOR NAMED ENTITY RECOGNITION [arxiv.org/pdf/1707.05…](https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F1707.05928.pdf "https://arxiv.org/pdf/1707.05928.pdf")
*   基于深度主动学习的命名实体识别[www.leiphone.com/news/201805…](https://link.juejin.cn?target=https%3A%2F%2Fwww.leiphone.com%2Fnews%2F201805%2FEguyIsqF4aecx2iA.html "https://www.leiphone.com/news/201805/EguyIsqF4aecx2iA.html")
*   主动学习方法在内容安全业务的实践与思考 [www.atatech.org/articles/16…](https://link.juejin.cn?target=https%3A%2F%2Fwww.atatech.org%2Farticles%2F160990%3Fspm%3Da1z2e.8101737.webpage.dtitle9.4fcf4f9bBVl3CL "https://www.atatech.org/articles/160990?spm=a1z2e.8101737.webpage.dtitle9.4fcf4f9bBVl3CL")
*   LTP: A New Active Learning Strategy for Bert-CRF Based Named Entity Recognition [arxiv.org/pdf/2001.02…](https://link.juejin.cn?target=https%3A%2F%2Farxiv.org%2Fpdf%2F2001.02524.pdf "https://arxiv.org/pdf/2001.02524.pdf")

* * *

加入我们
====

扫码发现职位 & 投递简历：

![image.png](/images/jueJin/ca6390dee6fb448.png)

官网投递：[job.toutiao.com/s/FyL7DRg](https://link.juejin.cn/?target=https%3A%2F%2Fjob.toutiao.com%2Fs%2FFyL7DRg "https://link.juejin.cn/?target=https%3A%2F%2Fjob.toutiao.com%2Fs%2FFyL7DRg")