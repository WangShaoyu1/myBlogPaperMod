---
author: "机器之心"
title: "ChatGPT后，人工智能的终极里程碑却倒了"
date: 2024-08-17
description: "「图灵测试是一个糟糕的测试标准，因为对话能力和推理完全是两码事。」最近几天，AI 圈里一个新的观点正在流行。"
tags: ["人工智能中文技术社区","前端开发社区","前端技术交流","前端框架教程","JavaScript 学习资源","CSS 技巧与最佳实践","HTML5 最新动态","前端工程师职业发展","开源前端项目","前端技术趋势"]
ShowReadingTime: "阅读14分钟"
weight: 1
selfDefined:"likes:1,comments:0,collects:0,views:155,"
---
> 大模型的拟人行为，在让我们产生恐怖谷效应。

「图灵测试是一个糟糕的测试标准，因为对话能力和推理完全是两码事。」最近几天，AI 圈里一个新的观点正在流行。

如今已是生成式 AI 时代，我们评价智能的标准该变了。

![](/images/jueJin/1c6c27707370471.png)

「机器能思考吗？」这是艾伦・图灵在他 1950 年的论文《计算机器与智能》中提出的问题。图灵很快指出，鉴于定义「思考」的难度，这个问题「毫无意义，不值得讨论」。正如哲学辩论中常见的做法，他建议用另一个问题代替它。

图灵设想了一个「模仿游戏」，在这个游戏中，一位人类评判员分别与一台计算机和一名人类（陪衬者）对话，双方都试图让评判员相信自己才是真正的人类。

重要的是，计算机、陪衬者和评判员之间互相无法对视，他们完全通过文本进行交流。在与每个候选者对话后，评判员猜测谁是真正的人类。

图灵的新问题是：「是否存在可想象的数字计算机，能在模仿游戏中表现出色？」

![](/images/jueJin/d7b308793edf4c2.png)

论文链接：

[academic.oup.com/mind/articl…](https://link.juejin.cn?target=https%3A%2F%2Facademic.oup.com%2Fmind%2Farticle%2FLIX%2F236%2F433%2F986238%3Flogin%3Dfalse "https://academic.oup.com/mind/article/LIX/236/433/986238?login=false")

这个由图灵提出的游戏，现在被广泛称为图灵测试，用以反驳广泛存在的直觉性认知：「由于计算机的机械性质，在原理层面上就不可能思考。」

图灵的观点是：如果一台计算机在表现上与人类无法区分（除了它的外貌和其他物理特征），那么为什么我们不将其视为一个有思维的实体呢？为什么我们要将「思考」的资格仅限于人类（或更广泛地说，仅限于由生物细胞构成的实体）？正如计算机科学家斯科特・阿伦森所描述的，图灵的提议是「一种反对『肉体沙文主义』的呼吁」。

**图灵测试是一种思想而不是「方法」**

图灵将他的测试作为一个哲学思想实验，而不是一种可以实际衡量机器智能的方法。然而，在公众的认知中，图灵测试已经成为人工智能（AI）的终极里程碑 —— 评判通用机器智能是否到来的主要标准。

如今，近 75 年过去了，关于 AI 的报道充斥着声称图灵测试已被通过的论调，尤其是像 OpenAI 的 ChatGPT 和 Anthropic 的 Claude 这样的聊天机器人的推出之后。

去年，OpenAI 的首席执行官山姆・奥特曼发文称：「面对技术变革，人们的适应能力和韧性得到了很好的体现：图灵测试悄然过去，大多数人继续他们的生活。」 

![](/images/jueJin/aa39a0abb0ca4dc.png)

各大媒体也发表了类似的标题，例如某家报纸报道称，「ChatGPT 通过了著名的『图灵测试』—— 表明该 AI 机器人具有与人类相当的智能。」

![](/images/jueJin/633f1e61197d485.png)

_英国每日发行的老牌报刊 —— 每日邮报_

甚至像 BBC 这样的全球最大媒体之一、具有广泛影响力的公共传媒机构甚至在 2014 年就提出计算机 AI 已经通过了图灵测试。

![](/images/jueJin/0679437c3e034d7.png)

_[www.bbc.com/news/techno…](https://link.juejin.cn?target=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-27762088 "https://www.bbc.com/news/technology-27762088")_

然而，问题是：现代聊天机器人真的通过了图灵测试吗？如果是这样，我们是否应该像图灵所提议的那样，赋予它们「思考」的地位？

令人惊讶的是，尽管图灵测试在文化上具有广泛的重要性，长久以来，AI 社区却对通过图灵测试的标准没有达成一致意见。许多人怀疑，具备能够欺骗人的对话技能是否真正揭示了系统的底层智能或「思考」能力。

一千个人眼中很可能有一千个图灵测试标准。

图灵奖得主 Geoffery Hinton 在一次访谈中谈及了他的「图灵测试标准」，他认为 Palm 等聊天机器人可以解释笑话为何有趣，这可以视为其智能的标志。如今的大模型，如 GPT-4 非常擅长解释一个笑话为何有趣，这被认为是其图灵测试的一部分标准。

相比于其他科学家对于图灵测试严肃的定义，Hinton 的看法虽然诙谐，但仍是道出了他对「人工智能是否有思考能力」这一终极命题的思考。

![](/images/jueJin/677e10837d1f4af.png)

_访谈视频链接：[www.youtube.com/watch?v=PTF…](https://link.juejin.cn?target=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPTF5Up1hMhw "https://www.youtube.com/watch?v=PTF5Up1hMhw")_

**一场「图灵闹剧」**

由于图灵并没有提出一个具有完整实际操作说明的测试。

他对「模仿游戏」的描述缺乏细节：

*   测试应该持续多长时间？
    
*   允许提出什么类型的问题？
    
*   人类的评判员或「陪衬者」需要具备什么样的资格？
    

对于这些具体问题，图灵并未详细说明。不过，他做了一个具体的预测：「我相信，大约在 50 年内，计算机可以通过编程变得极其出色，以至于普通的审问者在经过五分钟的提问后，识别出真实人类的概率不超过 70%。」简而言之，在五分钟的对话中，评判员有平均 30% 的几率会被误导。

有些人将这一随意的预测视为通过图灵测试的「官方」标准。2014 年，英国皇家学会在伦敦举办了一场图灵测试比赛，参赛的有五个计算机程序、30 个人类陪衬和 30 名评判员。

参与的人类群体多样，既有年轻人也有年长者，既有母语是英语的人也有非母语者，既有计算机专家也有非专家。每位评判员分别与一对参赛者（一名人类和一台机器）进行了多轮五分钟的并行对话，之后评判员必须猜测谁是人类。

一款名为「尤金・古斯特曼（Eugene Goostman）」的聊天机器人，扮演一名少年，竟然成功欺骗了 10 名评判员（欺骗率：33.3%）。

显然「欺骗率」已经超过了图灵当时所言的 30%。

![](/images/jueJin/2251ac189d0740a.png)

_尤金・古斯特曼（Eugene Goostman）模拟的是一名 13 岁男孩。_

根据「在五分钟内有 30% 几率欺骗」的标准，组织者宣布：「65 年前的标志性图灵测试首次被计算机程序『尤金・古斯特曼』通过了图灵测试，这一里程碑将载入史册……」。

AI 专家在阅读这场图灵测试中的主角「尤金・古斯特曼（Eugene Goostman）」对话的记录后，对该聊天机器人通过图灵测试的说法嗤之以鼻，认为这个不够复杂且不像人类的聊天机器人并未通过图灵设想的测试。

有限的对话时间和评判员的专业水平不均使得这次测试更像是对人类轻信的考验，而非机器智能的展示。结果却是「ELIZA 效应」的一个鲜明例子 —— 这个效应得名于 1960 年代的聊天机器人 ELIZA，尽管其极其简单，但仍能让许多人误以为它是一个理解并富有同情心的心理治疗师。

这凸显了我们人类倾向于把能够与我们对话的实体赋予智能的心理倾向。

![](/images/jueJin/6a0bbe6740b744d.png)

_ELIZA 是图灵测试「发表」后的最早期聊天机器人之一，它是一个非常基础的罗杰斯派心理治疗聊天机器人。_

另一项图灵测试比赛 —— 勒布纳奖（Loebner Prize）允许更长的对话时间，邀请了更多专家评委，并要求参赛机器至少欺骗一半的评委。有趣的是，**当标准提高后，在近 30 年的年度比赛中，没有一台机器通过这一版本的测试。**

**图灵测试开始转向**

尽管图灵的原始论文中缺乏关于如何具体实施测试的细节，但很明显，模仿游戏需要三名参与者：一台计算机、一位人类陪衬者和一位人类评判员。然而，随着时间的推移，「图灵测试」这一术语在公众讨论中的含义逐渐演变为一个明显弱化的版本：任何人与计算机之间的互动，只要计算机表现得足够像人类，就被视为通过了图灵测试。

例如，当《华盛顿邮报》在 2022 年报道「谷歌的 AI 通过了一个著名测试 —— 并展示了该测试的缺陷」时，他们指的并不是模仿游戏，而是谷歌工程师布雷克・莱莫因（Blake Lemoine）认为谷歌的 LaMDA 聊天机器人具有\*\*「感觉能力（sentient）」\*\*。

2024 年，斯坦福大学发布的一份新闻稿宣布，斯坦福团队的研究「标志着人工智能首次通过了严格的图灵测试之一」。但这里所谓的图灵测试是通过比较 GPT-4 在心理调查和互动游戏中的行为统计与人类的行为统计来完成的。

这种定义可能与图灵的初衷相去甚远：斯坦福团队的定义是「我们认为，当 AI 的回应在统计上无法与随机选取的人类回应区分开来时，它就通过了图灵测试。」

最新的，声称聊天机器人通过图灵测试的案例来自 2024 年的一项研究，该研究采用了「双人模式」的测试：与图灵的「三人」模仿游戏（其中裁判会同时询问计算机和人类陪衬）不同，这里每个裁判只与计算机或人类互动。

研究人员招募了 500 名人类参与者，每人被分配为裁判或人类陪衬。每位裁判与陪衬、GPT-4（提示了人类编写的如何欺骗裁判的建议）或 ELIZA 聊天机器人版本玩一轮五分钟的游戏。通过网络界面交谈五分钟后，裁判猜测他们的对话对象是人类还是机器。

最后，人类陪衬在 67% 的回合中被判定为人类；GPT-4 在 54% 的回合中被判定为人类，ELIZA 在 22% 的回合中被判定为人类。作者将「通过」定义为超过 50% 的时间欺骗评委 —— 也就是说，超过随机猜测的概率。按照这个定义，GPT-4 通过了，尽管人类对手的得分仍然更高。

令人担忧的是，大多数人类判断者在五分钟的对话之内被 GPT-4 欺骗了。使用生成式人工智能系统冒充人类传播虚假信息或进行诈骗是社会必须应对的风险。但如今的聊天机器人真的通过了图灵测试吗？

答案当然是，这取决于你谈论的是哪个版本的测试。一场由专家评委和更长对话时间组成的三人模仿游戏仍然没有被任何机器通过（有人计划在 2029 年举行一个超严格的版本）。

由于图灵测试的重点是试图骗过人类，而不是更直接地测试智能。许多人工智能研究人员长期以来一直认为图灵测试是一种干扰，是一种「不是为了让人工智能通过，而是为了人类失败」的测试。但该测试的重要性在大多数人眼中仍然占据主导地位。

进行对话是我们每个人评估其他人类的重要方式。我们很自然地认为，能够流利交谈的智能体必须拥有类似人类的智能和其他心理特征，如信仰、欲望和自我意识。

然而，如果说人工智能的发展历史教会了我们什么，那就是这些假设往往是站在错误的直觉上的。几十年前，许多著名的人工智能专家认为，创造一台能够在国际象棋比赛中击败人类的机器需要与人类完全智能相当的东西。

人工智能先驱 Allen Newell 和 Herbert Simon 在 1958 年写道：「如果一个人能设计出一台成功的国际象棋机器，他似乎已经深入到人类智力的核心。」认知科学家 Douglas Hofstadter 在 1979 年预测，未来「可能会有程序可以在国际象棋比赛中击败任何人，但…… 它们将是通用智能的程序。」

当然，在接下来的二十年里，IBM 的 DeepBlue 击败了世界国际象棋冠军加里・卡斯帕罗夫，使用的是一种与我们所说的「通用智能」相去甚远的蛮力方法。同样，人工智能的进步表明，曾经被认为需要通用智能的任务 —— 语音识别、自然语言翻译，甚至自动驾驶 —— 都可以由缺乏人类理解能力的机器来完成。

图灵测试很可能会成为我们不断变化的智力观念的又一个牺牲品。1950 年，图灵直觉地认为，像人类一样交谈的能力应该是「思考」及其所有相关能力的有力证据。这种直觉至今仍然很有说服性。但也许我们从 ELIZA 和 Eugene Goostman 身上学到的东西，以及我们可能仍能从 ChatGPT 及其同类产品中学到的东西是，能够流利地说出自然语言，比如下棋，并不是通用智力存在的确凿证据。

事实上，神经科学领域中有越来越多的证据表明，语言流利程度与认知的其他方面出奇地脱节。麻省理工学院神经科学家 Ev Fedorenko 等人通过一系列细致而令人信服的实验表明，他们所谓的「正式语言能力」（与语言生成相关的能力）背后的大脑网络与常识、推理和我们可能称之为「思考」的其他方面背后的网络大体上是分开的。这些研究人员声称，我们直觉上认为流利的语言是一般智力的充分条件，这是一种「谬论」。

图灵在 1950 年的论文中写道：「我相信，到本世纪末，词语的使用和普遍受教育的观点将会发生巨大的变化，人们将能够谈论机器思考，而不会遭到反驳。」如今的我们还没有到达那个地步。图灵的预测是否只是偏离了几十年？真正的改变是否发生在我们对「思考」的概念上？—— 还是说真正的智能比图灵和我们所认识到的更复杂、更微妙？一切还有待观察。

有趣的是，最近谷歌前 CEO 埃里克・施密特在那场斯坦福大学的演讲里也发表了观点。

在很长一段历史中，人类对宇宙的理解更多是神秘的，科学革命改变了这种情况。然而现在的 AI 却再次让我们无法真正理解其中的原理。知识的本质是否正在发生变化？我们是否要开始接受这些 AI 模型的结果，与此同时不再需要它们解释给我们听呢？

![](/images/jueJin/d2981e2a44634e0.png)

施密特是这样说的：我们可以将其比作是青少年。如果你有个十来岁的孩子，你知道他们是人类，但你却无法完全理解他们的想法。我们的社会显然适应青少年的存在。我们可能会有无法完全理解的知识系统，但我们理解它们的能力范围。

这可能就是我们能够获得的最好结果。