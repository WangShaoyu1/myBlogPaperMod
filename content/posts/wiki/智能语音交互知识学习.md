---
author: "王宇"
title: "智能语音交互知识学习"
date: 四月20,2023
description: "郑小涵"
tags: ["郑小涵"]
ShowReadingTime: "12s"
weight: 390
---
下图是一次智能语音交互的过程演示。

过程中作为人类可感知的其实只有两部分：一个是输入——用户口头说出一句话“明天出门要带伞吗？”；一个是输出——智能语音交互产品播报一段音频“您好，成都明天晴，气温5-10℃，不需要带伞哦。”。

但实际上整个智能语音交互过程却是更为复杂的，从输入到输出的过程，机器需要经历**“听清”、“识别”、“理解”、“行动”、“话术”、“播报”**整整六个环节，类比人听到同样的语音指令所需要进行的步骤。这六个环节对应到智能语音交互技术上，分别是：

*   信号处理（SSP）
*   语音识别（ASR）
*   自然语言理解（NLU）
*   自然语言生成（NLG）
*   语音合成（TTS）
*   智能调音（Intelligent Tone Tuning，ITT）

![](https://pic2.zhimg.com/80/v2-5610d18a53b65b8b55c91d46a563047d_720w.webp)![](https://pic3.zhimg.com/80/v2-1ca06d14c97f1ae052ad4a09692a1d1a_720w.webp)

1、信号处理

首先解释一下**“声学场景”**的概念：声音是一种波，在传播中会不断的反射、折射、衍射、吸收，在任何场景中设备接收到的声音都会包含**”需要识别的声音”、“不需要识别的的声音”**。不需要识别的声音会是多元的，包含噪音、额外的人声、回声、混响叠加在一起，在不同的场景中会有不同的叠加效果，形成一个专有的声场，传入机器的耳朵（麦克风）中。我们一般把声源和麦克风之间的距离在半米以内称为“近场场景”，大于半米的称为“远场场景”。**不同的声学场景需要不同的前端信号处理方案去适配**。

通常我们所说的智能语音交互，狭义的概念中只包含了ASR、NLP、TTS三个部分，这是因为在语音交互的初期，产品大多是手机里的app，即使用户身处在一个嘈杂的环境，但用户输入语音时通常是将手机靠近嘴巴，这时候的**声学场景**大多是低噪音+近场，近场场景下，需要识别的声音远大于不需要识别的声音，因此信号处理技术很少被提及。

在远场场景下，比如机场候机厅或医院导诊室里的机器人，此时的声学场景=需要识别的声音+不需要识别的声音，因此信号处理的结果直接决定了下一步语音识别的质量，以及最终语音交互的体验。

一个好的智能语音产品，信号处理和语音识别的能力必然是针对其投放的“声学场景”深度适配优化后的，才能对后续的语音交互体验有所保障。

2、语音交互，Voice User Interface（VUI）

与GUI（Graphic Usr Interface）对应，GUI是强视觉弱逻辑的交互，而VUI是强逻辑弱视觉（或无视觉），通过语音的方式进行人机交互。

广义的VUI过程就包括上述六个环节，且人与机器之间反复进行多次交互才能形成一个完整的语音交互过程。狭义的则指ASR、NLP、TTS。NLP自然语言处理是NLU与NLG的统称。

\*3、**智能调音**

在播报的环节，除了TTS，我还增加了一个技术是**智能调音**，这一块虽然不会对整个智能语音交互过程的信息以及处理结果产生影响，但是会对整个智能语音交互过程的用户体验产生影响，也需要关注。

智能调音技术与扬声器、音响等设备有关。

  

  

参考：[https://zhuanlan.zhihu.com/p/109885562](https://zhuanlan.zhihu.com/p/109885562)

  

  

  

  

[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)

[Configure buttons visibility](/users/tfac-settings.action)