---
author: "王宇"
title: "3.2、语音交互体验需求说明（针对科大讯飞与鱼亮科技提供的技术支持）"
date: 三月10,2023
description: "3、语音交互体验需求"
tags: ["3、语音交互体验需求"]
ShowReadingTime: "12s"
weight: 73
---
背景：
===

万得厨团队之前进行过语音相关的软硬件一体化性能测试：[9.1、万得厨语音部分的软硬一体化性能测试方案](/pages/viewpage.action?pageId=95556039)，主要是测试语音唤醒率、识别（响应）率、误唤率这几个方面，侧重于性能。

而关于识别的准确率、语音交互的体验感等非性能方面，则另外进行了测试：[3.1、万得厨2.0虚拟人语音交互功能测评](/pages/viewpage.action?pageId=95559396)，根据测试的结果，整理出一份针对科大讯飞与鱼亮科技提供的技术支持的需求说明，希望通过技术上的改进来提高用户与虚拟人语音交互的体验感。

  

需求说明清单（0221细化版）：
================

![](/download/attachments/95560754/image2023-2-21_20-6-30.png?version=2&modificationDate=1678432623052&api=v2)

2019年由电信终端产业协会发布的团体标准文件：

[TTAF 041-2019 智能产品语音识别测评方法 第一部分车载语音交互系统.pdf](/download/attachments/95560754/TTAF%20041-2019%20%E6%99%BA%E8%83%BD%E4%BA%A7%E5%93%81%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%B5%8B%E8%AF%84%E6%96%B9%E6%B3%95%20%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E8%BD%A6%E8%BD%BD%E8%AF%AD%E9%9F%B3%E4%BA%A4%E4%BA%92%E7%B3%BB%E7%BB%9F.pdf?version=1&modificationDate=1677028835361&api=v2)

[TTAF 043-2019 智能产品语音识别测评方法 第二部分：智能音箱.pdf](/download/attachments/95560754/TTAF%20043-2019%20%E6%99%BA%E8%83%BD%E4%BA%A7%E5%93%81%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%B5%8B%E8%AF%84%E6%96%B9%E6%B3%95%20%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%EF%BC%9A%E6%99%BA%E8%83%BD%E9%9F%B3%E7%AE%B1.pdf?version=1&modificationDate=1677028835541&api=v2)

序号

技术模块

需求点

需求点描述

需求现状描述

需求期望描述

技术类指标

优先级

需求相关方

备注

**1**

**唤醒**

可以配置进入休眠的时间

在虚拟人进入休眠前，用户只需唤醒一次就可以在一定的时间内持续对话（无需重复输入唤醒词）

目前虚拟人被唤醒后，超过2s再输入下一指令，就已无法准确接收且做出反馈（也可能是受到干扰）；且每次输入新的指令都需要重复唤醒，影响用户体验，对虚拟人的运行来说也容易造成卡顿和延迟

希望可以自行配置虚拟人进入休眠的时间

（参考小鹏的智能语音助手小P：时间设定有4档：20s-30s-60s-120s）

（各个方位的）唤醒率：≥95%

（各个方位的）误唤醒率：≤5%

one-shot后续指令的识别率：≥95%

p0

科大讯飞

内部可进行设置不同休眠时间的用户体验测试（由鹏哥带领）

**2**

支持one-shot功能

“唤醒词+意图识别”一体化，支持用户可以在说出唤醒词之后不作停顿

现目前不支持one-shot交互，且简单指令唤醒率也不算高

需要支持one-shot交互，比如：

用户：“小万小万，帮我打开炉门”，

小万：“好的，以为你打开炉门”

one-shot交互更便捷、自然，可以极大的提升了交互便利性

p1

科大讯飞

  

**3**

**ASR**

识别速度/响应速度

语音输入到响应在1s以内。

识别结果响应时间低于200ms。

反应很迟钝，多数情况下隔一秒钟就不能回答

甚至语音唤醒识别都很迟钝，而且需要提高人声。

排查具体问题，进行全链路优化，做到语音输入到响应在1s以内

响应时间：≤1s

WER（ASR字错率）：≤5%

SER（ASR句错率）：≤5%

p0

科大讯飞、

影子虚拟人团队

需要进行全链路耗时分析瓶颈进行具体优化：  
1、ASR识别耗时

2、接口请求耗时

3、TTS响应耗时

**4**

ASR识别自动校正

基于相关专业领域的知识库，能够做到在用户输入的语音指令不准确时对识别内容自动进行校正

当前我们采取的头部领域是菜名，通过对各种与菜名相关的问题进行测试，发现识别率不高，且兜底话术很少

需要在正常环境使用过程中4米以内识别准确率需要做到>=95%，需要增加专业领域知识以及兜底话术，例如：

用户：“小万小万，鱼香肉丝怎么做？”

小万：“已为您找到鱼香肉丝的食谱，第一步，需要胡萝卜、木耳、肉丝少许.....”（而不会查到京酱肉丝的做法）

p0

科大讯飞、

杭州虚拟人公司、

影子虚拟人团队

需要补充相关专业领域知识；

指令中包含实体的需要开放配置

**5**

全双工/连续对话

在唤醒后的一段时间内语音助手都会处于收音状态，可以提供连续对话（边听边说）；

当下一句指令与上一句之间完全没有联系的时候，不会对收听到的无关背景对话进行响应（即“后续query”的NLP不支持长尾词的理解）

小万不支持连续对话

需要做到完整的连续对话；且对于突然插入的指令，若不支持长尾词的理解，需要给出兜底话术，例如：

用户：“小万小万，开始烹饪”

小万：“好的主人，已为您烹饪”

用户：“你会说方言吗”

小万：“这个技能我不会”（此回答为兜底话术，也可用其他替代，比如：“你在说什么”）

p0

杭州虚拟人公司

需要能够联系上下文的含义

**6**

**NLU/NLP**

语义打断

在较长的播报中，可以随时发出新指令，且上下文之间语义是连贯的

测试的较长播报是播报食谱，目前在播报食谱的过程中不能打断，即使已经获取到了新指令，也只有在播报结束后才会执行

希望能够实现语义打断，在播报食谱的过程中可以随时打断，并且上下文连贯。比如：

用户：“番茄炒蛋怎么做？”

小万：“番茄炒蛋的做法是…………”

用户：（打断）“那水煮肉片呢？”

小万：“水煮肉片的做法是…………”（会进行水煮肉片的食谱播报，而不是调取水煮肉片的烹饪方案）

语义理解准确率（查准率）： ≥ 90%

召回率（查全率）：≥ 90%

（能够正确识别用户话术到某个意图中，则为对应场景的召回率，召回率= 正确识别到该场景的话术/用户话术中存在该意图的话术。）

F1值：≥ 90%

（准确率和召回率的调和平均数；F1值越高越好）

  

（离线状态）语义理解准确率：≥90%

（离线状态）召回率：≥90%

p0

科大讯飞、

杭州虚拟人公司

需要能够联系上下文的含义

与【连续对话】的区别：语义打断是用一条新的任务型指令来中断播报（通常与当前场景有联系）；而连续对话的打断是指与当前场景无上下文联系的问答

**7**

兜底回复

在用户提问时如果没有满足条件的知识点，则进行兜底回复

目前小万的兜底回复较少，且有时小万已经无法接收用户指令（间隔时间过长），但却没有响应的兜底回复，容易给用户带来困惑

补充兜底回复，自行配置，例如，不能查询到相应食谱时：

用户：“小万小万，麻婆豆腐怎么做？”

小万：“对不起，主人，这道菜小万还没学会”

p0

影子虚拟人内部

按意图设置不同的兜底回复（比如，首先是识别到了做菜的意图，然后在未查询到用户输入的对应食谱后，给出兜底：“这道菜还没学会”）

**8**

离线意图识别

离线识别是指在弱网或无网状态下保留基础指令

目前小万不支持离线状态下的唤醒与意图识别

离线识别需要做到常用简单的指令的识别，例如“开始烹饪”、“天气怎么样”等等，通过正则表达式获取关键词，匹配指令后执行。

p1

科大讯飞、

杭州虚拟人公司

与烹饪相关的基础指令需要支持离线识别

**9**

敏感词过滤

敏感词指涉黄、涉暴、涉政、涉恐或者儿童不宜的词汇。敏感过滤把敏感词屏蔽，不展示给用户。

敏感词过滤默认关闭

（目前后台暂未打开敏感词过滤，还未进行体验测试）

能够自行设置需要屏蔽的敏感词

p2

科大讯飞

  

**10**

**TTS**

自定义语音包（定制音色）

类似百度地图可以录制专属语音包，用户录制几句标准语音即可以生成相应的导航语音包

目前小万的音色不支持修改和定制

在敬老爱幼等场景中，自定义语音包可以支持将用户家人的语音录入并匹配给虚拟人，从而给用户带来亲切感

【采用5分制主观评分】

TTS自然度：＞3分

(指TTS在播报时的流畅程度，更加贴近于人在说话，而非机器音，是一个偏主观的指标。)

  

同理心：＞3分

趣味性：＞3分

人设一致性：＞4分

p2

科大讯飞

  

**11**

TTS个性化播报

根据场景定制TTS文本

当前小万不支持TTS个性化播报，无论是谁、无论何时何地、什么场景向小万发出指令或询问，得到的反馈回答都是一致的

能够支持根据场景定制TTS文本，比如：

用户询问“有什么美食推荐”的时候，可以根据当前的时间给出不一样的回答；

通过身份识别技术识别到当前和虚拟人对话的家庭成员是谁，从而可以在TTS反馈文本中加入用户昵称等信息，体现亲切感

p2

影子虚拟人团队、

科大讯飞

可能会涉及到其他有关时间空间信息、身份理解以及性别、年龄预测等技术

  

阿里云不支持自定义，但可以指定某个发音人

[https://help.aliyun.com/document\_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG](https://help.aliyun.com/document_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG)

科大讯飞支持定制

[https://www.xfyun.cn/solution/soundLibrary](https://www.xfyun.cn/solution/soundLibrary)，具体还要和科大讯飞商谈

by renpeng

参考链接：[https://blog.csdn.net/mingzheng114/article/details/120121572](https://blog.csdn.net/mingzheng114/article/details/120121572)

[https://coffee.pmcaff.com/article/2999187665857664/](https://coffee.pmcaff.com/article/2999187665857664/)

[语音量化分析指标.xmind](/download/attachments/95560754/%E8%AF%AD%E9%9F%B3%E9%87%8F%E5%8C%96%E5%88%86%E6%9E%90%E6%8C%87%E6%A0%87.xmind?version=1&modificationDate=1676985397974&api=v2)

[万得厨”小飞小飞“语音【识别率】测试报告](/pages/viewpage.action?pageId=95561208)   by renpeng

  

  

  

  

  

  

需求说明清单（初稿）：
===========

序号

需求分类

需求点

需求描述

需求现状描述

需求期望描述

优先级

需求相关方

备注

1

唤醒（听得见）

可以配置进入休眠的时间

可自行配置虚拟人进入休眠的时间，在进入休眠前，虚拟人只需唤醒一次就可以在一定的时间内持续对话（无需重复输入唤醒词）

目前虚拟人被唤醒后，超过2s再输入下一指令，就已无法准确接收且做出反馈（也可能是受到干扰）；且每次输入新的指令都需要重复唤醒，在识别准确率与响应速度都不太高的情况下，频繁的说唤醒词不仅影响用户体验，对虚拟人的性能运行来说也容易造成卡顿和延迟

希望虚拟人进入休眠的时间可以自行配置

（参考小鹏的智能语音助手小P：时间设定有4档：20s-30s-60s-120s）

p0

科大讯飞

内部可进行设置不同休眠时间的用户体验测试（由鹏哥带领）

2

唤醒词+离线意图识别

离线识别是指在弱网或无网状态下保留基础指令

目前小万不支持离线状态下的唤醒与意图识别

离线识别需要做到常用简单的指令的识别，例如“开始烹饪”、“天气怎么样”等等，通过正则表达式获取关键词，匹配指令后执行。

p1

科大讯飞、

杭州虚拟人公司

与烹饪相关的基础指令需要支持离线识别

3

支持one-shot功能

“唤醒词+意图识别”一体化，支持用户可以在说出唤醒词之后不作停顿，立刻说出后续需求。

现目前不支持one-shot交互，且简单指令唤醒率也不算高

需要支持one-shot交互，比如：

用户：“小万小万，帮我打开炉门”，

小万：“好的，以为你打开炉门”

one-shot交互更便捷、自然，可以极大的提升了交互便利性

p1

科大讯飞

  

4

聆听（听得清）

  

**识别速度/响应速度**

语音输入到响应在1s以内

反应很迟钝，多数情况下隔一秒钟就不能回答

甚至语音唤醒识别都很迟钝，而且需要提高人声。

排查具体问题，进行全链路优化，做到语音输入到响应在1s以内

p0

科大讯飞、

影子虚拟人团队

需要进行全链路耗时分析瓶颈进行具体优化：  
1、ASR识别耗时

2、接口请求耗时

3、TTS响应耗时

5

语义打断

在较长的播报中，可以随时发出新指令，且上下文之间语义是连贯的

测试的较长播报是播报食谱，目前在播报食谱的过程中不能打断，即使已经获取到了新指令，也只有在播报结束后才会执行

希望能够实现语义打断，在播报食谱的过程中可以随时打断，并且上下文连贯。比如：

用户：“番茄炒蛋怎么做？”

小万：“番茄炒蛋的做法是…………”

用户：（打断）“那水煮肉片呢？”

小万：“水煮肉片的做法是…………”（会进行水煮肉片的食谱播报，而不是调取水煮肉片的烹饪方案）

p0

科大讯飞、

杭州虚拟人公司

需要能够联系上下文的含义

6

全双工/连续对话

在唤醒后的一段时间内语音助手都会处于收音状态，可以提供连续对话；

且当下一句指令与上一句之间完全没有联系的时候，不会对收听到的无关背景对话进行响应（即“后续query”的NLP不支持长尾词的理解）

小万不支持连续对话

需要做到完整的连续对话；且对于突然插入的指令，若不支持长尾词的理解，需要给出兜底话术，例如：

用户：“小万小万，开始烹饪”

小万：“好的主人，已为您烹饪”

用户：“你会说方言吗”

小万：“这个技能我不会”（此回答为兜底话术，也可用其他替代，比如：“你在说什么”）

p0

杭州虚拟人公司

需要能够联系上下文的含义

与【语义打断】的区别是：语义打断使用的是一条新的任务型指令（通常与当前场景有联系）；而连续对话中的打断，是指与当前场景无上下文联系的问答

7

理解（听得懂）

**ASR识别自动校正**

正常环境使用过程中4米以内ASR识别准确率需要做到>=95%

当前我们采取的头部领域是菜名，通过对各种与菜名相关的问题进行测试，发现识别率不高，且兜底话术很少

需要在正常环境使用过程中4米以内识别准确率需要做到>=95%，需要增加专业领域知识以及兜底话术，例如：

用户：“小万小万，鱼香肉丝怎么做？”

小万：“已为您找到鱼香肉丝的食谱，第一步，需要胡萝卜、木耳、肉丝少许.....”（而不会查到京酱肉丝的做法）

  

  

p0

科大讯飞、

杭州虚拟人公司、

影子虚拟人团队

需要补充相关专业领域知识；

指令中包含实体的需要开放配置

8

播报（能反馈）

兜底回复

在用户提问时如果没有满足阈值的知识点，则进行兜底回复

目前小万的兜底回复较少，且有时小万已经无法接收用户指令（间隔时间过长），但却没有响应的兜底回复，容易给用户带来困惑

补充兜底回复，自行配置，例如，不能查询到相应食谱时：

用户：“小万小万，麻婆豆腐怎么做？”

小万：“对不起，主人，这道菜小万还没学会”

p0

影子虚拟人内部

是否可以按意图设置不同的兜底回复？（比如，首先是识别到了做菜的意图，然后在未查询到用户输入的对应食谱后，给出兜底：“这道菜还没学会”）

9

自定义语音包（定制音色）

类似百度地图可以录制专属语音包，用户录制几句标准语音即可以生成相应的导航语音包

目前小万的音色不支持修改和定制

在敬老爱幼等场景中，自定义语音包可以支持将用户家人的语音录入并匹配给虚拟人，从而给用户带来亲切感

p2

科大讯飞

  

10

TTS个性化播报

根据场景定制TTS文本

当前小万不支持TTS个性化播报，无论是谁、无论何时何地、什么场景向小万发出指令或询问，得到的反馈回答都是一致的

能够支持根据场景定制TTS文本，比如：

用户询问“有什么美食推荐”的时候，可以根据当前的时间给出不一样的回答；

通过身份识别技术识别到当前和虚拟人对话的家庭成员是谁，从而可以在TTS反馈文本中加入用户昵称等信息，体现亲切感

p2

影子智能研究院、

科大讯飞

可能会涉及到其他有关时间空间信息、身份理解以及性别、年龄预测等技术

  

阿里云不支持自定义，但可以指定某个发音人

[https://help.aliyun.com/document\_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG](https://help.aliyun.com/document_detail/84435.html?spm=a2c4g.11186623.0.0.352e142aXtPDwG)

科大讯飞支持定制

[https://www.xfyun.cn/solution/soundLibrary](https://www.xfyun.cn/solution/soundLibrary)，具体还要和科大讯飞商谈

by renpeng

1、细化内容（0221下午）

2、补充需求相关方——识别意图前都属于科大讯飞

3、识别速度与识别准确率的现状，提供数据上的体现（志川、鹏哥）

[Filter table data](#)[Create a pivot table](#)[Create a chart from data series](#)

[Configure buttons visibility](/users/tfac-settings.action)